<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Techniques in Machine Learning AIML231</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="22f78694-2a08-4c8e-a712-034be8b9dcfd" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">🤖</span></div><h1 class="page-title"><strong>Techniques in Machine Learning AIML231</strong></h1><p class="page-description"></p><table class="properties"><tbody><tr class="property-row property-row-created_time"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesCreatedAt"><path d="M8 15.126C11.8623 15.126 15.0615 11.9336 15.0615 8.06445C15.0615 4.20215 11.8623 1.00293 7.99316 1.00293C4.13086 1.00293 0.938477 4.20215 0.938477 8.06445C0.938477 11.9336 4.1377 15.126 8 15.126ZM8 13.7383C4.85547 13.7383 2.33301 11.209 2.33301 8.06445C2.33301 4.91992 4.84863 2.39746 7.99316 2.39746C11.1377 2.39746 13.6738 4.91992 13.6738 8.06445C13.6738 11.209 11.1445 13.7383 8 13.7383ZM4.54102 8.91211H7.99316C8.30078 8.91211 8.54004 8.67285 8.54004 8.37207V3.8877C8.54004 3.58691 8.30078 3.34766 7.99316 3.34766C7.69238 3.34766 7.45312 3.58691 7.45312 3.8877V7.83203H4.54102C4.2334 7.83203 4.00098 8.06445 4.00098 8.37207C4.00098 8.67285 4.2334 8.91211 4.54102 8.91211Z"></path></svg></span>Created time</th><td><time>@February 25, 2024 2:53 PM</time></td></tr></tbody></table></header><div class="page-body"><figure id="48301a79-1c97-4739-a787-c9de5b1c9a1b"><a href="https://ecs.wgtn.ac.nz/Courses/AIML231_2024T1/CourseOutline" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Course Outline - Courses/AIML231_2024T1 | ECS | Victoria University of Wellington</div><div class="bookmark-description">This course introduces core concepts and techniques in machine learning, as well as commonly used software libraries for implementing machine learning pipelines. It includes an overview of the machine learning field, including supervised and unsupervised learning; fundamental machine learning techniques including neural networks; tools to understand data such as exploratory data analysis, pre-processing, and visualisation; and the design machine learning pipelines. This course balances theoretical concepts of machine learning and the use of programming libraries for hands-on practice.</div></div><div class="bookmark-href"><img src="https://ecs.wgtn.ac.nz/favicon.ico" class="icon bookmark-icon"/>https://ecs.wgtn.ac.nz/Courses/AIML231_2024T1/CourseOutline</div></div></a></figure><ul id="eb16d5d0-5058-4ebc-93c3-def1b91859bd" class="toggle"><li><details><summary>Introduction</summary><ul id="cd5870f6-c64c-476a-b2b5-3de468fd45e6" class="toggle"><li><details><summary>AIML131</summary><figure id="41ecd3fe-aa26-484a-a75c-a7102cbbfc32"><div class="source">https://docs.google.com/document/d/12f74DMawV4spSI9gwXY9ti0cI42zkkrAqD9_fryemGo/edit</div></figure><figure id="aa2e3021-2e41-40b8-8db2-fdef9ce176bf"><div class="source">https://docs.google.com/document/d/16pFyDKxLmDBcWm19pdW-eJ_xuhGy4pul4n5Etg2dBI4/edit</div></figure></details></li></ul><ul id="2f75360e-cbbf-48f9-8b1c-269be9569276" class="toggle"><li><details><summary>What is machine learning</summary><figure id="b79c11a6-035a-489b-8fc8-9171d818e815" class="image"><a href="Untitled.png"><img style="width:1504px" src="Untitled.png"/></a></figure></details></li></ul><ul id="d0118876-8e81-424f-a563-508c6c08822e" class="toggle"><li><details><summary>Five Tribes of Machine Learning</summary><figure id="eb308b29-6285-41be-b49d-bd5de096de38" class="image"><a href="Untitled%201.png"><img style="width:1454px" src="Untitled%201.png"/></a></figure></details></li></ul><ul id="4e02b1b1-d7a2-4775-b75d-320ec27cebd1" class="toggle"><li><details><summary>Data</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="b64a63b8-59e9-43d0-8a38-1140a1fefb80"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Makes up and builds networks using information from datasets<p id="255cc0d1-3242-4a12-99f6-a9e66d3f7d45" class="">● <a href="http://datahub.io/">datahub.io</a><br/>● <br/><a href="http://openml.org/">openml.org</a><br/>● Kaggle, UCI<br/></p><p id="b4198184-ea43-49d6-9ee0-728454654467" class="">e.g.<br/>○ iris -- <br/><a href="https://www.openml.org/d/61">https://www.openml.org/d/61</a><br/>○ penguins -- <br/><a href="https://www.openml.org/d/42585">https://www.openml.org/d/42585</a><br/>○ diabetes -- <br/><a href="https://www.openml.org/d/37">https://www.openml.org/d/37</a><br/>○ banknotes -- <br/><a href="https://www.openml.org/d/1462">https://www.openml.org/d/1462</a></p></div></figure><ul id="f22651e5-da72-4705-9609-400fbccbb625" class="toggle"><li><details><summary>Tools</summary><ul id="466c5d02-7489-4a26-b5e8-5072348bff33" class="bulleted-list"><li style="list-style-type:disc">python -language</li></ul><ul id="8ca43db8-d275-45a2-a7e8-1ebba1b3249f" class="bulleted-list"><li style="list-style-type:disc">numpy -lib in python for data structure</li></ul><ul id="d7d0347e-7199-4591-9f7c-f2eb11c68ba5" class="bulleted-list"><li style="list-style-type:disc">sklearn -open source machine learning algorithms </li></ul><ul id="3b60d79a-6820-4f14-a7eb-264d52a3863b" class="bulleted-list"><li style="list-style-type:disc">pandas -data manipulation library </li></ul><ul id="4ad13f1b-ce12-4a96-8d3a-e3522ef66363" class="bulleted-list"><li style="list-style-type:disc">matplotlib -lib for data visualisations </li></ul><ul id="00800bc4-ef88-4ee0-a75f-438543a2a0c2" class="bulleted-list"><li style="list-style-type:disc">Jupyter notebook -web based ide</li></ul></details></li></ul><ul id="32a4bbc1-6202-4a02-8c90-dc8b1787677c" class="toggle"><li><details><summary>Dataset eg</summary><figure id="e1aa3391-82ec-4466-be2c-da2b568dba01" class="image"><a href="Untitled%202.png"><img style="width:1538px" src="Untitled%202.png"/></a></figure><ul id="4bd239e0-c0da-4242-806e-4d35f2cd820f" class="bulleted-list"><li style="list-style-type:disc">x is the input, y is the output</li></ul><h3 id="f17a33c5-2bc9-42b3-8d82-f112896dfba3" class="">Data as “vectors” in “space”</h3><figure id="eef99f58-02f2-4014-af02-c27fafc0b9e9" class="image"><a href="Untitled%203.png"><img style="width:1536px" src="Untitled%203.png"/></a></figure><figure id="feb2900f-90b2-427a-93c8-1b7ff057c85a" class="image"><a href="Untitled%204.png"><img style="width:1504px" src="Untitled%204.png"/></a></figure><ul id="48c84cf5-7f9d-4b9f-aa9e-40f6049153a8" class="bulleted-list"><li style="list-style-type:disc">a row can be a point in data space</li></ul><figure id="4a250109-832c-4ada-9b87-eedb8b5dc396" class="image"><a href="Untitled%205.png"><img style="width:1470px" src="Untitled%205.png"/></a></figure><ul id="e6130dce-e705-46e8-beb8-f26d7c2548f1" class="bulleted-list"><li style="list-style-type:disc">as humans its hard to visulize more than 3 dimensional, so for more than that in dimensional space we can use graphs </li></ul><h3 id="720bba51-82bf-4884-93e4-e2f1bf70ceae" class="">more than 3 dimensional </h3><figure id="3a01964d-8244-41c1-aa54-b37a0860099b" class="image"><a href="Untitled%206.png"><img style="width:1572px" src="Untitled%206.png"/></a></figure></details></li></ul></details></li></ul></details></li></ul><ul id="918349a5-420d-49c9-96eb-ad1038d5793e" class="toggle"><li><details><summary>ML Tasks</summary><ul id="79f83820-4237-413f-8767-ee9055c916f1" class="toggle"><li><details><summary>What a machine learning task isn’t </summary><ul id="ac697426-068d-497f-aae6-c581a285dc8d" class="toggle"><li><details><summary>Application </summary><figure id="cea8fd8e-1501-4992-844a-3a4bf0f443b4" class="image"><a href="Untitled%207.png"><img style="width:846px" src="Untitled%207.png"/></a></figure><ul id="efb27be4-e039-4291-b42c-f0b3b5d1e7f8" class="bulleted-list"><li style="list-style-type:disc">this is not a machine learning task</li></ul></details></li></ul><ul id="cbeea20c-ddb9-46dc-8d48-479602159136" class="toggle"><li><details><summary>ML Process</summary><h3 id="243d5616-0dd3-455a-ac15-e3ef0a17358d" class="">Business oriented diagram </h3><figure id="5d324f00-823c-464b-8be5-3e0e4e4272bf" class="image"><a href="Untitled%208.png"><img style="width:1606px" src="Untitled%208.png"/></a></figure><ol type="1" id="d7d28007-0fa2-4f74-b8f7-8bd662e88e69" class="numbered-list" start="1"><li>frame the problem<ol type="a" id="d8501142-dc39-4878-80a7-36901dac2bdb" class="numbered-list" start="1"><li>form a team and discuss the real word problems</li></ol><ol type="a" id="a77185b1-dc3c-4a11-8e55-c3ee4a65eac7" class="numbered-list" start="2"><li>find a ML <mark class="highlight-red">task</mark></li></ol></li></ol><ol type="1" id="ba7f20eb-8d95-4031-aa47-3177d5fb31b8" class="numbered-list" start="2"><li>collect raw data<ol type="a" id="3353895b-2012-40e1-a55e-1314369f0249" class="numbered-list" start="1"><li>can be collected in an unstructured form (eg: websites, databases ect)</li></ol><ol type="a" id="d2f1f5c6-4f5f-4dab-aa1c-f3c2b6a1ca0d" class="numbered-list" start="2"><li>then we can structure it by formatting it into csv, json, xml ect</li></ol></li></ol><ol type="1" id="97844cd4-96ea-403e-8b70-47423f391c04" class="numbered-list" start="3"><li>process data<ol type="a" id="ef76fa0a-4c4d-4cf0-9b14-8dd53e214a03" class="numbered-list" start="1"><li>high level processing</li></ol><ol type="a" id="29bb0661-b2d1-42bc-88fd-1d4c20738dfa" class="numbered-list" start="2"><li>can be seen as a table, column is a feature, rows are points</li></ol><ol type="a" id="b7b5d0dd-2b81-47a1-81d6-1efa34dce4eb" class="numbered-list" start="3"><li>we want to <mark class="highlight-red">understand the feature</mark> of each column</li></ol><ol type="a" id="2bcca2aa-bc6b-4f73-8d1a-770c7d834c98" class="numbered-list" start="4"><li>we want to find out which ones are numerical or categorical </li></ol><ol type="a" id="2c502fe9-ab31-4718-9c1a-79843ccedfb6" class="numbered-list" start="5"><li>we can <mark class="highlight-red">“clean” the data </mark>by removing data which are corrupted, error prone, missing values ect</li></ol></li></ol><ol type="1" id="d6afabbf-a41d-41d2-9f46-e27af46eff56" class="numbered-list" start="4"><li>explore the data<ol type="a" id="6f91a52b-bc16-45bf-b03d-8631f18befcd" class="numbered-list" start="1"><li>feature engineering </li></ol></li></ol><ol type="1" id="d9d33f51-fb1b-4111-b880-bb76b990512f" class="numbered-list" start="5"><li>in-depth analysis<ol type="a" id="26ac7063-e29f-4dd7-8bf4-b331a8cb303e" class="numbered-list" start="1"><li>get the knowledge from the data and find a way to mathematically to represent the knowledge </li></ol><ol type="a" id="5307f9f9-78a3-44bf-bc75-c6d3617b4e6f" class="numbered-list" start="2"><li>eg: predictive models </li></ol><ol type="a" id="00c7b39a-8f13-4658-aa9e-391d6364f91e" class="numbered-list" start="3"><li>identify the best model and refine it</li></ol></li></ol><ol type="1" id="b1ced276-e290-4152-b1cb-535b3e5686be" class="numbered-list" start="6"><li>communicate results<ol type="a" id="ecbcba01-b557-4e37-bfb7-c3f405c25e64" class="numbered-list" start="1"><li>monitor the model and test day to day use</li></ol><ol type="a" id="89d1b8b7-81fb-43ff-a8eb-ca8bc9da8e76" class="numbered-list" start="2"><li>when the model works and when it fails</li></ol><ol type="a" id="020f064b-7464-4a0f-ba7b-883afb3e6d99" class="numbered-list" start="3"><li>then we can go back to step 1</li></ol></li></ol><h3 id="9732e157-3f14-4a13-9d14-0584937c90fa" class="">Technological oriented Process</h3><figure id="253a06c1-f5c3-4c00-a65f-03b00cf9ae99" class="image"><a href="Untitled%209.png"><img style="width:1774px" src="Untitled%209.png"/></a></figure><ol type="1" id="29a0f7a9-72ce-4fb6-9099-0c27d8a6b007" class="numbered-list" start="1"><li>access and load the data<ol type="a" id="47454ba6-b290-4233-8412-9c58a8fbbeaf" class="numbered-list" start="1"><li>python lib panda </li></ol></li></ol><ol type="1" id="e74531b0-88d7-4f5f-a2c6-e0b34e92c77c" class="numbered-list" start="2"><li>preprocess the data<ol type="a" id="b8670cd1-8dea-4adc-af7b-9cd29ff74dce" class="numbered-list" start="1"><li>panda </li></ol><ol type="a" id="518b5e9a-ead6-4b94-b644-3792f053d8b9" class="numbered-list" start="2"><li>clean data</li></ol></li></ol><ol type="1" id="78102737-fa5d-4738-ac86-529a87b945d9" class="numbered-list" start="3"><li>derive feature <ol type="a" id="2070eafb-cd22-4398-aeee-102b776714e6" class="numbered-list" start="1"><li>feature engineering</li></ol><ol type="a" id="ec0a7b9f-ad1a-4a17-8d57-6493cd896a11" class="numbered-list" start="2"><li>tackle the curse of dimensionality (representing the data as 2d or 3d for data which is more than 3d) <ol type="i" id="56b393ce-bd22-45fa-a339-6b271d71cf59" class="numbered-list" start="1"><li>done by reduce the number of features</li></ol></li></ol></li></ol><ol type="1" id="8bbe1cc5-005b-43f5-bc99-41071bf29cf9" class="numbered-list" start="4"><li>train the model<ol type="a" id="5108f082-6ef0-4fb2-8fd5-0b198135221b" class="numbered-list" start="1"><li>using the feature derived from step 3</li></ol><ol type="a" id="49b901fd-8004-46d6-b447-4e61f6e52b50" class="numbered-list" start="2"><li>sklearn python lib<ol type="i" id="4e579dd1-eb8a-425f-b116-f27f9bc73aff" class="numbered-list" start="1"><li>also helps do feature engineering</li></ol></li></ol></li></ol><ol type="1" id="b00b97d0-3d26-45af-8a8c-84a0636ecbfc" class="numbered-list" start="5"><li>iterative process of find the best model <ol type="a" id="76474b7a-48a4-4f9b-a8d9-d772c2f5f2d0" class="numbered-list" start="1"><li>fine tuning the model and hyper parameter</li></ol><ol type="a" id="05a558b5-9c53-46e4-9d1c-d7244dffbd0d" class="numbered-list" start="2"><li>can be automated by AutoML tech</li></ol></li></ol><ol type="1" id="f05c9365-703d-4a9f-8828-0430b53b14fe" class="numbered-list" start="6"><li>integration <ol type="a" id="6c958601-2df0-4fe9-b807-632c7f802f2c" class="numbered-list" start="1"><li>best trained model into a production server</li></ol><ol type="a" id="245c7352-d588-4069-9928-f0610a39fa8e" class="numbered-list" start="2"><li>model server<ol type="i" id="fb5f46b6-d3f2-4f16-9d09-16ba76741459" class="numbered-list" start="1"><li>can deploy changes into model server rather than the application server to not interfere with the user experiences </li></ol></li></ol><ol type="a" id="682b74e8-fdbf-4d1f-8593-4a84ddc9ac93" class="numbered-list" start="3"><li>application server allows user to access the model</li></ol></li></ol></details></li></ul></details></li></ul><ul id="f49fedeb-8c0d-46c7-8640-feda1a008b21" class="toggle"><li><details><summary>What is it</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="3e52b12a-5515-4d2d-911e-6144d3442c12"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a <mark class="highlight-red">problem</mark> that can be <mark class="highlight-red">solved by machine learning</mark><figure id="bb8ea4fb-25f3-4a42-ad35-3f5ceedbefa0" class="image"><a href="Untitled%2010.png"><img style="width:1414px" src="Untitled%2010.png"/></a></figure><ul id="45ef9958-304d-45a2-9bf7-15680567fe29" class="bulleted-list"><li style="list-style-type:disc">supervised learning vs unsupervised learning<ul id="8c7f999e-2f2d-4ca9-afaa-f5231a17131e" class="bulleted-list"><li style="list-style-type:circle">supervised learning is when we’re working with labeled data set<ul id="1b2b1751-51de-4797-a0b0-b67cacaf1a20" class="bulleted-list"><li style="list-style-type:square">used for classification and regression </li></ul><ul id="bd22babb-f2bc-45cd-a427-ffdb9ddb08cc" class="bulleted-list"><li style="list-style-type:square">environment contains a teacher that provides correct responses for certain environment states </li></ul><ul id="d6732cce-117a-4410-a196-ad10de80a775" class="bulleted-list"><li style="list-style-type:square">classification the label associated with each data point is discrete</li></ul><ul id="4dfe57d5-84a6-453f-9707-b02c56f5508b" class="bulleted-list"><li style="list-style-type:square">regression is when the label associated with each data point is continuous </li></ul><ul id="d3e0506d-6949-4df1-b01d-6b6854f7dc48" class="bulleted-list"><li style="list-style-type:square">objective in nature</li></ul><ul id="ea0cf2b6-eba0-48b3-9e64-db41a65cf3d9" class="bulleted-list"><li style="list-style-type:square">we are solving a single step decision problem <ul id="68c76383-ee2a-4a7a-a93d-3c0c04431841" class="bulleted-list"><li style="list-style-type:disc">each problem has one decision (is it a cat or dog)</li></ul></li></ul></li></ul><ul id="2b7e97f7-dbc4-4ab1-ab72-9013029213b3" class="bulleted-list"><li style="list-style-type:circle">unsupervised is when working with unlabelled data set <ul id="5406aee1-72e6-4ddb-b900-0ff4e1ecc52f" class="bulleted-list"><li style="list-style-type:square">clustering is when the unlabelled data which are simliar to each other will be closer together in clusters and ones that are different are further away</li></ul><ul id="357772e5-47b4-46d4-abc8-8c2bc9a69a8b" class="bulleted-list"><li style="list-style-type:square">doesn’t have a teacher that gives correct answers in the task environment</li></ul><ul id="8ecfe21b-b0df-4623-b391-d1138a1e8d45" class="bulleted-list"><li style="list-style-type:square">subjective, ill-posed </li></ul></li></ul></li></ul><ul id="ecf22663-feba-499b-b40f-4a9509b820cc" class="bulleted-list"><li style="list-style-type:disc">there is also self supervised learning and reinforcement learning ect</li></ul><ul id="2a66d91d-9507-49b7-8235-6fd3783cf704" class="bulleted-list"><li style="list-style-type:disc">reinforcement learning<ul id="69a09b76-fc46-4d6b-a418-5ffb0d2a241f" class="bulleted-list"><li style="list-style-type:circle">reinforcement learning is sequential decision problems </li></ul></li></ul></div></figure><ul id="85d84549-f6b2-4d07-8be7-27d66d2873ff" class="bulleted-list"><li style="list-style-type:disc">there are two components in ML tasks<ul id="fa2c53e0-e74d-404d-8ae7-8b19ffbe324e" class="bulleted-list"><li style="list-style-type:circle">agent </li></ul><ul id="22876592-d52c-4dd8-ad0e-8e31a81aec70" class="bulleted-list"><li style="list-style-type:circle">environment</li></ul><ul id="e7c8fa4d-bc5a-4b4f-9798-d892911f51c2" class="bulleted-list"><li style="list-style-type:circle">the interaction between the agent and the environment gives us the ML task</li></ul></li></ul></details></li></ul><ul id="f5b6b320-44d1-4a28-b106-dc5777128f93" class="toggle"><li><details><summary>3 way split </summary><p id="8dfdf6fb-7bca-4c18-88c3-45a426e9141d" class="">3 way split of supervised, unsupervised and reinforcement learning</p><figure id="77af9ed3-8f4d-4591-989c-2624131df44d" class="image"><a href="Untitled%2011.png"><img style="width:1684px" src="Untitled%2011.png"/></a></figure></details></li></ul><ul id="250b30bd-7b4e-428f-bcd2-b7cbf701db72" class="toggle"><li><details><summary>Outputs</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="c7a19536-c0dd-4c94-9a38-77804455ddb9"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">outcomes from ML can be simple or more information<ul id="ad843f12-2e2e-4bb3-81e0-eefd31dadff9" class="bulleted-list"><li style="list-style-type:disc">different outcomes can have different amount of information content</li></ul><ul id="89e53417-a9e5-4d37-99ff-c37e0788f82d" class="bulleted-list"><li style="list-style-type:disc">eg: classifier classifies some novel input patterns <figure id="69ee8c92-65bf-449d-8705-4a2033ea2a5a" class="image"><a href="Untitled%2012.png"><img style="width:1398px" src="Untitled%2012.png"/></a></figure><ul id="5ebd5c95-346b-4cdf-a452-22ea31a10859" class="bulleted-list"><li style="list-style-type:circle">classification task (eg: rating a movie) can be turned into a regression task in the scoring process<p id="13300d73-ee6e-4e34-a9dd-2d6081d603ba" class="">10/10 is classification, 9.9 is regression </p></li></ul></li></ul></div></figure></details></li></ul><ul id="cfadd2d5-a5fb-4130-968d-0a43c61b4879" class="toggle"><li><details><summary>The main divisions within learners</summary><figure id="c38c8e72-3521-4130-b15b-dda63d651c13" class="image"><a href="Untitled%2013.png"><img style="width:1338px" src="Untitled%2013.png"/></a></figure></details></li></ul></details></li></ul><ul id="9640c865-e1b1-4bf3-9771-67c681f1a876" class="toggle"><li><details><summary>ML Data</summary><ul id="cbad2c94-4f4c-4b1e-ba16-7a1638429117" class="toggle"><li><details><summary>Real World Application for ML </summary><ul id="066678dd-424c-4403-8946-bcb6e8605eca" class="toggle"><li><details><summary>Examples</summary><h3 id="2d37f621-7c42-4f7a-a547-bdb7f5fa2c2f" class="">Machine Learning in Netflix</h3><p id="8f9a8312-eba3-4a32-9346-3746d69fb2c1" class="">machine learning is used in netflix for recommending movies</p><ul id="40ab0949-f11b-4a3f-9cc2-f9ba8388b6b3" class="bulleted-list"><li style="list-style-type:disc">it uses data from the user and other users to recommend movies similar to what you like and what other people also like</li></ul><figure id="bdec9658-09fb-461c-b336-b6d276f007c9" class="image"><a href="Untitled%2014.png"><img style="width:601px" src="Untitled%2014.png"/></a></figure><h3 id="95c4c0f6-3579-404c-8740-e101588bd6da" class="">Machine Learning in Retail</h3><figure id="f2525829-b0db-4121-b00a-8f52d0a32ba7" class="image"><a href="Untitled%2015.png"><img style="width:624px" src="Untitled%2015.png"/></a></figure></details></li></ul><ul id="b81a90bd-ce6f-4346-86d7-11665c500143" class="toggle"><li><details><summary>Typical Applications </summary><figure id="4845a1a2-f53b-42de-b743-55a316f9b31a" class="image"><a href="Untitled%2016.png"><img style="width:624.02783203125px" src="Untitled%2016.png"/></a></figure><ul id="63572b4a-dadd-43a7-9a06-67ddee78742c" class="bulleted-list"><li style="list-style-type:disc">used in healthcare/medicine such as cancer </li></ul></details></li></ul></details></li></ul><ul id="cfab554f-ae0e-48d5-94f5-1750f8a04227" class="toggle"><li><details><summary>From Data to Information</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="363d959c-e075-41ed-a34a-33768a1c5eaa"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Data is <em>potentially </em>valuable, but raw data is useless.<br/>we need techniques to extract information/patterns from data for it to be used<br/></div></figure><h3 id="839fcafe-4fef-4140-b478-afd7e9769d3f" class="">Data vs Information</h3><ul id="99bde089-306f-4770-b388-82f39c89dde9" class="bulleted-list"><li style="list-style-type:disc">Data records facts</li></ul><ul id="c86b50e3-f5fa-41d1-950e-58259e85dc5f" class="bulleted-list"><li style="list-style-type:disc">Information: patterns underlying data</li></ul><figure id="8928acac-9f7a-4744-8f16-29d232e05000" class="image"><a href="Untitled%2017.png"><img style="width:652.013916015625px" src="Untitled%2017.png"/></a></figure><h3 id="18caee3b-4e19-4a03-8fc2-5d95b1a3ac8b" class="">Data Mining vs Machine Learning</h3><ul id="f3111404-5b5c-4b1b-a928-60bc204f4b97" class="bulleted-list"><li style="list-style-type:disc">data mining is the process of extracting information/patterns from data by analyzing</li></ul><ul id="95e1a5dd-8f49-4c85-b001-ef9b1d3e9a9a" class="bulleted-list"><li style="list-style-type:disc">machine learning extracts information/patterns and then uses it to make predictions</li></ul><p id="02d1ef3a-ec04-42c4-ba8e-ea4f12d9af58" class="">they are similar but are not interchangeable, sometimes machine learning is used as apart of a datamining project</p><figure id="f4653713-c1a6-4414-a00a-de77375c612a" class="image"><a href="Untitled%2018.png"><img style="width:652.013916015625px" src="Untitled%2018.png"/></a></figure></details></li></ul><ul id="cd955f9f-fa12-4f4a-a530-67034ce369ff" class="toggle"><li><details><summary>Cross Industry Standard Process for Data Mining (CRISP-DM)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="316990f2-2b1e-450a-9d52-bb84cdd68c2d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a data mining “nonproprietary standard methodology” / process by data analysists in 1996<ul id="79c0c624-9353-4e37-9627-44471479101b" class="bulleted-list"><li style="list-style-type:disc">has 6 phases with dependencies to other phases</li></ul><ul id="eb45124c-edac-4c38-9b8f-00f4a118efcb" class="bulleted-list"><li style="list-style-type:disc">order of phases is flexible (can revisit phases and be customized )</li></ul><figure id="701b2a3a-84f7-4dc2-adda-979e5e6f924d" class="image"><a href="Untitled%2019.png"><img style="width:592.0486450195312px" src="Untitled%2019.png"/></a></figure></div></figure><h3 id="485c771b-f0b4-4acb-bdf4-028a0dd04c0b" class="">Why Successful?</h3><ul id="ede88bb0-925f-4b30-b463-a85c690750c5" class="bulleted-list"><li style="list-style-type:disc">its simple and structured</li></ul><ul id="0f6a4bcb-76a3-4d8f-afd5-57945316da3f" class="bulleted-list"><li style="list-style-type:disc">easy to implement</li></ul><ul id="9f3a70f4-3eec-40bc-a77e-6c5facf68edb" class="bulleted-list"><li style="list-style-type:disc">“domain agnostic”: works for industry and research communities</li></ul><h3 id="175b0f4c-11dd-436d-9c5a-e3b070e2d3c1" class="">Phase 1: Business Understanding</h3><p id="56ea8a27-c09e-43bd-a27d-5d5b5c59311a" class="">identify specific goals and problems the business wish to solve</p><p id="4b11e62d-95ba-42f1-af02-d4676c63d79d" class="">tasks:</p><ol type="1" id="d3e36e3e-a5cf-4649-a15d-0435f434b05f" class="numbered-list" start="1"><li>determine business objectives</li></ol><ol type="1" id="22998613-dbe4-40f1-915d-986cffa287b1" class="numbered-list" start="2"><li>assess situation (resources availability , project requirements, assess risks)</li></ol><ol type="1" id="ed61f5c9-0789-4187-8b37-a7dfe0287418" class="numbered-list" start="3"><li>determine/converting business objectives to data mining goals</li></ol><ol type="1" id="b4f89d45-e86d-4395-b7ec-b976f8b3e7fb" class="numbered-list" start="4"><li>produce project plan</li></ol><h3 id="7efec1db-7d8f-4f76-a79a-62b5023d49b3" class="">Phase 2: Data Understanding</h3><p id="daa8b66b-db88-44c6-b4bf-7e8426dfe43d" class="">Take a closer look at the data, access and explore the data, match between the business problem and the data</p><p id="9451ab96-2346-4493-bfec-12bad51d2019" class="">tasks:</p><ol type="1" id="4efccf89-c3ed-4cde-ab32-15d5ec5e54b2" class="numbered-list" start="1"><li>Collect initial data</li></ol><ol type="1" id="e8c71f70-97ee-4e49-bbf4-4f48af309f22" class="numbered-list" start="2"><li>Describe data</li></ol><ol type="1" id="8b253e80-a466-4ea3-a9d9-8a046f4d5f35" class="numbered-list" start="3"><li>Explore data</li></ol><ol type="1" id="2d29aafa-6b54-48d0-9d0d-297de9174722" class="numbered-list" start="4"><li>Verify data quality</li></ol><h3 id="1bad082c-2921-4b84-94a6-6281bf3ac97f" class="">Phase 3: Data Preparation/Pre-processing</h3><p id="47fc2356-e753-4700-9c1c-7e20ed62be35" class="">prepare the final data set(s) for modelling</p><ul id="8f1f3b5d-62f9-4b42-9ca4-696628d856ad" class="bulleted-list"><li style="list-style-type:disc">most important phase (garbage im, garbage out)</li></ul><ul id="9180f1bb-ebdd-4d6b-ab07-211e8136d84f" class="bulleted-list"><li style="list-style-type:disc">take about 80% of the project time/effort</li></ul><p id="92271953-55ba-4456-8f7d-c9f6ab99e45d" class="">tasks:</p><ol type="1" id="e62b6d09-3144-4779-90bc-e33d1ca38ee5" class="numbered-list" start="1"><li>Data Selection</li></ol><ol type="1" id="0a919765-854c-4bba-b42b-6b2b5c8052f3" class="numbered-list" start="2"><li>Data Cleaning</li></ol><ol type="1" id="dac24ec9-707c-49e3-aa88-3accf6561b57" class="numbered-list" start="3"><li>Data Construction</li></ol><ol type="1" id="b9bcbd95-4622-470e-86d0-ee554d31e835" class="numbered-list" start="4"><li>Integrate data</li></ol><ol type="1" id="027cea5b-c8e6-4bf0-8c40-b8c62e4709dc" class="numbered-list" start="5"><li>Format data</li></ol><h3 id="758adb10-395c-4b4d-b697-76769509fa46" class="">Phase 4: Model Building</h3><p id="b0f8b743-f46f-4558-b38a-b5c1723bd591" class="">Build and assess various models based on several different<br/>modeling techniques<br/></p><figure id="8e09d35f-8089-4089-9309-a9d71f12cf84" class="image"><a href="Untitled%2020.png"><img style="width:384px" src="Untitled%2020.png"/></a></figure><ol type="1" id="ee2cc0ee-6731-43aa-9687-3eb1a83e5d30" class="numbered-list" start="1"><li>Select modelling technique</li></ol><ol type="1" id="a4f7580a-3c5d-419e-b110-df82613fe10e" class="numbered-list" start="2"><li>Generate test design</li></ol><ol type="1" id="8533cc87-0842-4513-9b76-66e301bebe9c" class="numbered-list" start="3"><li>Build model</li></ol><ol type="1" id="a2635fba-5b9c-4726-9a31-ec93ad8deb90" class="numbered-list" start="4"><li>Assess model</li></ol><h3 id="015602b5-863d-430e-a8ed-58fa9372fb44" class="">Phase 5: Model Evaluation</h3><p id="4e735f3d-2197-49bd-a280-a98990e1d877" class="">Evaluate and determine which model best meets the business and what to do next</p><ol type="1" id="330a9c92-de23-4159-bad2-7c082d463591" class="numbered-list" start="1"><li>Evaluate results<ol type="a" id="e76f3f65-7a85-4bb3-af34-ab1d9242f7a0" class="numbered-list" start="1"><li>assesses model meets business objectives</li></ol></li></ol><ol type="1" id="953e58d6-0b34-4396-b882-a76742294327" class="numbered-list" start="2"><li>Review process<ol type="a" id="633029df-448e-4ea6-a47f-38fe8e808628" class="numbered-list" start="1"><li>do a more thorough review of the data mining engagement , also cover quality assurance issues</li></ol></li></ol><ol type="1" id="e096c184-1041-4de7-abe9-3f874563b618" class="numbered-list" start="3"><li>Determine next steps<ol type="a" id="273240dd-8e22-43e1-a74d-5b511aa96773" class="numbered-list" start="1"><li>decide how to proceed depending on the results of the assessment and the process review</li></ol></li></ol><h3 id="e7189e09-de45-4621-a0aa-6a6cbb01a1ca" class="">Phase 6: Deployment</h3><p id="bc5fb3b9-a0ec-4abc-bd36-22c4a177cfa2" class="">process of using new insights to make improvements, a formal integration of model, use the insights gained from data mining to make change</p><ol type="1" id="5294d3f2-ee06-4dc0-9920-aa9d509de1c9" class="numbered-list" start="1"><li>Plan deployment</li></ol><ol type="1" id="405d8a9c-7099-4270-b928-9d0d288ef43a" class="numbered-list" start="2"><li>Plan monitoring and maintenance</li></ol><ol type="1" id="220963b5-4dc4-4722-80e9-c755aa1b2181" class="numbered-list" start="3"><li>Produce final report</li></ol><ol type="1" id="52d53c91-f922-4b92-86de-378d3491599f" class="numbered-list" start="4"><li>Review project</li></ol></details></li></ul><ul id="42b5ffd7-98c9-445d-92e3-ec72460c8262" class="toggle"><li><details><summary>Exploratory Data Analysis (EDA)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="56fe7f68-c3b5-4ed6-8f4a-e5c9db6c4666"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">EDA is an iterative process for summarizing, visualizing and understanding characteristics of data<figure id="60cb1b58-3394-405c-893e-389074d70ca8" class="image"><a href="Untitled%2021.png"><img style="width:592px" src="Untitled%2021.png"/></a></figure><blockquote id="6cdde9c2-ab13-44e1-83e1-545f8e1425d0" class="">helps you choose the most suitable algorithm for your dataset and ensures you are ready to use ML techniques in a new project by analyzing the data</blockquote></div></figure><ul id="bc5fcbd4-0250-4fee-845e-928206d3f422" class="toggle"><li><details><summary>Key Concepts of Exploratory Data Analysis</summary><h3 id="4574ee8e-d1ac-4ccc-9a66-8801a6058b7b" class="">4 Objectives of EDA</h3><ul id="32243d09-5daf-4196-be7f-0dd7b4460ec7" class="bulleted-list"><li style="list-style-type:disc">Discover Patterns</li></ul><ul id="fb6457e4-58e4-4645-b774-860f86bd706c" class="bulleted-list"><li style="list-style-type:disc">Spot Anomalies (outliers)</li></ul><ul id="f8ef836c-495c-4b5a-89b1-a65196b37628" class="bulleted-list"><li style="list-style-type:disc">Make Hypothesis based on data</li></ul><ul id="5746dc5e-6bcc-4688-b6f7-7c5eabcd0e8f" class="bulleted-list"><li style="list-style-type:disc">Check Assumptions</li></ul><h3 id="7c0a9a9e-996c-40e1-8ffb-271cbdaa1834" class="">Stuff done during EDA</h3><ul id="386797d3-16c9-4e53-a526-362bf088f8dd" class="bulleted-list"><li style="list-style-type:disc">Measures of central tendency (mean, median, mode)</li></ul><ul id="c9180c27-fab3-4df2-9339-6c5423de9f04" class="bulleted-list"><li style="list-style-type:disc">Spread measurement (standard deviation, variance)</li></ul><ul id="bfb28988-d801-48a5-b36a-e6b515742a22" class="bulleted-list"><li style="list-style-type:disc">Shape of distribution (distribution, trends)</li></ul><ul id="12b1317c-0c66-4d97-8fad-deedd0041ea3" class="bulleted-list"><li style="list-style-type:disc">Find/Remove Outlier</li></ul><ul id="e4cbc092-2722-413f-af3f-5aa10e8291f2" class="bulleted-list"><li style="list-style-type:disc">Correlations</li></ul><ul id="c00dbbc7-fbc3-4a84-b0e4-6c5ad316e8f9" class="bulleted-list"><li style="list-style-type:disc">Visual Exploration</li></ul></details></li></ul><ul id="91687503-59d1-441b-9d14-41370d0f0a09" class="toggle"><li><details><summary>Making Sense of Data – Distinguish Types of Attributes</summary><figure id="b2a7ede2-ebcb-4ad8-8612-c19822c38ec4" class="image"><a href="Untitled%2022.png"><img style="width:751px" src="Untitled%2022.png"/></a></figure></details></li></ul><ul id="7a0a2e14-d1bd-404d-b699-71cddd10fe88" class="toggle"><li><details><summary>Types of EDA Methods</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="2e8cccbd-9eb8-459c-896d-54d51ce651ec"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">classified into two ways <h3 id="51c7ef38-8fc7-4d1f-8f76-b9ee9962ae2b" class="">graphical or non-graphical/quantitative</h3><ul id="f30534cb-9ff3-49b8-a347-aacc8829db34" class="bulleted-list"><li style="list-style-type:disc">graphical: summarizing data in a <mark class="highlight-blue_background">visual </mark>way (graphs) easy to understand</li></ul><ul id="1e38426f-bc83-4618-8404-36268b718d99" class="bulleted-list"><li style="list-style-type:disc">quantitative: <mark class="highlight-blue_background">calculation </mark>of summary statistics</li></ul><h3 id="75598200-dc9e-40b0-a20b-749e22c78217" class="">univariate or multivariate</h3><ul id="f2da8d6a-9026-4329-9cab-c9f15f9dfa20" class="bulleted-list"><li style="list-style-type:disc">univariate: <mark class="highlight-blue_background">statistics for each</mark> feature/attribute</li></ul><ul id="2b91f6aa-c068-4664-9198-1d81d2156616" class="bulleted-list"><li style="list-style-type:disc">multivariate: <mark class="highlight-blue_background">find relationship between</mark> features</li></ul><figure id="bf92a033-dea0-44c8-b2be-98b529b42c07" class="image"><a href="Untitled%2023.png"><img style="width:624px" src="Untitled%2023.png"/></a></figure></div></figure></details></li></ul><ul id="f32c59ae-34d1-4ca9-8e5d-67ba454236ba" class="toggle"><li><details><summary>How to do EDA</summary><ol type="1" id="0ef113c9-0a63-4633-849d-8d00884e7923" class="numbered-list" start="1"><li><mark class="highlight-blue_background">identification </mark>of <mark class="highlight-blue_background">variables </mark>and data <mark class="highlight-blue_background">types</mark> (nominal, categorical, Discrete or Continuous)</li></ol><ol type="1" id="7f45edaf-80bf-43e6-b032-013fe9e8fea2" class="numbered-list" start="2"><li>non-graphical and graphical univariate analysis <ul id="3cc26acb-253d-42de-a644-a8cd6e51a34e" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-blue_background">analysis each feature independently</mark></li></ul></li></ol><ol type="1" id="3f137d3e-4328-4ca4-870a-28d0a1ac5540" class="numbered-list" start="3"><li>bi-/multivariate analysis, correlation analysis <ul id="724281e2-a4c8-41ab-bf4b-77739ee7c7fc" class="bulleted-list"><li style="list-style-type:disc">find the relationship between features</li></ul></li></ol><ol type="1" id="2224ac01-6e74-4ec9-8361-5f5ad499956f" class="numbered-list" start="4"><li>detect missing values and anomalies</li></ol><ol type="1" id="59a18c98-11f1-445e-9a7a-6150d54244ad" class="numbered-list" start="5"><li>detect outliers</li></ol><figure id="a259091f-7a0c-48d6-8687-1612a49f50d5" class="image"><a href="Untitled%2024.png"><img style="width:1111px" src="Untitled%2024.png"/></a></figure></details></li></ul></details></li></ul><ul id="73ca79d9-a874-4acb-aa16-0340976ab5f9" class="toggle"><li><details><summary>Visualization</summary><ul id="1b110359-2b65-47d3-a50a-0de860ac3c31" class="toggle"><li><details><summary>Types of graphs</summary><ul id="803f4d69-06d7-4a01-98ad-eb89d4307785" class="toggle"><li><details><summary>Composition</summary><ul id="6caebac1-6909-42dc-a018-3cff0684ee19" class="toggle"><li><details><summary>Pie Chart</summary><table id="f692ece0-c5ff-4e2f-95f2-c79c6622b248" class="simple-table"><tbody><tr id="e0a14f05-7de8-44c6-9aab-1c41dc5dedbb"><td id="`&lt;L;" class="">pros</td><td id="IQvT" class="">cons</td></tr><tr id="87173047-8303-42d4-98ff-5262e064f186"><td id="`&lt;L;" class="">easy to understand</td><td id="IQvT" class="">difficult to compare a few pieces</td></tr><tr id="22a3e186-9251-4bc8-abd1-2ac276b7c9d3"><td id="`&lt;L;" class="">understand info quickly</td><td id="IQvT" class="">unhelpful when observing trends over time</td></tr><tr id="e7f980e5-bee9-4869-8760-d843c21c1290"><td id="`&lt;L;" class=""></td><td id="IQvT" class="">hard to read when there are a lot of categoires</td></tr></tbody></table><figure id="7f3923bb-a220-4af9-8a90-390d3c8c78aa" class="image"><a href="Untitled%2025.png"><img style="width:568px" src="Untitled%2025.png"/></a></figure></details></li></ul></details></li></ul><ul id="918909ab-7730-4590-b9b9-177aa482cd7a" class="toggle"><li><details><summary>Comparison</summary><ul id="72f572ea-7e2c-49de-b7b4-63f85b9578f5" class="toggle"><li><details><summary>Bar &amp; Stacked Bar Chart</summary><figure id="9f89c892-9c99-4639-bb02-d45b693ef2c0" class="image"><a href="Untitled%2026.png"><img style="width:540px" src="Untitled%2026.png"/></a></figure></details></li></ul><ul id="7dd6210b-bba8-4990-a5ba-939013aa48cf" class="toggle"><li><details><summary>Area Chart/Stacked Chart (line chart)</summary><figure id="cc49a451-b163-4fcc-b501-d3b168d4aec6" class="image"><a href="Untitled%2027.png"><img style="width:540px" src="Untitled%2027.png"/></a></figure></details></li></ul></details></li></ul><ul id="a9e1d1af-ce6b-4b82-85b7-79078d72f72a" class="toggle"><li><details><summary>Distribution</summary><ul id="82119c27-8919-4825-9908-095c99ff04a7" class="toggle"><li><details><summary>Histogram</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="bad68bae-b107-42c5-af90-e05d74419a4a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">frequency distribution of numeric variables<p id="9df4e94e-8961-4296-b6e8-e858c756bc7e" class="">splitting values into small equal sized bins</p><ul id="86b61102-2552-4adf-8d92-4d8a98f3fcdb" class="bulleted-list"><li style="list-style-type:disc">shows you several information (central tendencies aka mean, data spread)</li></ul></div></figure><figure id="de46ade9-bdb3-47c9-ac1b-77c65ec3bb52" class="image"><a href="Untitled%2028.png"><img style="width:568px" src="Untitled%2028.png"/></a></figure></details></li></ul><ul id="1a592eb8-b8f6-40c2-96be-1e31416b32d5" class="toggle"><li><details><summary>Box Plot &amp; Violin Plot</summary><figure id="c7be939b-2d62-4273-8030-c831af47fa29" class="image"><a href="Untitled%2029.png"><img style="width:902px" src="Untitled%2029.png"/></a></figure></details></li></ul></details></li></ul><ul id="e4cafe50-e0f1-498e-b2e4-738bc22c5ae0" class="toggle"><li><details><summary>Relationship</summary><ul id="d160039e-96b0-4cc5-b0f8-48fa734661c4" class="toggle"><li><details><summary>Scatter Plot</summary><figure id="23a8693b-2554-45bd-9e8a-f84e867c4fa5" class="image"><a href="Untitled%2030.png"><img style="width:751px" src="Untitled%2030.png"/></a></figure></details></li></ul><ul id="276b8eac-6ac6-495f-9256-1f5453bf9068" class="toggle"><li><details><summary>Correlogram</summary><figure id="00bd902c-6ef3-469f-9a18-ce083bfb8f0f" class="image"><a href="Untitled%2031.png"><img style="width:642px" src="Untitled%2031.png"/></a></figure></details></li></ul><ul id="bb11adfe-ec10-4728-8102-fcdcda24d296" class="toggle"><li><details><summary>HeatMap</summary><figure id="34e3a538-dc29-4e9d-b715-1ede12b2353a" class="image"><a href="Untitled%2032.png"><img style="width:812px" src="Untitled%2032.png"/></a></figure></details></li></ul></details></li></ul></details></li></ul><ul id="82ed4255-2545-4dba-a7d4-82246764af00" class="toggle"><li><details><summary>Choose the Most Suitable Plots</summary><figure id="f7f46393-beb8-47ea-a922-385a75ecdb03" class="image"><a href="Untitled%2033.png"><img style="width:940px" src="Untitled%2033.png"/></a></figure></details></li></ul><ul id="bc416611-f161-416b-85bb-3d83fa7a726a" class="toggle"><li><details><summary>Data Shape</summary><h3 id="e392ae0c-6244-49fa-bddb-4ab727b86802" class="">Skewness</h3><p id="b31b9d61-a05a-4be5-b69b-a9b11d01cfaa" class=""> measure of the lack of symmetry</p><ul id="7c6d9358-62e1-4f5c-937a-618f64bd02dc" class="bulleted-list"><li style="list-style-type:disc">a distribution, or data set, is symmetric if it looks the same to the left and right of the center point (mean≈median≈mode)</li></ul><ul id="e114dd4b-41a7-4e95-9036-8b1fd7e2e95e" class="bulleted-list"><li style="list-style-type:disc">Positive vs Negative vs Zero skewness</li></ul><figure id="6f235380-9180-4199-962c-0543733d3038" class="image"><a href="Untitled%2034.png"><img style="width:568px" src="Untitled%2034.png"/></a></figure><h3 id="55ebbf1c-75b3-4b78-b0ad-751a23113e1a" class="">Kurtosis</h3><p id="0ba8f23c-c33b-4ced-961c-3fc1a52f4dae" class="">measures the degree to which the tails of a<br/>distribution differ from those of a normal distribution<br/></p><ul id="c0d80685-0c87-41b6-951f-3e037ed91360" class="bulleted-list"><li style="list-style-type:disc">important because most machine learning algorithm assume standard distribution </li></ul><figure id="53407f2f-1e45-4968-a00c-954c1c0bb7db" class="image"><a href="Untitled%2035.png"><img style="width:596px" src="Untitled%2035.png"/></a></figure></details></li></ul><ul id="dc28e427-8cc2-4cc8-b235-9ae5124d9103" class="toggle"><li><details><summary>Clustering Analysis for EDA</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="d38d9667-a5b9-4e55-bb60-11831db9fd2b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">clustering data to see any outliers or relationships between features<figure id="f75d7dea-d368-499f-8a15-75f6ac38d193" class="image"><a href="Untitled%2036.png"><img style="width:536px" src="Untitled%2036.png"/></a></figure></div></figure></details></li></ul><ul id="a84d50d3-a231-4b1d-9442-adf3f7a0e839" class="toggle"><li><details><summary>Dimensionality Reduction for EDA</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="8c6c3ce4-04cc-4a98-a91a-0ba4bfc1bb0d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">reducing high dimensional search spaces for humans to visualize/describing the relationship between features<figure id="8faa1b1a-d1d4-4f7a-9ecb-bd5d3ce38bca" class="image"><a href="Untitled%2037.png"><img style="width:949px" src="Untitled%2037.png"/></a></figure><ul id="9a5ed76c-ac1f-4770-8dd3-2a6250dfb284" class="bulleted-list"><li style="list-style-type:disc">PCA assumes linear relationship between features, if this isn’t true then use t-SNE</li></ul></div></figure></details></li></ul><ul id="9d7bcf5c-555c-4f4e-8c60-0952e33723b2" class="toggle"><li><details><summary>What to look for in your plots?</summary><ul id="c0a5859b-b906-466e-831f-27919b7b7e59" class="bulleted-list"><li style="list-style-type:disc">turn information into useful questions<ul id="22135b21-d56f-4fe5-92c1-89585e2bb1cf" class="bulleted-list"><li style="list-style-type:circle">which values are the most common and why?</li></ul><ul id="e925ae0d-5e5d-4a6e-b82f-12eb833336a7" class="bulleted-list"><li style="list-style-type:circle">which values are rare and why?</li></ul><ul id="16d908ec-13e7-44ff-8ba9-100d4714f400" class="bulleted-list"><li style="list-style-type:circle">can you see any unusual patterns and what might cause them?</li></ul></li></ul><ul id="7f0ed583-aff7-4b15-881d-4cb59119be7d" class="bulleted-list"><li style="list-style-type:disc">Clusters suggest that subgroups exist in your data<ul id="74f41a8f-6667-4e07-8dea-311d42379384" class="bulleted-list"><li style="list-style-type:circle">How can you explain or describe the clusters?</li></ul><ul id="45f565c8-18e2-4c6b-a5e0-e1811b113f30" class="bulleted-list"><li style="list-style-type:circle">How are the observations within each cluster similar to each other?</li></ul><ul id="b85580ae-6a7b-4a50-835a-fe72cc1e7b19" class="bulleted-list"><li style="list-style-type:circle">How are the observations in separate clusters different from each other?</li></ul></li></ul></details></li></ul></details></li></ul><ul id="d97fe558-ddc5-4817-aa38-3f2251148e49" class="toggle"><li><details><summary>Data Preprocessing</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="b7c01f92-0628-4675-b161-1ca11b3e34d7"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">process of prepares the final datasets for modeling <ol type="1" id="d8f45a0f-6d96-4cff-8c94-72d7ac3f278b" class="numbered-list" start="1"><li>Data Selection</li></ol><ol type="1" id="8509207c-b179-446a-b152-beb68ddf4ab2" class="numbered-list" start="2"><li>Data Cleaning</li></ol><ol type="1" id="a5f84105-d0cf-49a0-b319-e4c5f20d8ec4" class="numbered-list" start="3"><li>Data Construction</li></ol><ol type="1" id="014184e6-6bac-4aa0-ae45-d3faff7a3f2c" class="numbered-list" start="4"><li>Integrate data</li></ol><ol type="1" id="9eadcdd0-f0e9-4e4f-b8b1-622138ec17b5" class="numbered-list" start="5"><li>Format Data</li></ol></div></figure><ul id="48ce443c-b64a-49df-b00f-20e7f47bfa7a" class="toggle"><li><details><summary>Why data preprocessing? </summary><p id="b0202d11-9531-4ef1-adde-ea7d8df90c87" class="">raw data in the real world can have issues with it that we need to clean/fix/format so it can be used for modelling </p><ol type="1" id="52fc139a-0b72-428d-96ad-8650f3d18c2d" class="numbered-list" start="1"><li>incomplete data: missing values</li></ol><ol type="1" id="27061e0d-92f1-4b42-a4d2-bcfae60bef5b" class="numbered-list" start="2"><li>inconsistent: “03/07/2015” vs “March 07, 2015”</li></ol><ol type="1" id="19818cb2-7132-4f34-a137-f7be9ce66ebf" class="numbered-list" start="3"><li>noisy: contains errors or outliers</li></ol><ol type="1" id="dd64066a-5154-4ec0-9920-720e1b705773" class="numbered-list" start="4"><li>too complex: data has large amount features/instances </li></ol><ol type="1" id="926e60f4-e88d-4742-9f68-cbea18e1db8f" class="numbered-list" start="5"><li>different data types: numerical, nominal, text, web data, images, video, audio ect</li></ol></details></li></ul><ul id="7e6538c2-b4be-4829-8b4a-8e3e1b58d80c" class="toggle"><li><details><summary>Data Leakage</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="3df53692-cb1f-465b-a59f-02f6b3ec9ff4"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">data leakage is when information/knowledge is in both the training and test set. this causes incorrect estimates on the model’s predicting performance on unseen data.</div></figure><h3 id="22415700-c266-431d-9f22-7581c10467e4" class="">how to avoid data leakage</h3><p id="1db121dd-837d-4197-a796-9c97c6c1c8ca" class="">one way to avoid this is to split the dataset into training and test sets and then do data preprocessing on the training set.</p><figure id="7c24c424-f403-4c80-a24b-48c29eab757d" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>S</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>−</mo><mo>&gt;</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>−</mo><mo>&gt;</mo><mi>M</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">Data Splitting-&gt; Data Preparation-&gt; Modelling</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">Spl</span><span class="mord mathnormal">i</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord">−</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">re</span><span class="mord mathnormal">p</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord">−</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span></span></div></figure></details></li></ul><ul id="23c28e2b-7372-452b-b122-5c94fe23cfc3" class="toggle"><li><details><summary>Data Type Processing </summary><ul id="04c12c2f-e523-4dd1-9f8a-a8e26fc1b3d9" class="toggle"><li><details><summary>Categorical Data Encoding Scheme</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="906461f5-d91d-4359-9690-bee0a52e58fd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">convert categorical data to numerical values</div></figure><h3 id="9c9e5ad0-f941-41e4-a2e4-df1054294223" class="">One Hot encoding</h3><p id="bcc1506c-83db-432d-a5be-034c0f5229cd" class="">method to encode nominal data (categorical data which has no order) into binary</p><figure id="899df914-35e2-4e61-9802-b6a0cc52917a" class="image"><a href="Untitled%2038.png"><img style="width:596px" src="Untitled%2038.png"/></a></figure><ul id="85c0353d-c7ee-4c77-8ad1-54e1292754f9" class="bulleted-list"><li style="list-style-type:disc">problem is that it increases the number of attributes in a data line <p id="f4ba1c79-5831-45b0-a8ae-ce421c11a1e3" class="">(1 column to 3 columns in this case) </p></li></ul><h3 id="a165663d-183c-4042-ac00-eaa61edfa90f" class="">Ordinal encoding</h3><p id="08997465-1849-4ccb-9815-8651b129eb13" class="">method to encode ordinal data (categorical data which as order) by assigning each category to an integer value (similar to enums)</p><figure id="42b8a8df-baf5-4b18-8d5c-402842795ef8" class="image"><a href="Untitled%2039.png"><img style="width:596px" src="Untitled%2039.png"/></a></figure></details></li></ul><ul id="76d48e44-7fc6-41bb-8f82-d603b346ad5e" class="toggle"><li><details><summary>Normalisation/Scaling</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="c2a11f51-1d2e-447e-8038-8a60d65f807f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">converts numerical data features to a range of values between [0, 1]</div></figure><ul id="813912bb-c9f9-4cc1-9abe-d1ebcbedf6f9" class="bulleted-list"><li style="list-style-type:disc">ML methods KNN and SVM uses gradient descent which are greatly effected by the scale of data</li></ul><h3 id="9d71d116-0719-4dc3-a8de-902efefa536e" class="">Min-Max Normalisation/Scaling</h3><figure id="ec4758ec-eee0-4537-a57d-8db7c0ce0622" class="image"><a href="Untitled%2040.png"><img style="width:596px" src="Untitled%2040.png"/></a></figure><h3 id="5a6588a7-0cdf-4938-b869-8007ce33076f" class="">Z-Score Standardization</h3><ul id="63afc1d5-384c-4426-a159-05898ab6920f" class="bulleted-list"><li style="list-style-type:disc">center scaling </li></ul><ul id="6af32033-4270-4967-a77b-496bdc7d99f8" class="bulleted-list"><li style="list-style-type:disc">assumes our data follows a normal distribution </li></ul><figure id="05b569df-fefd-4325-a033-18f9ee9cb413" class="image"><a href="Untitled%2041.png"><img style="width:596px" src="Untitled%2041.png"/></a></figure><h3 id="d4dde7ca-2a7a-44bd-8070-b9978194a155" class="">Normalisation vs Standardization</h3><table id="b3680144-245d-40b1-b485-0232a8cd29b6" class="simple-table"><tbody><tr id="141b0de9-d2eb-44d2-aaf4-25c8956b9bee"><td id="]X=y" class="">Normalisation </td><td id="&gt;HSk" class="">Standardization</td></tr><tr id="6d45198e-97cc-4b3b-911c-d97074afcabb"><td id="]X=y" class="">makes different variables to have the same range</td><td id="&gt;HSk" class="">can give values that are both positive and negative centered around zero</td></tr></tbody></table><p id="cb50ea18-1cc1-4780-9543-24bfe3aa3d63" class="">only use standardization when our data follows a normal distribution. otherwise use Normalisation </p><ul id="c225a9d0-86b7-4dbc-b764-64dcc7996167" class="bulleted-list"><li style="list-style-type:disc">might be a good idea to have a mixture of standardised and normalised variables=&gt; standardised followed by normalised</li></ul></details></li></ul><ul id="7967831f-7465-4aa5-9c74-ffe929d64fe6" class="toggle"><li><details><summary>Discretization</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="265acd28-9d9a-46ff-80c4-bbf34307db78"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"></div></figure><ul id="9b0b399a-2994-4dc4-84de-b93f635f837c" class="bulleted-list"><li style="list-style-type:disc">Some algorithms prefer/require categorical inputs <p id="c0083273-4271-4da2-9de4-2648f1e74984" class="">(eg: Decision Tree)</p></li></ul><ul id="7faba9e0-36f3-416f-8092-d89317a0d271" class="bulleted-list"><li style="list-style-type:disc">for data smoothing, it can handle outliers </li></ul><ul id="4f494381-7d1b-4aa6-b84d-d56a3fcad878" class="bulleted-list"><li style="list-style-type:disc">Unsupervised discretization: does not depending on class label<ul id="6a172b1f-8c3c-4743-81f9-cddfaa4de5ba" class="bulleted-list"><li style="list-style-type:circle">sklearn.preprocessing.KBinsDiscretizer</li></ul></li></ul><ul id="b933d3df-732c-43d0-bcf5-a58b20aa9314" class="bulleted-list"><li style="list-style-type:disc">Supervised discretization: depends on class label<ul id="6b508acb-a1ab-46b6-90fe-8cf31a78bbd5" class="bulleted-list"><li style="list-style-type:circle">1RD, entropy-based</li></ul></li></ul><h1 id="d854bb94-8148-4560-85bf-d79a2fe5b7bf" class="">Equal-Width/Uniform</h1><p id="021c0aef-3e39-43b9-aef6-b35bb49acd05" class="">Convert a numerical attribute to an ordinal attribute with N<br/>possible values<br/></p><ol type="1" id="883c25c8-33ff-4548-8c7d-00551f613712" class="numbered-list" start="1"><li>Find the Maximum and Minimum values of the attribute</li></ol><ol type="1" id="6f3fdcbf-20a1-4e00-a47e-61455059a251" class="numbered-list" start="2"><li>Divides the range [Min, Max] into N intervals of equal size</li></ol><ol type="1" id="6eddccb0-bdf2-40f8-a6ed-0439a70328eb" class="numbered-list" start="3"><li>The width of intervals: W=(Max - Min)/N</li></ol><figure id="8b158f2b-1485-49a3-b0e5-b661bed4316f" class="image"><a href="Untitled%2042.png"><img style="width:821px" src="Untitled%2042.png"/></a></figure><h1 id="4d748248-ceca-4f83-8bc1-b967b6c216fa" class="">Equal-Depth/Frequency/Quantile</h1><p id="666cccd0-eed8-49c1-8403-2781dc4f660b" class="">instead of having the same size as the bin, we’re going to have bins with the same number of samples as each bin</p><ul id="7be5df39-fb13-4393-9ec3-fd2b6a87e856" class="bulleted-list"><li style="list-style-type:disc">insures the data has an equal distribution so that its not skewed. this helps with learning </li></ul><ol type="1" id="a186675c-5c18-42bc-846e-80ad1e6c91e2" class="numbered-list" start="1"><li>Divides the range [Max, Min] into N intervals</li></ol><ol type="1" id="e593f5d1-52fd-4fb7-b00c-a8225622987e" class="numbered-list" start="2"><li>Each interval including approximately same number of instances</li></ol><figure id="eda91b75-f5d3-4740-be06-565de1d2b814" class="image"><a href="Untitled%2043.png"><img style="width:596px" src="Untitled%2043.png"/></a></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="def4dcfe-3f71-405d-a57d-d54580653e06"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="00eb3aa8-45d7-4531-b003-6b3c4e5b2f96" class="">process of converting continuous values to discrete values</p></div></figure></details></li></ul></details></li></ul><ul id="1ceb4be9-175c-423f-9044-b8ea12f5a1db" class="toggle"><li><details><summary>Missing Values</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1914f07f-784a-46bf-86b3-09df198dec30"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">values for 1 or more variables are missing from recorded observations </div></figure><ul id="2fe9ab18-45ae-4ab8-9408-85a0486aa906" class="toggle"><li><details><summary>Type of Missing Data</summary><figure id="12a6f7b1-b56a-4a14-aa3e-3fae626ad2e0" class="image"><a href="Untitled%2044.png"><img style="width:807px" src="Untitled%2044.png"/></a></figure></details></li></ul><ul id="bc4f2f5f-e506-4281-83b0-6932d3491f5d" class="toggle"><li><details><summary>Handling missing values</summary><ul id="e61a5bc8-b287-4ad0-85de-0f452f5fc12e" class="toggle"><li><details><summary>Deletion approaches</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="0d6045df-880d-47a0-bcef-b27b0b4647b2"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">just delete the rows or columns with the missing values<figure id="a51a7fc4-2ddb-4d85-8230-fc91c1cd1395" class="image"><a href="Untitled%2045.png"><img style="width:568px" src="Untitled%2045.png"/></a></figure><figure id="ccd002db-d139-4096-b994-1fdba4d8988b" class="image"><a href="Untitled%2046.png"><img style="width:770px" src="Untitled%2046.png"/></a></figure><ul id="f3ad462f-8d91-403c-9892-23fc01e17bd1" class="bulleted-list"><li style="list-style-type:disc">only applicable when the amount of missing values is small</li></ul></div></figure></details></li></ul><ul id="4bad0d43-39f5-47b3-b14e-73149c222c28" class="toggle"><li><details><summary>Imputation Approaches</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="75601b55-d0aa-48c2-a3c5-82fc6cfc44c6"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">approximate/estimate the missing values</div></figure><h3 id="b8688a9e-816b-4d13-b358-a7c7a35e3d6a" class="">Mean/Mode imputation</h3><p id="78326e39-e03d-49eb-b68d-bac79c97a57b" class="">fills the missing values with either the average completed values (for continuous attributes) or fills  with the most frequent values (for categorical attributes)</p><ul id="0e454d4f-379d-4161-915b-eb6aeccb4859" class="bulleted-list"><li style="list-style-type:disc">(+) Simple, fast</li></ul><ul id="7a0da131-ac3a-41f5-a654-768b6f270b7d" class="bulleted-list"><li style="list-style-type:disc">(+) Doesn&#x27;t change the mean/mode of attributes </li></ul><ul id="8764ed9d-3732-49f1-9f23-81ec80cd711b" class="bulleted-list"><li style="list-style-type:disc">(-) Loss of information, depends on data types</li></ul><ul id="f81d224c-03af-44ba-bcc2-ecc1c0f1d9d8" class="bulleted-list"><li style="list-style-type:disc">(-) Not flexible, have to chose mean or mode</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b05ef72b-9be4-4dac-a160-31111fa3e8de" class="code"><code class="language-Python">sklearn.impute.SimpleImputer (strategy=&#x27;mean’) #for mean
sklearn.impute.SimpleImputer (strategy=‘most_frequent’) #for mode</code></pre><figure id="d33d96df-71a5-4f9c-9479-1719e10925d3" class="image"><a href="Untitled%2047.png"><img style="width:568px" src="Untitled%2047.png"/></a></figure><h3 id="898297c9-b226-4d3c-ba79-90f63b8340cf" class="">KNN imputation</h3><ol type="1" id="03c2554e-8b30-422c-a8d7-df951c74ad1c" class="numbered-list" start="1"><li>Find K nearest neighbours using observed values</li></ol><ol type="1" id="b2ccc211-b27d-4924-b48e-9d4c5eee8e22" class="numbered-list" start="2"><li>Estimate the missing value by the mean/mode from the K neighbours</li></ol><ul id="5c105e99-f610-49dd-bedd-9ec3f1ca826b" class="bulleted-list"><li style="list-style-type:disc">(+) Capture complex relationship</li></ul><ul id="314423bf-0d55-4d56-9fbc-3903805193f3" class="bulleted-list"><li style="list-style-type:disc">(+) Flexible</li></ul><ul id="7e7bbd86-a5c7-41f2-b780-14a298a5e781" class="bulleted-list"><li style="list-style-type:disc">(-) High computational complexity, Parameters </li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b1359994-d7ca-46ff-a2f7-d79607d9320c" class="code"><code class="language-Python">sklearn.impute.KNNImputer()</code></pre><figure id="c4a6ce6b-1fe2-4d75-b1b3-5c4d1bcfae26" class="image"><a href="Untitled%2048.png"><img style="width:568px" src="Untitled%2048.png"/></a></figure></details></li></ul></details></li></ul></details></li></ul></details></li></ul><ul id="eee9b97f-b5fc-4fec-aafe-1055f288d3e3" class="toggle"><li><details><summary>Why Dimensionality Reduction?</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="9b93f602-7464-41ca-8966-d1e171a1c433"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">we need to reduce the dimensionality of your data <ol type="1" id="76d31342-6b34-401d-b22c-0a75728cd698" class="numbered-list" start="1"><li>due to curse of dimensionality</li></ol><ol type="1" id="103fbf05-6ee6-43e1-9222-1c8917d6c67d" class="numbered-list" start="2"><li>to remove irrelevant/redundant information </li></ol><ol type="1" id="cda0d9b7-280c-4d51-93ae-5d71d6e4cbb6" class="numbered-list" start="3"><li>and save time/memory/money </li></ol></div></figure><h3 id="b1fbc4a2-dd11-47d1-aff2-bb79b844b02c" class="">curse of dimensionality</h3><blockquote id="56185991-66e7-49ce-a4f4-dde89408c218" class="">curse of dimensionality exponentially increases the number of samples we need when increasing the number of features<figure id="9cffd75e-093e-426d-998b-52f5258c3c93" class="image"><a href="Untitled%2049.png"><img style="width:752px" src="Untitled%2049.png"/></a></figure></blockquote></details></li></ul><ul id="9d6ae0ce-1be4-4b9d-a3fb-f197a18b3890" class="toggle"><li><details><summary>Feature Selection</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="4d83a6b6-a0ef-471e-a144-45b3dc65d70e"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">only chose a subset of features to be used <figure id="cc1a525c-a6e4-4d6d-ba19-1c6f949180e2" class="image"><a href="Untitled%2050.png"><img style="width:288px" src="Untitled%2050.png"/></a></figure></div></figure><ul id="5d75a36f-561a-4c39-ba2c-386839a8634c" class="toggle"><li><details><summary>Feature Selection Challenges</summary><h3 id="b77f067c-ecec-4d79-a41e-dc941b2679fd" class="">1) Large Search Space</h3><p id="435977d0-8e88-401c-bf97-0722daa602b0" class="">when the search space is too large, then manually selecting the ones we want takes too long</p><ul id="ce3505db-a83a-4329-8bbe-9f7656ce0ccc" class="bulleted-list"><li style="list-style-type:disc">Large (exponential) search space (2n – n is the number of features)</li></ul><h3 id="a6eae2b0-181a-4dac-a44e-fa41c5975432" class="">2) Complexity</h3><p id="9430de8f-ea4a-4378-af7e-702f7a076b90" class="">Top relevant features can be redundant, Weakly relevant features can be strongly relevant together</p><figure id="3846f888-1ab0-4bc7-a6f6-c0b57a349673" class="image"><a href="Untitled%2051.png"><img style="width:432px" src="Untitled%2051.png"/></a></figure></details></li></ul><ul id="bad70d25-a793-45f6-b0b2-9bfc6b6917c4" class="toggle"><li><details><summary>Overall Feature Selection System</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="216c4f4b-3f76-4a78-a106-5a0d6de4a7d1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><figure id="19ebc85b-163d-4c62-8b6f-06c8a9f60356" class="image"><a href="Untitled%2052.png"><img style="width:536px" src="Untitled%2052.png"/></a></figure><ul id="a6d2b302-4bc5-4e6c-b41b-c4ba970d53b0" class="bulleted-list"><li style="list-style-type:disc">subset evaluation is only done on the training set to prevent data leakage</li></ul></div></figure></details></li></ul><ul id="3ee4ff05-5d46-4f17-92f6-4b83f62871b2" class="toggle"><li><details><summary>Subset Evaluation (Filter, Wrapper and Embedded FS)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="a6716282-3f34-4ec4-8041-8a4a12601892"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">subset evaluation component has 3 different methods</div></figure><h3 id="c75090d3-bfb7-488f-ac51-f9e60909d104" class="">Filter</h3><p id="4b718256-762f-4f93-af9d-cce1c772bfd8" class="">Uses existing measures using statistics </p><ul id="d33baa23-d8e7-4f19-829e-3dc0282d0bdf" class="bulleted-list"><li style="list-style-type:disc">no learning algorithm needed</li></ul><figure id="f1fc1aa9-dcad-4025-9e75-7fa375f32de5" class="image"><a href="Untitled%2053.png"><img style="width:596px" src="Untitled%2053.png"/></a></figure><h3 id="1bc48b0a-ab6d-4307-9693-98f574635a66" class="">Wrapper</h3><p id="23eab279-4c64-45ad-b09b-c21a778f7629" class="">Uses learning performance</p><ul id="addfe868-87b3-4922-9b3f-c7eae801e980" class="bulleted-list"><li style="list-style-type:disc">train ML models many times</li></ul><ul id="91fa0a7f-a525-4444-abaa-4473f8b0b987" class="bulleted-list"><li style="list-style-type:disc">works well with KNN</li></ul><figure id="351505b6-812b-4b87-82b5-14c75fde4c38" class="image"><a href="Untitled%2054.png"><img style="width:596px" src="Untitled%2054.png"/></a></figure><h3 id="9a1e5e9f-c26a-4689-a8f6-a432ade3a02a" class="">Embedded</h3><p id="20ddd4fd-cb4e-430e-b04e-aad3f948cb81" class="">Train ML model once</p><ul id="c3258993-0d23-431c-879a-bccc00a57bf7" class="bulleted-list"><li style="list-style-type:disc">Select features based on the learned model</li></ul><ul id="8672f5b1-62be-4af3-966b-600b94c8ae50" class="bulleted-list"><li style="list-style-type:disc">works well with decision tree</li></ul><ul id="5de3d6cb-e32d-4c49-8ba8-90ba901136ab" class="bulleted-list"><li style="list-style-type:disc">not applicable to all machine learning algorithms </li></ul><figure id="1cb8c3e3-5047-4305-bb53-161b71834bac" class="image"><a href="Untitled%2055.png"><img style="width:596px" src="Untitled%2055.png"/></a></figure><p id="a70ef253-dc40-4560-b6b9-ebc6629259b9" class="">
</p></details></li></ul><ul id="42a12bfd-4c05-42c6-9522-63c130675b28" class="toggle"><li><details><summary>Feature Ranking vs Subset Selection</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="63117b2a-b796-4f65-9faf-cb752bf7c33d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">feature ranking is another component used in place of subset evaluation</div></figure><h3 id="c8224871-c5a9-40d3-b224-47315e8cd0f0" class="">Feature Ranking vs Subset Selection</h3><p id="6ccb916e-c62d-4e26-8d53-d02cfc273fa0" class="">Feature ranking:</p><ul id="c006ad7d-432a-4a9c-bcbb-90794ca185e2" class="bulleted-list"><li style="list-style-type:disc">feature ranking evaluates features individually and rank features and select top-ranked features</li></ul><ul id="a949ae40-e111-4e97-8e12-257be81ef904" class="bulleted-list"><li style="list-style-type:disc">Simple, efficient</li></ul><ul id="269c1e92-fa31-439b-af2d-caabbde01702" class="bulleted-list"><li style="list-style-type:disc">Ignore feature interactions (can select redundant features)</li></ul><ul id="a1c6c8ff-f4a6-4820-9671-89fe40331c10" class="bulleted-list"><li style="list-style-type:disc">less expensive </li></ul><p id="802e485e-b2dd-47b0-8d20-d18d451e39a9" class="">Feature subset selection:</p><ul id="0a5a5d8b-556a-4f2a-83f6-6014c33da3c2" class="bulleted-list"><li style="list-style-type:disc">Evaluate the whole feature subset</li></ul><ul id="0787ba6d-52cf-4fb5-9613-54679613538c" class="bulleted-list"><li style="list-style-type:disc">Often an iterative process to improve the feature subset<ul id="a9b89a3a-0a7b-4ae4-94e2-a7e7ec4a8a21" class="bulleted-list"><li style="list-style-type:circle">Sequential feature selection is an example</li></ul></li></ul><ul id="d6276e17-8e00-46ae-98b2-0dc97c50ea30" class="bulleted-list"><li style="list-style-type:disc">Consider feature interactions, usually better performance</li></ul><ul id="dcdf54ef-be39-4a9f-b1e7-a27649e45da3" class="bulleted-list"><li style="list-style-type:disc">More complicated search, usually more expensive than ranking</li></ul><p id="5726800b-eca6-477b-bdf1-2d07e94d1d1b" class="">
</p></details></li></ul><ul id="82288141-66b3-41e6-83cf-093f85144aa6" class="toggle"><li><details><summary>Univariate FS - Correlation based methods</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="8b72b5c5-7dfb-4e93-836c-bc2287af2ae2"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Univariate methods measure correlation between each input feature<br/>and the target variable/class label<br/><figure id="6da0515a-a47d-4bc8-877b-cff31f927bab" class="image"><a href="Untitled%2056.png"><img style="width:841px" src="Untitled%2056.png"/></a></figure></div></figure></details></li></ul><ul id="5322a604-e94a-4704-af8c-fe58a21d1eab" class="toggle"><li><details><summary>Some Other Measures for FS</summary><figure id="af3b4e48-0f95-43bc-97d6-851cfd45ecc4" class="image"><a href="Untitled%2057.png"><img style="width:805px" src="Untitled%2057.png"/></a></figure></details></li></ul><ul id="b1924583-1ef5-45a2-8af4-642fa7a4af0f" class="toggle"><li><details><summary>Subset Selection: Sequential Search</summary><figure id="48503795-8dc9-41f7-b9af-93530e0dda7b" class="image"><a href="Untitled%2058.png"><img style="width:596px" src="Untitled%2058.png"/></a></figure><h3 id="5e79143b-a488-4057-835d-8bc60c7d467b" class="">Subset Selection Illustration</h3><figure id="9bbdd973-9fe8-4709-881c-32a9c09623cf" class="image"><a href="Untitled%2059.png"><img style="width:596px" src="Untitled%2059.png"/></a></figure></details></li></ul><ul id="dd88e370-c3b9-4b8c-a17b-8ad4ca84d3c9" class="toggle"><li><details><summary>More advanced FS Methods</summary><figure id="2d42ad16-ae24-498c-8710-9d43d0f43581"><div class="source"><a href="https://www.notion.soundefined"></a></div></figure></details></li></ul></details></li></ul><ul id="dad615d0-84ca-4ff3-aecf-082fea52c844" class="toggle"><li><details><summary>Feature Construction </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="f21283fd-3da1-41df-ab24-f642f2ab2b11"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">create new features from the ones we have<figure id="260400a7-e606-4604-ab18-31c3f941f210" class="image"><a href="Untitled%2060.png"><img style="width:499px" src="Untitled%2060.png"/></a></figure></div></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="c0c7e2c7-ad7e-478d-bf03-23eaac3e925f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a <mark class="highlight-purple_background">kind of feature transformation</mark> to produce high-level constructed features that discover the relationships between features and augment the feature space<figure id="c7aebeed-309c-41da-a7d9-926466f8b10e" class="image"><a href="Untitled%2061.png"><img style="width:809px" src="Untitled%2061.png"/></a></figure></div></figure><ul id="4f86da04-29a3-403c-b901-e7a0094c7d82" class="toggle"><li><details><summary>What is a good feature</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="67a339eb-2958-4c12-af19-7fab77e38bf1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">measurement of goodness is subjective<p id="ded1e976-e665-4e68-858d-ab0bc95b1371" class="">it depends on the problem</p><figure id="db7d19ff-f257-4528-a8ce-a63dc769d95e" class="image"><a href="Untitled%2062.png"><img style="width:564px" src="Untitled%2062.png"/></a></figure></div></figure><h3 id="0e4269e5-cf8c-473d-bcd1-5e4db3f97919" class="">Example</h3><p id="f54d7ac3-cd8f-4687-a916-e8d176ee35d4" class="">the features bolow are good for for a simple linear classifier </p><figure id="98e73d6a-0f99-4823-a57f-0aed95369c2f" class="image"><a href="Untitled%2063.png"><img style="width:801px" src="Untitled%2063.png"/></a></figure><p id="223211f9-b338-4d3f-8029-3670368d4a98" class="">however its not going to be good for a decision tree classifier</p><p id="4ecbf99c-f660-4b7e-94b6-11304cc45255" class="">it will be bad for unseen data</p><figure id="60be79b4-b9d4-48a3-bdec-596704763844" class="image"><a href="Untitled%2064.png"><img style="width:624px" src="Untitled%2064.png"/></a></figure><p id="c5dd7a86-c029-4db9-91de-9aa32fd5c7bd" class="">
</p></details></li></ul><ul id="ead37194-54e4-4282-9a99-8397ffe5465b" class="toggle"><li><details><summary>Why feature construction?</summary><ul id="1d352603-7431-47c2-8536-e47fce661a12" class="bulleted-list"><li style="list-style-type:disc">the quality of input features can drastically <mark class="highlight-purple_background">effect the learning performance</mark><ul id="90a97c42-4b29-4ee5-873d-b230dc1a015c" class="bulleted-list"><li style="list-style-type:circle">so constructing good features can help it</li></ul></li></ul><ul id="61ab198f-b9cb-400e-85ca-8e6d693def45" class="bulleted-list"><li style="list-style-type:disc">even if the original features are high quality, <mark class="highlight-purple_background">transformations may be needed to use then with certain types of classifiers</mark></li></ul><ul id="f30236b1-9f4f-4c73-965a-a376a6bb6a32" class="bulleted-list"><li style="list-style-type:disc">a large number of classification algorithms are unable to transform their input space</li></ul><ul id="b074b838-4fff-4db7-af97-cb1690acf158" class="bulleted-list"><li style="list-style-type:disc">feature construction does not add the cost of acquiring original features <br/>it only cares about computational cost<br/></li></ul><ul id="21538e40-fb33-421c-874e-437cdf1381d3" class="bulleted-list"><li style="list-style-type:disc">often feature construction can lead to dimensionality reduction or implicit feature selection<p id="bcb9ca11-788a-4a28-8b08-14fca68a9644" class="">this is because finding a way to construct good features also finds how to make bad ones, and the bad ones are then not selected</p></li></ul></details></li></ul><ul id="0cd4f4b2-3a03-4738-b235-2bfdaaa22acb" class="toggle"><li><details><summary>Feature Construction Process</summary><figure id="14dc3413-a351-4dad-9b29-f1fdb2546d0b" class="image"><a href="Untitled%2065.png"><img style="width:843px" src="Untitled%2065.png"/></a></figure><ul id="8dde4d2b-cb87-4695-a9e8-49a3d43db34a" class="bulleted-list"><li style="list-style-type:disc">first we initlize the values (such as k in knn)</li></ul><ul id="9fd2b85c-498d-4edc-a887-80748fbd7f92" class="bulleted-list"><li style="list-style-type:disc">this process is done only on the training set to avoid data leakage</li></ul><ul id="c6b91030-4c80-49c3-b59d-d1de4eafcab7" class="bulleted-list"><li style="list-style-type:disc">we use the test set only after the procest</li></ul><ul id="147f341f-0acf-4776-91f0-e910a6744803" class="bulleted-list"><li style="list-style-type:disc">main part of the process is Construction discovery and feature evaluation<ul id="cb29f135-bd8f-4bca-bcec-35e9cb694cec" class="bulleted-list"><li style="list-style-type:circle">Construction discovery is a search mechanism to find different ways to combind the original features to form the high level features</li></ul><ul id="90e16e85-7a8e-431f-8d26-6e416f386b62" class="bulleted-list"><li style="list-style-type:circle">we then pass those high level features to the feature evaluation component that uses evaluation criteria (eg: entropy) to give feedback back to the construction discovery<figure id="657a4628-1aff-4616-af93-5230a101bb52" class="image"><a href="Untitled%2066.png"><img style="width:568px" src="Untitled%2066.png"/></a></figure></li></ul><ul id="a4be1889-4d00-475f-a2dd-24badf5550f4" class="bulleted-list"><li style="list-style-type:circle">this forms a loop/is repeated until a stopping point where the goodness of the high level features are good </li></ul></li></ul></details></li></ul><ul id="964cac30-5264-4c56-a324-513b2658ddb1" class="toggle"><li><details><summary>Construct New Features - Operators (First Component in Feature Construction Process)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="3f5e936e-1ad1-46e7-955c-d305f64eb5e3"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">The choice of operators/functions is based on domain knowledge and the type of features<ul id="fa1caea7-ee6c-4370-a1b2-8ef9ead3fa6b" class="bulleted-list"><li style="list-style-type:disc">Boolean features: Conjunctions, Disjunctions, Negation</li></ul><ul id="09cb5b67-69e1-4862-9a42-294773b223ca" class="bulleted-list"><li style="list-style-type:disc">Nominal features: Cartesian product, M of N etc.</li></ul><ul id="2c5b28ec-91e6-4986-a1f0-911c2416a6c5" class="bulleted-list"><li style="list-style-type:disc">Numerical features: Min, Max, Addition, Subtraction, Multiplication, Division, Average, Equivalence, Inequality etc</li></ul><p id="7142dfa1-2ffa-404d-ba7f-da27d3aded79" class=""><mark class="highlight-purple_background">choosing the correct set of operations and applying them appropriately is the major challenge </mark></p></div></figure><h3 id="98f44c97-5b4d-4d24-98f6-1c858d049e4e" class="">Search space in feature construction</h3><p id="cbde1968-6e34-4f6b-a85d-674342546b6f" class="">Search space in feature construction is the space of possible<br/>functions of input features<br/></p><ul id="f7004219-e913-4292-83ff-7f3f7f59bba1" class="bulleted-list"><li style="list-style-type:disc">- How big is it?</li></ul><p id="58463e6a-648c-412c-b2ec-466ebae960e3" class="">we need an effective search machanisum in feature construction, as if the search space is large, then the number of combinations the features can be combinded is near infinit </p></details></li></ul><ul id="3ee89a50-e618-4fb3-80cd-16cd94259053" class="toggle"><li><details><summary>Evaluate and Select New Features (Second Component in Feature Construction Process)</summary><figure id="4ade0062-ccd8-4888-9fdc-979d2a2690b9" class="image"><a href="Untitled%2067.png"><img style="width:875px" src="Untitled%2067.png"/></a></figure></details></li></ul><ul id="481e241f-4e39-43ea-8261-882d3b5a5747" class="toggle"><li><details><summary>Feature Construction Methods</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="70f7ffe5-f2cb-4aef-b4b3-7afd74eb8e0f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">there are 3 methods for feature construction, they are based on the evaluation process<ul id="aea0a19c-202a-45d1-a99c-1890c972d1bf" class="bulleted-list"><li style="list-style-type:disc">these 3 method are the same as the 3 different ways of evaluation features</li></ul></div></figure><h3 id="73f79fc4-0b79-4b7c-914a-b440b9bfd73c" class="">Filter</h3><p id="f739d0a8-dd54-4260-b807-3212b74e46de" class="">uses simple calculation(s) to measure feature’s goodness (eg: entropy) </p><figure id="3dd08c6b-039b-4d38-a7be-34dff14119cb" class="image"><a href="Untitled%2068.png"><img style="width:624px" src="Untitled%2068.png"/></a></figure><h3 id="8d21c3ba-edb5-4d65-8d60-dbb90ac2f526" class="">Wrapper</h3><p id="e57dd67d-cc1f-4d3c-bcec-10b89dcf63ae" class="">makes a classifier (eg: decistion tree) for each feature</p><p id="36cb45cf-8af9-470f-b026-30e1c438f8c0" class="">if we have 100 features, then it will make 100 classifiers</p><p id="7081fcbb-add1-49fc-bfdd-b353e2d835fe" class="">this is expensive but better than filter</p><figure id="13e8242d-2797-4e2d-9516-f7a0392e4373" class="image"><a href="Untitled%2069.png"><img style="width:1108px" src="Untitled%2069.png"/></a></figure><h3 id="1a4305e6-1007-4734-9d9c-b51ad9575627" class="">Embedded</h3><p id="931d9a0a-2c6c-4cad-b1b2-07da3477d9d7" class="">similar to wrapper but uses a classifier/learning algorithm (eg: svm) only one time</p><p id="eff53ebc-42d6-4983-9755-9db3130969c9" class="">better than filter but worse than wrapper</p><figure id="40b9f8cb-2f41-4077-9805-fe9dc8a8fc3b" class="image"><a href="Untitled%2070.png"><img style="width:624px" src="Untitled%2070.png"/></a></figure><h1 id="fa35c049-82ab-40ce-b1e2-4754c04b8309" class="">Comparison</h1><figure id="0a19a8ac-c51e-4c2c-a218-bc67f56d376b" class="image"><a href="Untitled%2071.png"><img style="width:624px" src="Untitled%2071.png"/></a></figure><p id="bf06f7bf-6955-4a70-8aed-098acaa7633d" class="">
</p></details></li></ul><ul id="870d7a51-d196-47ce-9b31-6bccbc13532e" class="toggle"><li><details><summary>Principal Component Analysis (PCA)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="649a97a8-2394-4a94-a9dc-ef41f2ecbfa7"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a mathematical procedure that linearly transforms (possibly) correlated features into a (smaller) number of uncorrelated features called principal components<ul id="f4d04558-dbc5-4675-8bdf-6bbd2cb2f3de" class="bulleted-list"><li style="list-style-type:disc">Invented by Karl Pearson (1901)</li></ul><ul id="9fb8e408-f613-4662-b8fb-0137d079682f" class="bulleted-list"><li style="list-style-type:disc">Goal is to achieve high data variance</li></ul><ul id="5cce63e5-bbd5-45d9-ac13-2ac1fe0bafd4" class="bulleted-list"><li style="list-style-type:disc">the higher the variance, the easier to extract/separate information<figure id="a636c2c4-2155-426f-8c42-38d10af31721" class="image"><a href="Untitled%2072.png"><img style="width:564px" src="Untitled%2072.png"/></a></figure><p id="3c2bfe32-fedf-4b31-8598-c1c1bc1d7e38" class="">principal component 1 has high data variance, so the 3 data sections are spread out and easy to separate</p><p id="82fa0418-c6fb-4cc1-ada3-78ae6216464a" class="">principal component 2 has low data variance, so the data sections are very close together and hard to see</p><p id="788cf8fc-1aaa-4b80-a26e-34ee8280cb5e" class="">
</p></li></ul><ul id="fa3f7281-1915-4417-aa30-a0b9dd186333" class="bulleted-list"><li style="list-style-type:disc">PCA is unsupervised because it doesn’t have class labels and doesn’t care about the output, it just preserves variance  </li></ul><ul id="16e7f5be-9697-453f-8aa6-61690c954f1e" class="bulleted-list"><li style="list-style-type:disc">its a filter approach because it doesn’t have learning </li></ul></div></figure><h3 id="1bfaf75a-5c72-4ad0-877e-0f676bffe976" class="">Principal Components</h3><figure id="8f14d66d-5638-4d2b-a4b2-e6650a9db42e" class="image"><a href="Untitled%2073.png"><img style="width:624px" src="Untitled%2073.png"/></a></figure><ul id="cca09659-cea6-4c65-bc31-e7e61b6fa9d9" class="bulleted-list"><li style="list-style-type:disc">a1 should be orthogonal to a2 , meaning if we take the dot production between a1 and a2 then the result should be 0</li></ul><ul id="aac90645-0471-4985-bd54-16c9017f51b8" class="bulleted-list"><li style="list-style-type:disc">explained variance ratio tells you how much data variance a principle component preserves <ul id="38817603-233a-44ba-a062-f71854fb05c0" class="bulleted-list"><li style="list-style-type:circle">meaning the higher the evr, the better</li></ul></li></ul><ul id="7d8b8398-6213-4554-8d95-d497cf39f26a" class="bulleted-list"><li style="list-style-type:disc">general rule to set k is to select a k value so that the total amount of data variance presurved by k is about 95%</li></ul></details></li></ul><ul id="6fefe465-df1d-42d8-8a14-2e6577051eb1" class="toggle"><li><details><summary>Independent Component Analysis (ICA)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="fc6f50ed-50e2-4e6d-b5bb-4480152950d1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">similar to PCA (creates new components that are linear combinations of the original variables) <br/>but instead of preserving variance, it builds components which are statistically independent <br/><figure id="0f189794-8ea0-4664-bcf2-8edc3cba8466" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x,y)=p(x)p(y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></div></figure></div></figure></details></li></ul><ul id="8e14ba0a-659a-432f-b996-2ee42ec1890c" class="toggle"><li><details><summary>Kernel Principal Component Analysis (KPCA)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="3926edb8-fe92-4066-88f5-7fd9453db6fe"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">extends PCA to to handle non linear cases using a kernel function <figure id="7e13ff2c-6dba-4df5-b095-20aa43adb330" class="image"><a href="Untitled%2074.png"><img style="width:941px" src="Untitled%2074.png"/></a></figure><ul id="e82a2d61-dec5-400a-8d5e-63a845ebe2bf" class="bulleted-list"><li style="list-style-type:disc">problem with KPCA is that its as not interpretable, because the calculation is done implicitly (cannot express new PCA components as a formula of original features)</li></ul></div></figure><p id="ae31738b-3b99-4abe-aa54-adbf7c6423a0" class="">
</p></details></li></ul><ul id="b5e58d18-b3b7-4c77-bdd4-29b7732ce43c" class="toggle"><li><details><summary>Polynomial Features</summary><figure id="94a59e73-1a92-47bd-a274-2f8872d37222" class="image"><a href="Untitled%2075.png"><img style="width:624px" src="Untitled%2075.png"/></a></figure></details></li></ul><ul id="0d724a04-f1bd-49c2-877d-2ea2714a04c9" class="toggle"><li><details><summary>GP For Feature Construction (Bonus)</summary><figure id="95455bb2-410e-4031-9b51-87f4ba325096" class="image"><a href="Untitled%2076.png"><img style="width:776px" src="Untitled%2076.png"/></a></figure></details></li></ul></details></li></ul></details></li></ul><ul id="e10adfeb-1911-48e2-a8b2-524751f4a5ce" class="toggle"><li><details><summary>ML Python</summary><ul id="4abf2ce2-06a2-4158-964c-73604f0c3618" class="toggle"><li><details><summary>Basic Python Programming</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="cd41cc94-6f7e-4b3a-aaba-77779f0bb45c"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">in machine learning and this course , we will be using python as the programming language<ul id="d89d9e6e-536f-49a0-ba65-2c872de39b73" class="bulleted-list"><li style="list-style-type:disc">its popular </li></ul><ul id="d5758b0a-d221-4c56-a6a4-86fa6798fa26" class="bulleted-list"><li style="list-style-type:disc">has many ml libraries (sklearn)</li></ul><figure id="849dda8a-9fab-41a0-b585-467489cd844a"><a href="https://ecs.wgtn.ac.nz/foswiki/pub/Courses/AIML231_2024T1/LectureSchedule/basic_python_programming_tutorial.pdf" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title"></div></div><div class="bookmark-href">https://ecs.wgtn.ac.nz/foswiki/pub/Courses/AIML231_2024T1/LectureSchedule/basic_python_programming_tutorial.pdf</div></div></a></figure></div></figure><ul id="90019213-1a49-4920-bf35-b06846381948" class="toggle"><li><details><summary>Anaconda</summary><h3 id="94f8b60a-8fea-4411-adec-ce27ce64cd54" class="">Anaconda</h3><p id="4f1f8130-1e35-40b9-af89-98d7c10e5e82" class="">python runtime popular for machine learning. it includes:</p><ul id="f050de2d-cc09-4e72-8280-f5bf55880e3d" class="bulleted-list"><li style="list-style-type:disc">jupyter notebook</li></ul><ul id="336314a2-da49-4f79-bbe1-00558e9c0f83" class="bulleted-list"><li style="list-style-type:disc">Main Python libraries: numpy, matplotlib, pandas, sklearn</li></ul><ul id="09bbbcc5-230c-47fd-b38b-5a02f375c006" class="bulleted-list"><li style="list-style-type:disc">u Others e.g., Spyder</li></ul></details></li></ul><ul id="69e8a4f4-8fbf-462f-ab45-c8cded9bdc4b" class="toggle"><li><details><summary>Basics </summary><h3 id="08a87dea-4ab7-4a0d-9b75-6707ae5a0043" class="">variables</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0ec94026-58fe-4621-bee9-1f9a776d1a16" class="code"><code class="language-Python">name = &quot;howard&quot;
age = 100
old = TRUE

x = 138
y = 7282488
z = 48 * x + y/2 + x * y + 32 * x **2</code></pre><h3 id="d5365620-54a1-4070-b686-ce0fcdd0ac3d" class="">print</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1ddc0aa0-ab07-46ba-acc8-d0b4bb9a6753" class="code"><code class="language-Python">print(&quot;hello &quot; + name + &quot; you are &quot; + str(age) + &quot; years old&quot;)</code></pre><h3 id="116ca460-a901-48ca-9364-755fbe6a6e13" class="">input</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c5661fb8-fbe2-4cfd-b83b-a006d1901ecd" class="code"><code class="language-Python">name = input(&quot;Enter a name here: &quot;)
print(&quot;Dear &quot; + name)
print(&quot;You are invited to my birthday
party&quot;)</code></pre><h3 id="52ac7c3d-055a-4c7f-8478-92f1369db258" class="">functions</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bdb1be47-4d1f-4eaa-8fe8-9c8a3154cbae" class="code"><code class="language-Python">def functionName(params):
	body</code></pre><h3 id="c2c7a648-4a22-4a40-901d-fb7872dc5247" class="">if else</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="dc5ae4cd-ebe1-490e-a67a-58352a487f5a" class="code"><code class="language-Python">if(condition):
	body
elif(condition):
	body
else:
	body</code></pre></details></li></ul><ul id="610937a6-8820-4de9-8b1a-44f9e2a7983c" class="toggle"><li><details><summary>Positional Arguments </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1b5225ff-1ad9-4ce4-af0e-b8f575ab754e"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">normally when passing parameters into a function, the order matters. with keyword arguments, the order doesn’t matter since we are specifying which parameter gets what value</div></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bd8edb02-7909-4958-9e89-bd6f7606665e" class="code"><code class="language-Python">def welcome(name, course, num):
	println(name + course + str(num))</code></pre><h3 id="d49834a0-c92b-4608-9e0a-5d0018c64892" class="">without keywords</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="17c7bf9c-fc8d-4b19-8f4b-80a11a9db3bd" class="code"><code class="language-C">welcome(&quot;alex&quot;, &quot;COMP102&quot;, 180) //works</code></pre><h3 id="154214ff-ab42-42cb-8bde-262a2dd98c28" class="">with keywords</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="afe484c9-9c80-4fd6-a0ca-f3c64ebc0fb9" class="code"><code class="language-C">welcome(name=&quot;alex&quot;, course=&quot;COMP102&quot;, num=180) //works</code></pre><h3 id="143458e1-7d47-422b-b7ff-4bfe0f2b3d53" class="">keywords with no order</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0a2b6e4a-a249-46c8-92a2-497d6d0636bd" class="code"><code class="language-C">welcome(course=&quot;COMP102&quot;, num=180, name=&quot;alex&quot;) //works</code></pre><h3 id="fab02ba5-89e3-40de-8019-63dbd3746eb3" class="">hybrid </h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="fd3823ea-c311-4cfa-b3bf-f83b7189813c" class="code"><code class="language-C">welcome(&quot;alex&quot;, num=180, course=&quot;COMP102&quot;) //works</code></pre></details></li></ul><ul id="a7a30048-229f-4909-9015-0326d80833bd" class="toggle"><li><details><summary>Default values</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="d476994b-c6e9-42b1-b4f9-9388d441677b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">we can set a default values for parameters which are not specified in a function all by assigning it in the function definition like we do in the positional argument call </div></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bdea1899-4753-4e2a-9276-a5ab6fa3a9df" class="code"><code class="language-Python">def pay(hour, rate=20): #rate&#x27;s default value is 20 if not specifed in the function call
	 total = hour * rate
	 print(total)</code></pre><h3 id="381e2d70-37dc-4679-960a-8274c3ae78fc" class="">normal</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="93b36e3a-1815-4c3b-96a4-f57b9a4f2e89" class="code"><code class="language-Python">pay(2, 25) #prints 50</code></pre><h3 id="c619d729-3bb0-4c32-8f91-157793de2962" class=""> default arguments</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="9387962b-d889-449a-ab75-59c7b200318c" class="code"><code class="language-Python">pay(2) #prints 40</code></pre></details></li></ul><ul id="96b58749-6288-4086-8c3a-52591e384c18" class="toggle"><li><details><summary>importing modules/libraries </summary><h3 id="00e61e2f-c0b3-439a-9d9d-a51b9a96c9fd" class="">importing the whole modules/libraries </h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="389a69f9-d4c6-4578-805f-10840f662daf" class="code"><code class="language-Python">import math #all functions from math are imported
print(math.pi) #have to specify that pi is from the math library to use it
print(math.sqrt(23.456))
</code></pre><h3 id="47a23e40-f58e-402e-9fbc-b37fed42f24b" class="">import specific functions from modules/library </h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="3aa99c95-ba6c-4002-900e-f92fa83013a0" class="code"><code class="language-Python">from math import pi, sqrt #importing just pi and sqrt
print(pi) #don&#x27;t need to specify its from math. just use it
print(sqrt(23.456))</code></pre></details></li></ul><ul id="8daa23d9-ef19-4e00-8020-5c6108eac570" class="toggle"><li><details><summary>For loops</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="380b7471-2bfa-4f71-b07b-729d9d3fbc98" class="code"><code class="language-Python">for x in range(10):
		print(x)</code></pre><h3 id="d9580378-90a0-44a7-a522-5c11640dba3c" class="">loops with lists</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f5684ff6-9c13-4043-9da8-9419a2319a96" class="code"><code class="language-Python">numbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]
sum = 0
for val in numbers:
		sum = sum + val
		print(&quot;The sum is&quot;, sum)</code></pre><h3 id="be7b0bdf-fcd8-4d0b-97ad-a25cd1783727" class="">breaks out of loop</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ec261c52-865a-44fd-8d74-7dced5576eeb" class="code"><code class="language-Python">for letter in &#x27;Python&#x27;:
	 if letter == &#x27;h&#x27;:
			 break
	 print(&#x27;Current Letter :&#x27; + letter)</code></pre><figure id="516d2466-4ddf-4803-8b7d-7ea20eccc2d6" class="image"><a href="Untitled%2077.png"><img style="width:402px" src="Untitled%2077.png"/></a></figure></details></li></ul><ul id="42ebbb8e-037d-4d74-b0c0-3869cbf78f2a" class="toggle"><li><details><summary>Lists []</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="9d97a459-b51f-4b8e-b798-201f8e3b8625"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">mutable collection of elements. meaning it can change<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f534ac8e-134e-4530-bb38-8249768c8557" class="code"><code class="language-Python">words = [&quot;Every&quot;, &quot;question&quot;, &quot;is&quot;, &quot;a&quot;, &quot;good&quot;, &quot;question&quot;]
nums = [2, 4, 2, 9, 1, 0]</code></pre><ul id="df72c2eb-fd0b-4fe9-bec2-7b4bd2c92fda" class="bulleted-list"><li style="list-style-type:disc">starts at index 0</li></ul><ul id="ca720e56-cdd7-46f1-868b-5690fff002ef" class="bulleted-list"><li style="list-style-type:disc">-1 index starts from the last element in the list<figure id="806c416c-6628-47ed-a2fc-2bd3116dea30" class="image"><a href="Untitled%2078.png"><img style="width:493px" src="Untitled%2078.png"/></a></figure></li></ul></div></figure><h3 id="d8b9cdb9-7b46-4b1e-a9d3-6c0321f7f2f0" class="">list() function</h3><p id="2ff58e65-4588-49ec-99ed-d92ffcadfff4" class="">can break any datatype into a list</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="98ea27ed-4b57-4f32-b812-88cd84471f63" class="code"><code class="language-Python">vowel_string = &#x27;aeiou&#x27;
print(list(vowel_string))
list(range(10))
list(range(1,5)) </code></pre><h3 id="9beee1b4-cf50-47d2-9e60-a846f3c31690" class="">To access an element</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="58e979f7-a111-4665-97f0-c52fc13fcd25" class="code"><code class="language-Python">names = [&quot;Liz&quot;, &quot;Sam&quot;, &quot;Alex&quot;, &quot;Tom&quot;]
names[0]</code></pre><h3 id="20639beb-c3f1-4dd7-aa9a-c3881ffc8ec1" class="">To change an element</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f926537e-e832-41f7-a10c-601ad15777fc" class="code"><code class="language-Python">names[1] = &quot;Bob&quot;</code></pre><h3 id="d48b0939-1a53-4416-a199-3a64e6e6a4af" class="">To get a sublist</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="dc322efa-d425-4b12-a370-834623253383" class="code"><code class="language-Python">names[1:3]</code></pre><h3 id="f6d5ff2e-2f74-4656-a57e-c97d7b76a608" class="">List methods</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="81df36a6-090c-435d-9854-416830d71af9" class="code"><code class="language-Python">data.reverse()
 data.sort()
 data.insert()
 data.remove()
 data.append()
 data.count()</code></pre><h3 id="40f77158-7589-4a96-8d58-0ee2a013997e" class="">List operators</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0f5cc5b6-eae9-4b40-9fcc-35244ade8e77" class="code"><code class="language-Python">#Addition operator
odd = [1, 3, 5]
print(odd + [9, 7, 5])
print(odd)

#Multiplication operator
odd = [1, 3, 5]
print(odd * 3)

#Del operator
my_list = [&#x27;p&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;b&#x27;, &#x27;l&#x27;, &#x27;e&#x27;, &#x27;m&#x27;]
# delete one item
del my_list[2]
print(my_list)
del my_list</code></pre></details></li></ul><ul id="a311aef1-193c-4c0c-9f9a-81b0a720f80e" class="toggle"><li><details><summary>tuple ()</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="d480ffea-b97e-4619-ac43-30399182d89e"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Immutable collection of elements. meaning it can’t change after creation <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d5f94423-a1b7-4228-aad8-d52b7f94f6fb" class="code"><code class="language-Python">fruit=(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;)</code></pre></div></figure><h3 id="837be5eb-5e34-4138-8490-42e4fdb9e4d2" class="">can allow functions to return multiple values in the form of a tuple</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="00ee4ac6-3863-4b99-b47d-70b38b597b2c" class="code"><code class="language-Python">def test():
	 return &#x27;abc&#x27;, 100, [0, 1, 2] #basicly a tuple even tho it doesn&#x27;t have () around it
a, b, c = test()
print(a)
# abc
print(b)
# 100
print(c)
# [0, 1, 2]</code></pre></details></li></ul><ul id="9bd13b7f-ec30-40dc-b9b1-4260ed2598ad" class="toggle"><li><details><summary>dictionary {} </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="7a6a725b-e30b-4591-b839-8dce85a223a8"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">collection of elements consisting of key value pairs<ul id="55d0c9d3-cc02-49fe-887c-831bac372906" class="bulleted-list"><li style="list-style-type:disc">similar to a map in java</li></ul><figure id="e9d44301-33b6-420e-ade2-474665bbf5dd" class="image"><a href="Untitled%2079.png"><img style="width:621px" src="Untitled%2079.png"/></a></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f05df200-356b-4885-87cb-56a535709976" class="code"><code class="language-Python">my_dict = {} # empty dictionary
my_dict = {1: &#x27;apple&#x27;, 2: &#x27;ball&#x27;} # dictionary with integer keys
my_dict = {&#x27;name&#x27;: &#x27;John&#x27;, 1: [2, 4, 3]} # dictionary with mixed keys
my_dict = dict({1:&#x27;apple&#x27;, 2:&#x27;ball&#x27;}) # using dict()
my_dict = dict([(1,&#x27;apple&#x27;), (2,&#x27;ball&#x27;)]) # from sequence having each item as a pair</code></pre><ul id="72d20bee-54de-4702-b03f-e0ddde0784d2" class="bulleted-list"><li style="list-style-type:disc">key needs to be unique</li></ul></div></figure><h3 id="e5d29cb9-10c4-4df6-baac-f0704ff337da" class="">loops through dictionary</h3><p id="6d334dec-843d-449e-a5bb-b0bc9d521355" class="">we cannot loop through a dictionary. we have to call a direcitonary method called items()</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="752d277e-d609-437a-bfc6-d095867c9a86" class="code"><code class="language-Python">squares = {1: 1, 3: 9, 5: 25, 7: 49, 9: 81}
for key, value in squares.items():
 print(key)
 print(value )</code></pre><p id="b16de299-1aa5-4941-a6f0-496735a06eb4" class="">
</p></details></li></ul><ul id="7d20c354-f02c-4729-901c-3a592f7f036c" class="toggle"><li><details><summary>Numpy and Matplotlib Library</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="e53a0e59-55b2-4d68-bb5f-592080831807"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">python libraries for math functions and creating graphs</div></figure><h3 id="26c57302-834b-4066-af3b-1816d0f6e980" class="">Import</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="92c4c541-7839-4a06-9d84-a61c87b6c9fb" class="code"><code class="language-Python">import matplotlib.pyplot as plt #module from matplotlib for visulizting using plots
import numpy as np
import numpy.random as rng


np.setprintoption(precistion = 2)</code></pre><h3 id="8e72bb2c-825e-44ef-8b25-84835805f5ba" class="">ones function</h3><p id="c14d3354-1959-4761-abb3-e456177ae119" class="">function from numpy to create a matrix</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d139d99a-16c9-4403-a9c4-9994130c2eb9" class="code"><code class="language-Python">z=np.ones([4,3], dtype=float) #4 rows and 3 columns 
z[1, 2] = 3.14 #access the second row (index 1) and third column (index 2) point in the matrix and set it to pi</code></pre><figure id="9e23bb00-26e6-45ec-906a-e5154157ac90" class="image"><a href="Untitled%2080.png"><img style="width:214px" src="Untitled%2080.png"/></a></figure><h3 id="a901c4c3-13b8-4c6d-bf41-7ee68e8896de" class="">randint, sort, imshow</h3><p id="1aa71d39-7c9a-4b65-bacb-1331bd26f53c" class="">rng function generates a random number</p><p id="7cdc8b72-e1b4-4ffc-8f12-6417f16ad660" class="">randint take two arguments, the range and size</p><ul id="5586744e-f08b-4ff0-a3a6-7a3d09ef7c1c" class="bulleted-list"><li style="list-style-type:disc">number generated will be between the range<ul id="2698e067-a540-40fc-be04-d669778c72ef" class="bulleted-list"><li style="list-style-type:circle">10 is exclusive so the number will never be 10</li></ul><ul id="e133b6c1-0a68-484a-8f61-c0f32131d9e4" class="bulleted-list"><li style="list-style-type:circle">0 is inclusive so the number can be 0</li></ul></li></ul><ul id="c50f77f7-2367-4bc7-84c8-48141885ddc5" class="bulleted-list"><li style="list-style-type:disc">the size species the size of the matrix<ul id="8211c83d-b13e-4d01-8270-97c7fec0b52e" class="bulleted-list"><li style="list-style-type:circle">(4,6) specifies that the matrix will have 4 rows and 6 columns </li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0d3c5439-cb6c-435e-84f9-ab53e114f27b" class="code"><code class="language-Python">A = rng.randint(0, 10, size=(4,6))
A = np.sort(A, axis=0) #sorts and rotates
print(A) #prints matrix
plt.imshow(A, cmap=&quot;Blues&quot;) #plots graph of matrix where elements are blue squares with relative shading based on the element value</code></pre><figure id="11fd7aba-fe10-4931-9386-531dda31375a" class="image"><a href="Untitled%2081.png"><img style="width:618px" src="Untitled%2081.png"/></a></figure><p id="312c1a41-bfe9-47eb-924d-4d5d50ae8310" class=""><div class="indented"><p id="de22c2b1-bd61-4325-b331-2871d3812596" class="">
</p></div></p></details></li></ul></details></li></ul><ul id="be0be6b6-95c1-4f93-849e-e771f6aca289" class="toggle"><li><details><summary>Panda &amp; SkLearn Library</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="8b734d26-9807-41b7-b685-6c95fcc03d24"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">python libraries used for machine learning</div></figure><ul id="b5be48d6-036d-4c15-93f8-f0ae062cd74c" class="toggle"><li><details><summary>Panda Functions</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c0dbd649-7066-44dc-9fa8-86beda51cff2" class="code"><code class="language-Python">import pandas as pd</code></pre><h3 id="50742dc5-3962-48cf-811c-2e229e13d748" class="">Dataframe</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c5b688e8-39e7-4232-a0b1-7ead51e64b45" class="code"><code class="language-Python"># from the cheatsheet....
df = pd.DataFrame(
    [[4, 7, 10],
    [5, 8, 11],
    [6, 9, 12]],
    index=[1, 2, 3],
    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])

df</code></pre><figure id="ecc03961-5541-4e09-afd5-6cdc46f17aaf" class="image"><a href="Untitled%2082.png"><img style="width:88px" src="Untitled%2082.png"/></a></figure><h3 id="ba0e189d-f44a-4fe1-86b4-3dc7d06afedb" class="">Describe dataframe</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="206ec917-c428-4d68-8319-6de1aee537c9" class="code"><code class="language-Python">df.describe()</code></pre><figure id="eb03333c-d34c-4cc5-85a5-af2afefc0ebe" class="image"><a href="Untitled%2083.png"><img style="width:146px" src="Untitled%2083.png"/></a></figure><h3 id="9cc056ef-a30e-407e-8450-a7e2b2527f3d" class="">Reading csv</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="5b2f8ec9-9490-4577-b14f-eb448b0db393" class="code"><code class="language-Python">df = pd.read_csv(&#x27;banknote.csv&#x27;) #, index_col=0)
df.head()</code></pre><h3 id="2aa3f625-6901-4323-9a2a-7f955b8b3e0f" class="">Rename columns</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a3b49d56-8523-4afb-8cf4-5142aaa19379" class="code"><code class="language-Python">print(&quot;original names: &quot;, df.columns)
df.columns = [&#x27;dogs&#x27;, &#x27;cats&#x27;, &#x27;pigs&#x27;, &#x27;sheep&#x27;, &#x27;Class&#x27;]
df.tail()</code></pre><h3 id="b22dd0cb-b603-442d-ba6a-cae20d35be8f" class="">Add to Dataframe</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="5eb4e523-0bf8-4dd3-86e9-d587eee5a508" class="code"><code class="language-Python">df[&#x27;new&#x27;] = df.dogs * df.dogs</code></pre><h3 id="6eddef8a-bdf1-436c-92a8-f1921f75b5d3" class="">Plot graph</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="af4de9e4-1c65-4f07-ad4f-258f8966683c" class="code"><code class="language-Python">df.plot.hist(y=[&#x27;cats&#x27;]) #histogram
df.plot.hist(y=[&#x27;cats&#x27;,&#x27;pigs&#x27;]) #histogram with two variables
df.plot.scatter(x=&#x27;cats&#x27;, y=&#x27;pigs&#x27;) #scatter plot</code></pre></details></li></ul><ul id="35bb9ec7-0cee-4812-863b-2fda15642808" class="toggle"><li><details><summary>SkLearn Functions</summary><h3 id="33a7bfc0-c36e-4891-8cad-ec5b980cda7a" class="">Load dataset</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a5b020ad-7bd6-40e0-91c1-efb7dba86298" class="code"><code class="language-Python">#import the required libraries
from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

iris = datasets.load_iris()
#data # this is like a dict so far, not a dataframe...

x = iris.data
labels =  iris.target

print(type(x))
print(type(labels)) 

print(x.shape)
print(labels.shape)

print(labels)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e3d56609-34c8-49a5-a0a9-2570b1f2a4c8" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">&lt;class &#x27;numpy.ndarray&#x27;&gt;
&lt;class &#x27;numpy.ndarray&#x27;&gt;
(150, 4)
(150,)
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]</code></pre><h3 id="cd77ce2f-d946-4e9c-9871-1b8a284fd6b9" class="">Random Numbers &amp; seed</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="96339d92-b0c1-4862-a4b0-240feb001e8c" class="code"><code class="language-Python">#Random number generator need a seed
#supply a specific seed to the random generator,
#you will get the same numbers every time you execute a program
import random

random.seed(130)
print(random.random())

import numpy.random as rng # random number generator

rng.seed(130)
print(rng.random())
#random_state simply sets a seed to the random generato
#rn = rng.RandomState(10)
#print(rn.random())
</code></pre><h3 id="6e18d5e3-45c8-487d-8a19-a280b9d89930" class="">Training Test Split</h3><p id="6642fdf7-48f1-4ce2-8eb8-faabe2c7b5b3" class="">splits set into training set and test set</p><p id="f0b9cafc-b820-4ffc-a69e-6a0d881f8ce8" class="">x = data points</p><p id="5a3a36d6-9652-457a-91da-fc36eec007d1" class="">y = target class labels</p><p id="789daa60-3bf4-44f4-afe5-769bdc77856b" class="">test_size = % of data set used for the test set. the remaining will be for training set</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0c08850d-34c4-45fe-92d1-022afa960303" class="code"><code class="language-Python">from sklearn.model_selection import train_test_split

trainingDataPoints, testDataPoints, trainingClassLabels, testClassLabels
= train_test_split(x, labels, test_size=0.3, random_state=309, shuffle=True)
    #returns 4 outputs
print(ta_x.shape)
print(te_x.shape)
print(ta_y.shape)
print(te_y.shape)

# print(ta_x)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="7e0c7c0c-0ce6-4704-aeb1-6173d2292ccb" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">(105, 4)
(45, 4)
(105,)
(45,)</code></pre><h3 id="d15bb3a0-3aaa-4f2b-a188-177795494471" class="">KN classifier </h3><p id="80485e17-98a3-4321-9bce-038d5791146a" class="">n_neighbors = k value </p><p id="b30cec9d-1440-45a9-ac85-58977c5bac8e" class="">
</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="55922f80-8196-479b-8eea-1648d99752db" class="code"><code class="language-Python">#KNN, setting k - &quot;n_neighbors&quot; 
from sklearn.neighbors import KNeighborsClassifier

neigh = KNeighborsClassifier(n_neighbors=3) #, weights=&quot;distance&quot;)# by default-&quot;uniform&quot;
neigh.fit(ta_x, ta_y)
pre_y = neigh.predict(te_x[0:1])

print(&quot;Predictions:&quot;+str(pre_y))

#print(&quot;Labels     :&quot;+str(te_y))
#neigh.predict(te_x[20:30])
#neigh.predict_proba(te_x[20:30])</code></pre><p id="44cc0925-35b8-480a-89eb-33af228fa879" class="">fit function is used for training</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="4c3fc967-c741-4785-bbe4-734942bdf5d0" class="code"><code class="language-Python">classifier.fit(dataPoints, classLabels)</code></pre><p id="e0d06eff-4ea9-4605-8832-d0823eab622c" class="">predict function tests the classifier by trying to predict the class labels for the input test data point</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a64649f2-17c9-4c52-ac12-6c6e9d03f36d" class="code"><code class="language-Python">pre_y = neigh.predict(te_x[0:1])
</code></pre><h3 id="f8368d0a-dd4c-4188-bb99-8c8cf64d159b" class="">accuracy_score()</h3><p id="9695104d-5d8b-49c2-a7b2-06fd5477227e" class=""><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score</a></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e2d29cfb-bef3-44bd-b2d4-855113dcba13" class="code"><code class="language-Python"></code></pre><h3 id="2ca3b778-aa4d-41c5-b72a-bc951dc49f1e" class="">LogisticRegression classifier </h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a2bd787a-fc90-421d-8242-ddd474d57232" class="code"><code class="language-Python">from sklearn.linear_model import LogisticRegression

# Create an instance of Logistic Regression Classifier and fit the data.
logreg = LogisticRegression(C=10.) # optinal: max_iter=100
logreg.fit(ta_x, ta_y)
print(logreg.coef_)

#[[-0.36413869  2.05357592 -4.08650706 -1.87455824]
# [ 1.12061041 -0.19239231 -0.97905595 -1.61659555]
# [-0.75647172 -1.86118361  5.06556301  3.49115379]]


logreg = LogisticRegression(C=0.3)
logreg.fit(ta_x, ta_y)
print(logreg.coef_)

#[[-0.35847397  0.53398199 -1.54062167 -0.61441525]
#[ 0.16995085 -0.42594791 -0.02855264 -0.40508516]
#[ 0.18852312 -0.10803408  1.56917432  1.01950041]]

logreg = LogisticRegression(C=1, max_iter=100)

logreg.fit(ta_x, ta_y)

logreg_pre_y = logreg.predict(te_x[0:5])

print(&quot;Predictions:&quot;+str(logreg_pre_y))
print(&quot;Labels     :&quot;+str(te_y[0:5]))
#Predictions:[0 2 0 1 2]
#Labels     :[0 1 0 1 2]</code></pre><p id="d1a12db2-4045-44c2-9a01-59b58b6ec897" class="">
</p><p id="63519f06-9696-4032-9bca-b3e605da4c64" class="">
</p></details></li></ul></details></li></ul><ul id="318823f4-188a-450c-9b9c-92aef559357e" class="toggle"><li><details><summary>SkLearn Data Processing</summary><figure id="96060344-458f-4099-beef-a561e66aa742"><a href="https://vuw.zoom.us/rec/play/zwnKRpPeb8Nbn9Cf2tSu1-71g4uUg5DAtM1cQmSdDljwPYJY7uiyjo79Z3P1B3bHiNjyPrjaZdw8-U8F.4Rdys-RoOs937Np9?canPlayFromShare=true&amp;from=share_recording_detail&amp;continueMode=true&amp;componentName=rec-play&amp;originRequestUrl=https://vuw.zoom.us/rec/share/i7z4U6rfuWiJLibtYXdYPkPh8AO9aMThXpR3OJ3zQ995ZgqVAvBQhZvfjiYebXlT.B0r-neghU4UxHrPl" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Video Conferencing, Web Conferencing, Webinars, Screen Sharing</div><div class="bookmark-description">Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference room solution used around the world in board, conference, huddle, and training rooms, as well as executive offices and classrooms. Founded in 2011, Zoom helps businesses and organizations bring their teams together in a frictionless environment to get more done. Zoom is a publicly traded company headquartered in San Jose, CA.</div></div><div class="bookmark-href"><img src="https://st1.zoom.us/zoom.ico" class="icon bookmark-icon"/>https://vuw.zoom.us/rec/play/zwnKRpPeb8Nbn9Cf2tSu1-71g4uUg5DAtM1cQmSdDljwPYJY7uiyjo79Z3P1B3bHiNjyPrjaZdw8-U8F.4Rdys-RoOs937Np9?canPlayFromShare=true&amp;from=share_recording_detail&amp;continueMode=true&amp;componentName=rec-play&amp;originRequestUrl=https://vuw.zoom.us/rec/share/i7z4U6rfuWiJLibtYXdYPkPh8AO9aMThXpR3OJ3zQ995ZgqVAvBQhZvfjiYebXlT.B0r-neghU4UxHrPl</div></div><img src="https://st3.zoom.us/static/6.3.20541/image/thumb.png" class="bookmark-image"/></a></figure><h3 id="23676d26-b781-416c-93d5-9c16dbd27173" class="">Encoding categorial data</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="3c539a8d-c1d1-4181-a060-ee3cf39bd138" class="code"><code class="language-Python">from sklearn.preprocessing import OrdinalEncoder
import numpy as np

#create encoder object
encoder_sex = OrdinalEncoder(handle_unknown = &#x27;use_encoded_value&#x27;, unknown_value=np.nan)
X_titanic_train_encoded = X_titanic_train.copy()

#transform the old data set with categorical features to 
#the new data set with encoded values

X_titanic_train_encoded[&#x27;sex&#x27;] = encoder_sex.fit_transform(X_titanic_train_encoded[&#x27;sex&#x27;].values.reshape(-1, 1))
X_titanic_test_encoded=X_titanic_test.copy()

X_titanic_test_encoded[&#x27;sex&#x27;] = encoder_sex.transform(X_titanic_test_encoded[&#x27;sex&#x27;].values.reshape(-1, 1))
X_titanic_train_encoded</code></pre><p id="4a8a99e6-ea42-4549-945a-533993f03572" class=""><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html</a></p><h3 id="3cedb97b-6145-4bda-a04c-a087abad38ae" class="">Impute missing values</h3><p id="d6336cec-7211-45f2-b5de-fd6168eaa4c8" class="">SimpleImputer using mean/mode</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="28654b31-5bf7-46fa-a02e-4859decc0c16" class="code"><code class="language-Python">
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy=&#x27;mean&#x27;)
X_titanic_train_impute = imputer.fit_transform(X_titanic_train_encoded)
X_titanic_test_impute = imputer.transform(X_titanic_test_encoded)
X_titanic_train_impute </code></pre><p id="d898c9ed-2c4a-4cc7-a836-1abf667866e5" class="">KNNImputer using knn</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ebcf747d-f14e-4685-b30c-4c20f50c7bb5" class="code"><code class="language-Python">from sklearn.impute import KNNImputer

eX_titanic_train_impute = imputer.fit_transform(X_titanic_train_encoded)
X_titanic_test_impute = imputer.transform(X_titanic_test_encoded)
X_titanic_train_impute </code></pre><p id="b4876930-2ebe-4a50-beaa-ec4784015453" class="">checking performance of dataset after imputing missing values</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c3ea9bf4-3e04-4ae4-9534-429439170273" class="code"><code class="language-Python"># Checking the performance
from sklearn.metrics import balanced_accuracy_score
from sklearn.neighbors import KNeighborsClassifier as KNN
clf = KNN(n_neighbors=5)
clf.fit(X_titanic_train, y_titanic_train)
y_titanic_test_pred = clf.predict(X_titanic_test)
acc = balanced_accuracy_score(y_titanic_test, y_titanic_test_pred)
acc</code></pre><h3 id="328245b7-497b-4076-b3b9-293b47f0c85a" class="">Normalization/Standardization </h3><figure id="07a1945f-d9d3-44a1-a8e3-39f96b5de26d"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">sklearn.preprocessing.StandardScaler</div><div class="bookmark-description">Examples using sklearn.preprocessing.StandardScaler: Release Highlights for scikit-learn 1.4 Release Highlights for scikit-learn 1.2 Release Highlights for scikit-learn 1.1 Release Highlights for s...</div></div><div class="bookmark-href"><img src="https://scikit-learn.org/stable/_static/favicon.ico" class="icon bookmark-icon"/>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</div></div></a></figure><figure id="0b0f512c-c347-4499-853e-1f2d4a1b30d9"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">sklearn.preprocessing.MinMaxScaler</div><div class="bookmark-description">Examples using sklearn.preprocessing.MinMaxScaler: Release Highlights for scikit-learn 0.24 Image denoising using kernel PCA Time-related feature engineering Univariate Feature Selection Scalable l...</div></div><div class="bookmark-href"><img src="https://scikit-learn.org/stable/_static/favicon.ico" class="icon bookmark-icon"/>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</div></div></a></figure><h3 id="f44576a5-9a39-4e8c-8d99-247ed99a3301" class="">Feature Selection</h3><blockquote id="8be13585-134e-4af5-8c1b-a1d9fd9aed4e" class="">We can calculate the relevance between each feature and the class label and then select the top ranked one<br/>In this tutorial, I will use Pearson correlation to do Feature Ranking<br/></blockquote><p id="35b4d24f-0b6d-49d8-96f7-496cc586a77f" class="">to get the correlation between features we can use the r_regression function</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="4a730ec8-039d-4a22-a48a-e93b6e5270d7" class="code"><code class="language-Python"># convert from Panda dataframe to numpy
X_titanic_train, X_titanic_test = X_titanic_train.values, X_titanic_test.values
y_titanic_train, y_titanic_test = y_titanic_train.values, y_titanic_test.values

from sklearn.feature_selection import r_regression
cor = r_regression(X_titanic_train, y_titanic_train)
cor</code></pre><p id="e2ff59a3-1125-4bdd-80c0-24dea92402a1" class="">we can more clearly see the correlation by removing the sign via the absolute value and sort the array </p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="221ce0c6-a67a-432b-8dfe-0d329114b308" class="code"><code class="language-Python"># We must calculate the absolute value
cor = np.abs(cor)
sorted_features = np.argsort(cor)[::-1]
sorted_features</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c82fa1ca-2204-43bf-80f0-b47ae08ebfe3" class="code"><code class="language-Python">sel_feas = sorted_features[:3]
sel_feas</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="69f272a4-4ee4-4ea6-9f53-c7812613d3fb" class="code"><code class="language-Python"># Evaluate the performance
clf.fit(X_titanic_train[:, sel_feas], y_titanic_train)
y_titanic_test_pred = clf.predict(X_titanic_test[:, sel_feas])
acc = balanced_accuracy_score(y_titanic_test, y_titanic_test_pred)
acc</code></pre><p id="4a7b0de1-1e84-4b1c-af0a-e8f2b755c547" class="">we can find redundancies in our features by making a feature to correlation table</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="16fcad28-7b46-40be-a2c3-6b351fc8dc65" class="code"><code class="language-Python"># Correalation between selected features, any redundancy?
import numpy as np
cor_feas = np.abs(np.corrcoef(X_titanic_train[:, sel_feas].T))
cor_feas</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="cb6c0511-87fe-465a-b56e-11839beb11ef" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">array([[1.        , 0.11680316, 0.22434893],
       [0.11680316, 1.        , 0.03890795],
       [0.22434893, 0.03890795, 1.        ]])</code></pre><p id="d101d87f-76d7-43fc-95e7-cf56a155812e" class="">
</p></details></li></ul><ul id="638283d2-e6e2-415b-b45a-42a5db89aeed" class="toggle"><li><details><summary>SciKitLearn Feature construction &amp; PCA/ICA</summary><p id="021474e0-83d4-476a-93fa-94464d46529d" class="">
</p></details></li></ul><ul id="41dbb095-4924-4924-b585-e3100cd273e3" class="toggle"><li><details><summary>Pytorch</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="32f3e679-9e79-41bb-8209-9cc7372adcab"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">machine learning framework for python<ul id="1c6cf6e7-e949-4fa4-b347-08b0ec3f3c8c" class="bulleted-list"><li style="list-style-type:disc">used for tasks such as neural networks, natural language processing, image generation, speech recognition, reinforcement learning</li></ul><figure id="ab5f1ac1-4d23-49c1-8b26-e05105a4931c" class="image"><a href="Untitled%2084.png"><img style="width:432px" src="Untitled%2084.png"/></a></figure></div></figure><ul id="d5604317-b2ff-4630-8291-f363973d07f6" class="toggle"><li><details><summary>Basics &amp; Tensors </summary><h3 id="159441bf-cdad-4f9a-9f1b-177e0101f703" class="">Install</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a7b1611f-2e41-4d01-9243-897a8f5ec245" class="code"><code class="language-Markdown">using pip: pip install torch torchvision torchaudio
using conda: conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch #repalce &quot;cudatoolkit&quot; with the appropriate version corresponding to your CUDA installation.
</code></pre><h3 id="b05cfc51-1ac7-4fd1-94b0-112cc53da2bb" class="">Tensors</h3><p id="ab175eb9-9872-4291-8201-bc5010de4d60" class="">pytorch uses Tensor data structures like numpy arrays</p><ul id="6d366177-01de-41e2-8ef6-4a9aebe3adf4" class="bulleted-list"><li style="list-style-type:disc">allows you to use gpu or cpu to process</li></ul><ul id="5337f2a2-c584-41db-aacb-36a9b08ac4bd" class="bulleted-list"><li style="list-style-type:disc">auto grad </li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="8e4eca62-f0c4-4a25-909d-741ed44bfeb9" class="code"><code class="language-Python"># Creating tensors
tensor_a = torch.tensor([1, 2, 3])  # A simple 1D tensor from a Python list
tensor_b = torch.tensor([[1, 2], [3, 4]])  # A 2D tensor from a list of lists
tensor_c = torch.zeros(3, 3)  # A 3x3 tensor initialized to zero
tensor_d = torch.ones(2, 2)  # A 2x2 tensor initialized to one
tensor_e = torch.rand(4, 4)  # A 4x4 tensor with random values

# Displaying the tensors
print(&quot;Tensor A:&quot;, tensor_a)
print(&quot;Tensor B:&quot;, tensor_b)
print(&quot;Tensor C:&quot;, tensor_c)
print(&quot;Tensor D:&quot;, tensor_d)
print(&quot;Tensor E:&quot;, tensor_e)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="eef84afb-fbce-4d25-bde3-9465555fc526" class="code"><code class="language-Plain Text">Tensor A: tensor([1, 2, 3])
Tensor B: tensor([[1, 2],
        [3, 4]])
Tensor C: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
Tensor D: tensor([[1., 1.],
        [1., 1.]])
Tensor E: tensor([[0.4141, 0.1674, 0.3127, 0.5849],
        [0.2709, 0.7657, 0.7401, 0.7397],
        [0.1840, 0.3389, 0.2495, 0.0961],
        [0.6792, 0.8373, 0.6464, 0.0633]])</code></pre><h3 id="efae3540-cd8f-4438-9f26-6e14a63f0ce4" class="">Tensor Operations</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="211ad5ec-083b-4e77-8051-4ab0292b9564" class="code"><code class="language-Python"># Basic operations on tensors
sum_tensors = tensor_a + tensor_a  # Element-wise addition
diff_tensors = tensor_b - tensor_b  # Element-wise subtraction
mult_tensors = tensor_d * tensor_d  # Element-wise multiplication
div_tensors = tensor_e / tensor_e  # Element-wise division

# Displaying results of operations
print(&quot;Sum of tensors A and A:&quot;, sum_tensors)
print(&quot;Difference of tensors B and B:&quot;, diff_tensors)
print(&quot;Multiplication of tensors D and D:&quot;, mult_tensors)
print(&quot;Division of tensors E and E:&quot;, div_tensors)

# More advanced operations
dot_product = torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))
matrix_mult = torch.mm(tensor_b, tensor_b)

# Displaying more complex operations
print(&quot;Dot product:&quot;, dot_product)
print(&quot;Matrix multiplication of Tensor B:&quot;, matrix_mult)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="019951a8-2ea9-4de2-8480-45ed9904840f" class="code"><code class="language-Plain Text">Sum of tensors A and A: tensor([2, 4, 6])
Difference of tensors B and B: tensor([[0, 0],
        [0, 0]])
Multiplication of tensors D and D: tensor([[1., 1.],
        [1., 1.]])
Division of tensors E and E: tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
Dot product: tensor(7)
Matrix multiplication of Tensor B: tensor([[ 7, 10],
        [15, 22]])</code></pre><h3 id="92d442c5-b221-4f29-87e1-831e16ac6303" class="">Random tensor</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ff3028af-6f0d-40c4-a4e1-7c52f40b169c" class="code"><code class="language-Python">torch.randn(4,5)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f4b781e0-fbe0-4d1d-a1ca-f4b09c6386b0" class="code"><code class="language-Plain Text">tensor([[ 0.2874, -1.0648, -1.3282,  1.8006, -0.4403],
        [-0.1754,  0.6757, -1.3833, -0.0856,  0.6371],
        [ 0.9765, -0.3588, -0.4264,  0.5399, -0.0535],
        [ 0.1576,  0.2735,  1.9238, -0.2700,  1.0261]])</code></pre><h3 id="460635e5-eda8-4c41-b125-c8f909d70d28" class="">Convert Numpy Array to Torch Tensor</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b2c8cd84-03a1-4ae7-94d0-e9203c543a45" class="code"><code class="language-Python"># transfer to numpy array; array from tensor -&gt; numpy 
tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])
numpy_array_a = tensor_a.numpy()

# transfer from numpy array: array from numpy to tensor
numpy_array_b = np.array([[1, 2, 3], [4, 5, 6]])
tensor_b = torch.from_numpy(numpy_array_b)</code></pre><h3 id="e87ea2ed-754c-417b-9a64-d85e22569882" class="">Calculating tensors with GPU</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ba2ea4b3-bc91-41a2-bbcf-90ca15e630d9" class="code"><code class="language-Python"># define the device of the created tensor
# Define a tensor on CPU, defaut 
tensor_cpu = torch.tensor([[1, 2, 3], [4, 5, 6]])
# Define a tensor on GPU (if available)
if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)          # Define the device as GPU, &quot;cuda:1&quot;, &quot;cuda:2&quot; if more than one cuda card
    tensor_gpu = torch.tensor([[1, 2, 3], [4, 5, 6]], device=device)  # Specify the device
    
#move GPU tensor to cpu
tensor_cpu = tensor_gpu.cpu()
#move CPU tensor to GPU
if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;) 
    tensor_gpu = tensor_cpu.to(device) </code></pre><h3 id="69e5ea0a-4af6-438c-a24e-3e40146384f2" class="">Require Grad</h3><p id="bf3bda78-6f57-41cd-a689-71c7fc8e3b86" class="">because pytorch is used for NN, we can set require grad for tensors to true so that the program can track the gradients for tensors which can be used for the gradient descent algorithm  </p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1c5d6768-fd80-4430-ac7d-e377f3e856b8" class="code"><code class="language-Python"># other properties of tensors
# Define tensors with requires_grad=True to track gradients
weight_x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
weight_y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

# Gradients are stored in .grad property of tensors involved in the computation
print(&quot;Gradient of weight_x:&quot;, weight_x.grad)
print(&quot;Gradient of weight_y:&quot;, weight_y.grad)</code></pre></details></li></ul><ul id="d9e3efaf-c191-46f6-8d70-8d7cbd2df025" class="toggle"><li><details><summary>importing data</summary><h3 id="7b341ca4-bbb4-4293-9a97-0ae4d57487da" class="">Get data</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="04575cee-f044-4753-af1d-7d14305f9250" class="code"><code class="language-Python">from sklearn.datasets import fetch_olivetti_faces
olivefaces = fetch_olivetti_faces()
type(olivefaces)

print(olivefaces.data.shape)
print(olivefaces.target.shape)</code></pre><p id="98864dfd-27ae-4598-a1d0-9f30ea48f730" class="">#(400, 4096)<br/>#(400,)<br/></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a99e9736-5642-4a4c-90f8-d38ab226ed97" class="code"><code class="language-Python">i = rng.randint(0,400)   #pick one at random to check
plt.imshow(olivefaces.data[i,:].reshape(64, 64), cmap=&#x27;gray&#x27;)
print(i, &quot; has the identity of  &quot;, olivefaces.target[i])</code></pre><figure id="867dd657-38e0-4126-8fbc-e4940d743839" class="image"><a href="Untitled%2085.png"><img style="width:444px" src="Untitled%2085.png"/></a></figure><h3 id="900056d1-98de-41f2-a749-bce5c9bbe8da" class="">Convert input data into torch tensors</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="293f23fb-c375-4789-a252-eb2886080836" class="code"><code class="language-Python">print(olivefaces.data.dtype)
data = torch.from_numpy(olivefaces.data).float()
targ = torch.from_numpy(olivefaces.target)
print(data.dtype)</code></pre><p id="e40e789f-8cc9-4d69-90d2-a3b99f446642" class="">float32<br/>torch.float32<br/></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="830b2132-96a8-4845-8582-b827c7a02713" class="code"><code class="language-Python">#matplot can also use tensors</code></pre><figure id="82d7d64e-0cf3-4ee5-aa72-03884234728b" class="image"><a href="Untitled%2086.png"><img style="width:441px" src="Untitled%2086.png"/></a></figure><p id="4b4054a7-3bba-45d0-89fd-806fcfd5b454" class="">
</p></details></li></ul><ul id="f123a768-05e5-4c59-83ba-af78b3306011" class="toggle"><li><details><summary>building NN without high level APIs</summary><h3 id="64c67a65-9aff-48d6-8691-3cdc2712029c" class="">Building Network without torch.nn</h3><blockquote id="b49ca27d-1c58-4d32-9662-d169bef4cd3c" class="">Now we will firt try to construct a neural network using the PyTorch library but without relying on any higher-level APIs or frameworks that may simplify the process. A good structure to adopt is the make the network a class, with two methods <strong>init</strong>() to set up the tensors forward() to define the computational graph Let&#x27;s try to build a network with one hidden layer, so in terms of parameters we will need: two weights matrices, but different shapes two vectors of &quot;bias weights&quot; (not connected to inputs) We give these random starting values, and don&#x27;t forget to tell torch to track gradients through them. We are going to use 𝑧 to refer to the input activation level for a neuron, and we&#x27;ll use the relu (rectified linear) function as our non-linearity.</blockquote><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="921f3351-49da-4230-b1cc-1312f0aa4d2b" class="code"><code class="language-Python">class Net():
    def __init__(self):
        nHids = 5000
        self.w1= 0.1 * torch.randn(4096,nHids)      # weights to take us from those points to some hidden units
        self.b1= 0.1 * torch.randn(nHids)         # one bias, for each hidden unit
        self.w2= 0.1 * torch.randn(nHids,40)      # ditto for the second layer, but there&#x27;s only one output
        self.b2= 0.1 * torch.randn(40)            # just one bias, on the sole output

        self.w1.requires_grad = True
        self.b1.requires_grad = True
        self.w2.requires_grad = True
        self.b2.requires_grad = True


    def forward(self, x):
        z1 = torch.matmul(x,self.w1) + self.b1  # often have to fiddle to get shapes right
        h = (z1&gt;0.0) * z1                       # ReLU!
        z2 = torch.matmul(h,self.w2) + self.b2  # z2 is the weighted output.
        return z2</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="befddf92-7a10-47d8-8452-fc2dc35d51be" class="code"><code class="language-Python"># quick sanity check
net = Net()
net.forward(data)[0:4,:]</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="cfa0ff76-35ad-4228-bb7b-43c60ae6c611" class="code"><code class="language-Plain Text">tensor([[-10.8361, -21.0987, -35.8053, -15.5106, -23.1364,  11.5905,  -1.2215,
          17.8784,  23.3257,  -8.4359,   5.1362, -11.7064, -25.2742, -14.3353,
           2.5499,   1.8157,   3.0675, -22.2904,   4.8929,   7.0598, -13.9815,
          -8.3661,   9.0917, -15.4458,  -6.7715,  -5.8376,  -8.2008, -20.2398,
           1.1692,  14.2493,  10.8121,  -8.5421, -35.5018,  14.0591, -28.2007,
         -19.0841,  -9.0126, -12.5953, -30.8081,  22.8911],
        [ -2.6834, -27.9529, -28.7208, -14.1451, -16.3894,  23.6345,  -4.2454,
          13.7133,  21.0219, -13.1423,   3.6091, -17.4957, -35.7123,  -9.0710,
           5.6297,   5.5052,  -4.9968, -16.9229,   5.9966,   2.1685,  -4.3803,
          -6.9048,   4.3604, -12.0125,   1.4484,  -7.6398,  -5.1458, -22.9356,
          -3.0459,   9.1724,  11.1093,   1.4979, -43.7888,   5.0376, -26.8418,
         -15.3065, -11.8259,  -8.2908, -32.1930,  18.1213],
        [-11.8311, -18.3056, -28.2722, -14.6129, -22.8598,   9.9689,  -0.4175,
          11.5350,  21.3484, -12.9619,   6.9276, -12.2045, -23.7812,  -9.4961,
           4.1670,  -2.6103,   4.0864, -21.3204,   1.5615,   3.2363, -10.7991,
          -8.3220,   7.4172, -14.0647,  -5.7504,  -6.1303,  -7.7525, -20.2464,
          -1.0492,  13.9802,   9.7482,  -8.5766, -36.2958,  12.4025, -24.0849,
         -14.3581,  -7.0153, -13.0041, -29.9884,  23.8908],
        [ -3.5226, -12.1409, -31.9101, -12.1177, -19.2670,  19.6354, -10.0384,
           0.3620,  22.7313,  -4.0724,  23.6457, -11.8695, -19.2239,  -1.7555,
          -4.3784,  -7.6352,  10.7871, -22.5125,   3.2807,   5.5687, -10.7162,
           1.2851,  10.8962,  -7.0146, -18.0139,  -1.6887,  -7.2285, -24.4480,
          -6.7702,  16.0929,  13.2509, -12.4741, -29.4146,  19.2614, -30.2860,
         -16.2897,  -2.2617, -15.9282, -19.9602,  19.9882]],
       grad_fn=&lt;SliceBackward0&gt;)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="fca25cb0-e813-4b16-bde1-8b544c598731" class="code"><code class="language-Markdown">### NB: the output looks like it hasn&#x27;t done a classification yet
You might think we should return the output of doing a `softmax` operation at the end of `forward` (to get predicted probabilities of the classes), and then use the negative log Loss for classification (e.g. `torch.nn.NLLLoss`).

INSTEAD, here we simply return z2 just as it is, and use [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) from the `torch.nn` library instead.

_This does the softmax part itself internally_.

Doing it this way means we avoid having to include a numerical trick that is needed for doing softmax on large numbers.

Incidentally, the technical term for those &quot;naked&quot; z2 outputs is &quot;logits&quot;.</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a19343ca-daa6-4eb7-9d72-a9e65ada357a" class="code"><code class="language-Python">lossFn = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.SGD([net.w1, net.b1, net.w2, net.b2], lr=0.01) # other alternatives: torch.optim.Adam(), explore more!
optimizer</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0b89798d-68c6-4696-8b40-67ce23bf0687" class="code"><code class="language-Plain Text">SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d4a30545-6f42-4b31-9bf4-c769dd28aa76" class="code"><code class="language-Python">def train_show(network, data, targ, lossFunc, optimiser, epochs):
    lossHistory = []  # just to show a plot later...
    accuHistory = []

    for t in range(epochs):
        optimiser.zero_grad()      # Gradients accumulate by default, so don&#x27;t forget to do this.

        y = network.forward(data)  # the forward pass

        loss = lossFunc(y,targ)    # recompute the loss
        loss.backward()            # runs autograd, to get the gradients needed by optimiser(you will learn about this in Week 9)
        optimiser.step()           # take a step

        # just housekeeping and reporting
        accuracy = torch.mean((torch.argmax(y,dim=1) == targ).float())
        lossHistory.append(loss.detach().item())
        accuHistory.append(accuracy.detach())

    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.plot(lossHistory,&#x27;r&#x27;); plt.title(&quot;loss&quot;); plt.xlabel(&quot;epochs&quot;)
    plt.subplot(1,2,2)
    plt.plot(accuHistory,&#x27;b&#x27;); plt.title(&quot;accuracy&quot;)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="35790714-63dd-4dd4-b9b5-9a7e9f6cb3ee" class="code"><code class="language-Python"># Take it from the top!
net = Net()
lossFn = torch.nn.CrossEntropyLoss() # see note above

optimiser = torch.optim.SGD([net.w1, net.b1, net.w2, net.b2], lr=0.01)

train_show(net, data, targ, lossFn, optimiser, 200)</code></pre><figure id="6da8236c-fcd5-415f-b871-92cb7cab3688" class="image"><a href="Untitled%2087.png"><img style="width:808px" src="Untitled%2087.png"/></a></figure></details></li></ul><ul id="77192e09-5643-463d-8b63-9813c8cf947b" class="toggle"><li><details><summary>building NN with torch.nn</summary><h3 id="0203f247-6533-4941-bcc8-f017ad84b00b" class="">Import</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="24bb2f6d-ee24-4e34-97b3-16a9fbb2660d" class="code"><code class="language-Python">import torch.nn.functional as F</code></pre><h3 id="2a42bb19-dfef-4774-96a6-07efd7d1e469" class="">Network Class</h3><blockquote id="e6a791fd-057a-40e9-85dd-0eadea19aec8" class="">We&#x27;ll proceed as before, but make our Net a subclass of torch.nn.Module.<br/>one consequence is that, by default, parameters in __init__() get initialised and have requires_grad set true<br/>    a Linear layer takes care of any biases, as well as the weights<br/>    Notice the reuse of x in forward() seems gratuitous, but makes adding more layers trivial<br/><br/></blockquote><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="5ae66c44-c42e-4030-9666-34d5dbf13e77" class="code"><code class="language-Python">class OtherNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # here we set up the tensors......
        self.layer1 = torch.nn.Linear(4096, 5000) #input layer
        self.layer2 = torch.nn.Linear(5000, 40) #output layer

    def forward(self, x):
        # here we define the (forward) computational graph,
        # in terms of the tensors, and elt-wise non-linearities
        x = F.relu(self.layer1(x))
        x = self.layer2(x)
        return x</code></pre><h3 id="9fb2da6f-2fa7-4bb1-bbe9-ce6b398b14d0" class="">to check if the network has been fully built</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="62d6c73a-7f87-4029-8dad-6b0cac501fd0" class="code"><code class="language-Python"># sanity check
othernet = OtherNet()
y = othernet.forward(data)
print(y)</code></pre><p id="f70078ff-f140-46df-b5ce-4d85d252c6bf" class="">tensor([[-0.3768,  0.1096, -0.0221,  ..., -0.0577,  0.0441,  0.0901],<br/>[-0.3164,  0.0610, -0.0682,  ..., -0.0614,  0.0721,  0.0846],<br/>[-0.3284,  0.0707, -0.0306,  ..., -0.0580,  0.0787,  0.0961],<br/>...,<br/>[-0.3182,  0.0385, -0.0750,  ..., -0.0541,  0.0627,  0.0692],<br/>[-0.3604,  0.1424, -0.0920,  ..., -0.1002,  0.0465,  0.0942],<br/>[-0.3192,  0.0920, -0.0888,  ..., -0.0808,  0.0562,  0.0906]],<br/>grad_fn=&lt;AddmmBackward0&gt;)<br/></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="109ca4f8-448b-4dc8-9aa1-d8bffdfa9f40" class="code"><code class="language-Python">naked = othernet.forward(data).detach()
naked[0,:]</code></pre><p id="fd24c9f8-097c-47be-b41e-0be1604636ae" class="">naked = othernet.forward(data).detach()<br/>naked[0,:]<br/></p><h3 id="11b92c9e-da8a-4bf5-ae8d-a2ae5e63538a" class="">train show</h3><p id="a3177c4f-5e6e-482d-a9e9-f4ce3b96262a" class="">function</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ae9bc3e2-f076-4b51-a6f7-ea21e8526cf0" class="code"><code class="language-Python">#Given a defined NN structure, how to train (optimize) it:
def train_show(network, data, targ, lossFunc, optimiser, epochs):
    lossHistory = []  # just to show a plot later...
    accuHistory = []

    for t in range(epochs):
        optimiser.zero_grad()      # Gradients accumulate by default, so don&#x27;t forget to do this.

        y = network.forward(data)  # the forward pass

        loss = lossFunc(y,targ)    # recompute the loss
        loss.backward()            # runs autograd, to get the gradients needed by optimiser(you have learned about it now :), 
                                   #will have more discussion later)
        optimiser.step()           # take a step

        # just housekeeping and reporting
        accuracy = torch.mean((torch.argmax(y,dim=1) == targ).float())
        lossHistory.append(loss.detach().item())
        accuHistory.append(accuracy.detach())

    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.plot(lossHistory,&#x27;r&#x27;); plt.title(&quot;loss&quot;); plt.xlabel(&quot;epochs&quot;)
    plt.subplot(1,2,2)
    plt.plot(accuHistory,&#x27;b&#x27;); plt.title(&quot;accuracy&quot;)</code></pre><p id="d396bfe1-0a87-4c88-98ad-29e4e81295cc" class="">using function</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a858261a-18ec-42e8-b912-5aff10cecf02" class="code"><code class="language-Python">othernet = OtherNet()
lossFunction = torch.nn.CrossEntropyLoss()

optimiser = torch.optim.SGD(othernet.parameters(), lr=0.01)
# Notice the handy &quot;net.parameters()&quot;. Before, this was
#optimizer = torch.optim.Adam([net.w1, net.b1, net.w2, net.b2], lr=0.01)

train_show(othernet, data, targ, lossFunction, optimiser, 200)</code></pre><figure id="9d296770-7c70-4f4b-aa17-3f2f04e2cc54" class="image"><a href="Untitled%2088.png"><img style="width:812px" src="Untitled%2088.png"/></a></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1da92bb3-3834-4160-a7be-15e45175bf8a" class="code"><code class="language-Python">lossFn = torch.nn.CrossEntropyLoss()
loss = lossFn(y, targ)
print(loss)</code></pre><p id="440f690f-de4f-455e-9f3e-ab8d3d4c92c8" class="">tensor(3.7016, grad_fn=&lt;NllLossBackward0&gt;)</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="7d47bfab-afd6-4714-b98e-ff8307475f4b" class="code"><code class="language-Python">#how about change to another optimiser Adam
optimizer = torch.optim.Adam(othernet.parameters(), lr=0.01)
train_show(othernet, data, targ, lossFunction, optimiser, 200)</code></pre><figure id="dda249d7-1619-493d-a6ad-1e676fa09187" class="image"><a href="Untitled%2089.png"><img style="width:624px" src="Untitled%2089.png"/></a></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0f49550f-77a9-44a8-a080-845660fd2db3" class="code"><code class="language-Python">#increase the learning rate, see what will happen, from 0.01-&gt;0.02-&gt;0.04-&gt;0.1
optimiser = torch.optim.SGD( othernet.parameters(), lr=0.04)

train_show(othernet, data, targ, lossFunction, optimiser, 200)</code></pre><figure id="7d660116-9e6f-4ec8-9ea4-6db010abfd52" class="image"><a href="Untitled%2090.png"><img style="width:819px" src="Untitled%2090.png"/></a></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="3220f376-e66f-427f-aeb6-8f9a2d864f11" class="code"><code class="language-Python">#increase or reduce the number of epoch 200-&gt;400, 200-&gt;100
optimizer = torch.optim.SGD(othernet.parameters(), lr=0.01)
train_show(othernet, data, targ, lossFunction, optimiser, 400)</code></pre><figure id="ca38c8fd-6ffe-4683-9955-4dbe7588c322" class="image"><a href="Untitled%2091.png"><img style="width:624px" src="Untitled%2091.png"/></a></figure></details></li></ul></details></li></ul></details></li></ul><ul id="bf6c671f-c385-41b6-b0c7-7f4b681056ac" class="toggle"><li><details><summary>ML Models and Algorithms</summary><ul id="e058c02d-771a-4d12-8e63-91392016d027" class="toggle"><li><details><summary>Classification</summary><ul id="0c79382d-9b49-4d23-8caa-0d6f483a6385" class="toggle"><li><details><summary>Overview </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="b19e7980-26ec-4d30-adeb-7f59c7002aee"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">technological core of a machine learning system<ul id="859fca73-56b3-4888-9df6-2b09ffc37791" class="bulleted-list"><li style="list-style-type:disc">model is a theoretical biases on which classification is made<ul id="35f1d3e9-fa18-49eb-9bcd-043a9abe3e10" class="bulleted-list"><li style="list-style-type:circle">it is a mathematical function takes data points as an input and outputs predicted class labels </li></ul></li></ul><ul id="d91e0520-3804-4c72-ae81-3cc8a4954419" class="bulleted-list"><li style="list-style-type:disc">algorithm :the particular way we did it</li></ul></div></figure><ul id="c68e3b1d-2fb4-477c-b6d7-b35dce8b4364" class="bulleted-list"><li style="list-style-type:disc">purpose</li></ul><ul id="2921d9b7-c206-44cf-b9ba-a654892e5d3e" class="bulleted-list"><li style="list-style-type:disc">definition</li></ul><ul id="5083ffd1-ab2b-4545-85e3-c9175160fea1" class="bulleted-list"><li style="list-style-type:disc">different aspects</li></ul><ul id="8335dd4f-7d40-402c-803a-8f2562c0a7d7" class="bulleted-list"><li style="list-style-type:disc">Popular Classifiers<ul id="6cbaba78-28d1-4fb3-a07d-3250178d1053" class="bulleted-list"><li style="list-style-type:circle">KNN Classifier (K-nearest neighbors)<ul id="4be755c0-eea0-4030-bbf7-56faf4f8613b" class="bulleted-list"><li style="list-style-type:square">under feeding and over feeding</li></ul></li></ul><ul id="04d93395-dfab-43c3-ae0e-97eaa243fb75" class="bulleted-list"><li style="list-style-type:circle">bias - variance trade off</li></ul><ul id="cdfad1d3-0310-4276-843d-b50623a5629f" class="bulleted-list"><li style="list-style-type:circle">perceptron: mathematical model for single neuron <ul id="f2464bdb-8b96-4692-b409-8e2e000698e7" class="bulleted-list"><li style="list-style-type:square">multilayer perceptron (MLP)</li></ul></li></ul><ul id="99b72d4a-9a28-4382-afde-8c1beabee890" class="bulleted-list"><li style="list-style-type:circle">Decision Trees</li></ul><ul id="b8f714ba-ca56-4dfc-8a96-e8a1204b83cc" class="bulleted-list"><li style="list-style-type:circle">SVM</li></ul><ul id="8410d0c0-3c15-4041-92a7-9a3f28a030bc" class="bulleted-list"><li style="list-style-type:circle">Ensemble: group of classifiers</li></ul><ul id="c700dd48-bcf0-4dd7-a3f4-9589ce54b0b6" class="bulleted-list"><li style="list-style-type:circle">Boosting/Bugging algorithm </li></ul></li></ul><ul id="e9d31470-957d-47e2-8cec-61c4f25ceac2" class="bulleted-list"><li style="list-style-type:disc">evaluation metrics<ul id="97002885-1b6f-4039-b9c1-31cb45535a08" class="bulleted-list"><li style="list-style-type:circle">accuracy  </li></ul></li></ul><ul id="c6aaeaab-ecb0-470b-9d21-48c98b19902c" class="bulleted-list"><li style="list-style-type:disc">decision tree<ul id="bb70d409-fa17-4687-ad6c-4eece09ab99f" class="bulleted-list"><li style="list-style-type:circle">learning algorithm </li></ul></li></ul></details></li></ul><ul id="dc48fc98-6594-4468-9160-e5a30d6c3995" class="toggle"><li><details><summary>Why classification </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ccb4fefd-5f65-4cfb-ab28-76f3b2fdcfd1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">two purposes for classification models</div></figure><h3 id="97d99579-6c9e-4c5d-8a5d-ccbc42b2fcac" class="">prediction model</h3><p id="43f4ca90-939f-4015-b981-403a9a9972c2" class="">given past data, construct a model that can take new data and classify it </p><ul id="d0eaf969-1c7f-4433-b684-f57c40e5def2" class="bulleted-list"><li style="list-style-type:disc">estimate the worth of the model: accuracy<ul id="f7da61d6-c523-48d8-806d-5c3517bdc2bd" class="bulleted-list"><li style="list-style-type:circle">we have two types of accuracy, training accuracy which is how correct the model is on the data that its trained on and test accuracy which is how correct the model is on new data<ul id="f2f64b7f-5d87-4e5f-9dcb-a730e5eb5690" class="bulleted-list"><li style="list-style-type:square">out focus is on the test accuracy</li></ul><ul id="97016cc0-561b-4021-a6cc-11fc2adae89e" class="bulleted-list"><li style="list-style-type:square">situations where training accuracy is high but test accuracy is low is due to overfitting</li></ul><ul id="7fa23a1a-57ea-4bdf-a2a1-597f74ffee30" class="bulleted-list"><li style="list-style-type:square">test set independent of training set (else overfitting)<ul id="b92c6dc3-847b-4bd6-96c5-fd478252f6a0" class="bulleted-list"><li style="list-style-type:disc">if dataset is used to select/tune models, it is called validation set</li></ul><ul id="d3de2484-1f9f-4e19-80bd-1d60c2420a64" class="bulleted-list"><li style="list-style-type:disc">validation set is a small part of the training set which is used to prevent overfitting by validating the each iteration of the training process by testing the model’s accuracy.</li></ul><ul id="0bb64ee0-3786-4bb8-a56b-0efa9f5a2f6d" class="bulleted-list"><li style="list-style-type:disc">if the accuracy goes down then we can stop training on that said to prevent over fitting</li></ul><ul id="d1fd1a53-f5bb-4871-a824-8890af9bc55c" class="bulleted-list"><li style="list-style-type:disc">validation set also helps us fine tune hyper parameters and find the best classification model</li></ul></li></ul></li></ul></li></ul><ul id="bad14972-414f-4f38-888f-0beba9be2abb" class="bulleted-list"><li style="list-style-type:disc">in order to train a prediction model the training set will have to be split into 3 parts, training, testing and validation sets</li></ul><h3 id="57987414-89cc-4443-9018-0f804a7e33ff" class="">Insight into dataset</h3><p id="c4fde70b-f77d-4972-be5e-bdef6b713e02" class="">identify any redundant features, features importance and feature relationships to provide insight into the object</p><ul id="77a29016-1b98-4e9c-acb1-7d284f2fdef9" class="bulleted-list"><li style="list-style-type:disc">observations in the training set are used for model construction</li></ul><ul id="7c7bdb80-c3f5-4e51-8f60-ff41d53242ce" class="bulleted-list"><li style="list-style-type:disc">models can be rules, decision trees and networks (interpretable)</li></ul><p id="a5f7461d-7f30-43a4-9edd-52b749d43f64" class="">
</p></details></li></ul><ul id="dae17f9e-040a-4355-a916-9b3c707ff7d1" class="toggle"><li><details><summary>Definition </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ff851a13-4109-465d-acdd-74401f1ddbcc"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">classification problem</div></figure><h3 id="2e9df672-6adb-4da2-af28-85894a443934" class="">search space (S)</h3><p id="12cad2b6-565f-4e57-ba40-4c0832f5d1fc" class="">high dimensional space containing set of all possible instances/all features for each data in the data set</p><figure id="cfde8c15-44c0-4d65-9c25-148d88a1282c" class="image"><a href="Untitled%2092.png"><img style="width:869px" src="Untitled%2092.png"/></a></figure><ul id="115a3dc7-46d6-4d49-ad64-c3ba0012a9cc" class="bulleted-list"><li style="list-style-type:disc">each data point has a specific location </li></ul><ul id="a148a29a-f46f-438d-8a9a-9ca8c5b8a5ce" class="bulleted-list"><li style="list-style-type:disc">each circle corresponds to a different data point in the search space</li></ul><ul id="968da260-4cc7-4a3c-9164-ca7fa66ae2b3" class="bulleted-list"><li style="list-style-type:disc">the lines are the<mark class="highlight-yellow_background"> decision boundaries </mark></li></ul><ul id="ba574f35-0b9c-44df-b289-94c1c5c4849a" class="bulleted-list"><li style="list-style-type:disc">areas separated by boundaries are called <mark class="highlight-yellow_background">decision regions </mark></li></ul><ul id="38475905-447c-40b5-9aa3-f1ece6251bb1" class="bulleted-list"><li style="list-style-type:disc">training a model finds/implements these boundaries and regions</li></ul></details></li></ul><ul id="5c26e235-18b7-440c-bd89-05e4eea83a42" class="toggle"><li><details><summary>Aspects </summary><h3 id="ff753784-fbaf-473a-931e-3b831082d871" class="">Balance</h3><ul id="c832cba8-1dc6-4156-bbdc-17fed0146128" class="bulleted-list"><li style="list-style-type:disc">in the graph labeled instances, the dataset is <mark class="highlight-yellow_background">not balanced</mark> because there are more negative data points</li></ul><ul id="230324fd-4485-4125-8322-c2200de8b2f4" class="bulleted-list"><li style="list-style-type:disc">this set is not extremely imbalanced</li></ul><ul id="4fabfaca-f911-4203-8d39-6d4d10d7e17e" class="bulleted-list"><li style="list-style-type:disc"> for imbalanced datasets we need to change our algorithm </li></ul><figure id="940d9106-cc35-4c5f-9095-8ea79baaf440" class="image"><a href="Untitled%2093.png"><img style="width:925px" src="Untitled%2093.png"/></a></figure><h3 id="b7e53065-0873-4e70-8104-201e3053907a" class="">Location </h3><ul id="517b8628-9d37-4d3c-ba36-7ad5badf6a9a" class="bulleted-list"><li style="list-style-type:disc">negative data point in the decision region for classifying positive data points are called a false positive </li></ul><ul id="2e08628d-63cd-4210-bef5-9d6ac2c4c531" class="bulleted-list"><li style="list-style-type:disc">positive data point outside the decision region for classifying positive data points are called a false negative</li></ul><ul id="5d2da5ba-9864-4e6f-8993-6fcede61ee37" class="bulleted-list"><li style="list-style-type:disc">depending on which false positioning is worse depends on the type of data<ul id="1bde3967-f583-4714-90fb-fa8670f1fde8" class="bulleted-list"><li style="list-style-type:circle">a false negative is more costly than false positive for diagnosing a disease</li></ul><ul id="1e656f5a-185d-462b-b4e2-b22105b670d0" class="bulleted-list"><li style="list-style-type:circle">in this case we want to change the way decision boundaries work so the decision region for positives are larger </li></ul></li></ul><figure id="6d7f5c59-5b9f-492d-8c5e-7f79ec066e9e" class="image"><a href="Untitled%2094.png"><img style="width:866px" src="Untitled%2094.png"/></a></figure><h3 id="d9611de6-8f72-4036-ba48-5c77c7c725a3" class="">Assert aspect </h3><p id="aff6c870-9eb7-4745-abda-71f3b36921b9" class="">considers the class labels assigned to each data point</p><p id="4410c316-af81-4a4a-a32a-b04512dc2b68" class="">two types of classification problems (difference is based on the class labels)</p><ul id="be6690e7-4a1e-4d8a-81d7-865439825e40" class="bulleted-list"><li style="list-style-type:disc">binary: only two class labels</li></ul><ul id="47cd2f9a-5d69-43bb-abf0-dbf863373dfa" class="bulleted-list"><li style="list-style-type:disc">multi class: more than 2 different classification labels data points</li></ul><figure id="a948f553-4e1e-408e-9cbf-7f3f6e605e09" class="image"><a href="Untitled%2095.png"><img style="width:861px" src="Untitled%2095.png"/></a></figure><ul id="7a1b6589-6c6c-49f9-b870-88831a9f66b7" class="bulleted-list"><li style="list-style-type:disc">we can convert a multi class classification problem into binary by looking at only two classes one by one and the merge them together</li></ul><ul id="5270a616-f2d4-4ed9-8fc0-d62f5899823d" class="bulleted-list"><li style="list-style-type:disc">this makes binary classification problems are more fundamental cause it can be used for both</li></ul><h3 id="be6c409d-7baf-4ec9-ba11-b3cde11bdcb4" class="">Complexity</h3><p id="d57dd239-777c-4c12-bdcd-7596464913fb" class="">linearly sparable classification problems are simple, regions are decision by a straight line</p><ul id="73a37936-db1b-4eef-bbcb-1c852303c05a" class="bulleted-list"><li style="list-style-type:disc">line can be done with a linear equation </li></ul><p id="790c13e5-e53c-4600-b4b0-bfde06a59795" class="">none linear sparable classification are complex because we need curved data boundary lines to separate data points</p><ul id="f340fd4f-5941-44fb-b25a-ae756f4a22a9" class="bulleted-list"><li style="list-style-type:disc">these lines cannot be done by linear equations</li></ul><ul id="c36ce146-59c8-458e-8f0b-942980e3cd76" class="bulleted-list"><li style="list-style-type:disc">have to use non linear equation </li></ul><blockquote id="998d1a5e-1a97-4269-9fb7-94fee5b78032" class="">because of the two types of problems, we need two types of models, one for linear and one for non linear </blockquote><figure id="cfe08bf4-41b0-4abe-9367-828e2187414d" class="image"><a href="Untitled%2096.png"><img style="width:862px" src="Untitled%2096.png"/></a></figure></details></li></ul><ul id="e6087a00-fc70-4d8d-a8f1-d50b4cf55b68" class="toggle"><li><details><summary>Classifiers</summary><ul id="af3e7054-fb46-4e0f-a7c6-3c847cb9e8fd" class="toggle"><li><details><summary>KNN (K-nearest neighbors) Classifier</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="575688cb-776b-488c-9461-d063a349e544"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">KNN is the simplest classifier<p id="0643549e-29d8-43a3-8c98-9e03b661fa16" class="">it is a lazy learner meaning it doesn’t learn, instead if memorizing training data points/labels</p><p id="31dd12f4-7828-4eb0-903f-a5265fe99ef8" class="">classifies unseen &amp; unlabeled data by finding the closest data points to it in search space and assigning their values to it </p><ul id="d0c6e2c1-47a6-4a9f-8fe2-208e10e7c911" class="bulleted-list"><li style="list-style-type:disc">control parameter:<ul id="0d433e62-745b-4722-814d-0f44f5dbd399" class="bulleted-list"><li style="list-style-type:circle">k</li></ul><ul id="3330da5d-dd45-47d3-9bfc-9236d6d58ae8" class="bulleted-list"><li style="list-style-type:circle">distance </li></ul></li></ul><ul id="b1e81ad3-2bbf-46cf-8ce1-13e7aaf01919" class="bulleted-list"><li style="list-style-type:disc">if value for k is not proper we will run into over/under feeding issues</li></ul></div></figure><ul id="50ed86fd-ed66-48e2-8c85-88638869e465" class="toggle"><li><details><summary>Measure Distance</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="131ed2e9-b711-4fe4-8338-9dd23f2e0469"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Euclidean and Hamming distance methods are used to calculate the distance</div></figure><p id="16b2b10e-438a-4262-a182-dd884191f925" class="">Euclidean distance between any two data points with coordinates<br/>(x,y) and (a,b) is<br/></p><figure id="0a79dd4b-4ecb-4603-8297-f583ebdb6972" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mtext>√</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>a</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mi>b</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">Dist((x,y),(a,b))=√(x-a)^2+(y-b)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mopen">((</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.05em;vertical-align:-0.25em;"></span><span class="mord">√</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></div></figure><p id="40a96484-5421-4cc7-a705-352722a6b043" class="">Hamming distance between data instances with equal length is the number of positions the corresponding character/position are different</p><p id="0d11ceb4-e803-4185-ab16-15d684cf96e7" class=""><code>oneforone oneandone ⇒ 3</code></p><p id="0496aeb2-cd84-4638-96f1-c6d4bcf282e1" class=""><code>11010110110  11000111110 ⇒ 2</code></p></details></li></ul><ul id="004d976a-2f15-4554-87dd-1dcf2202a7bf" class="toggle"><li><details><summary>Classifying Process (2d data search space)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="a9148165-1462-4166-a482-0d6b14f05b51"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><ol type="1" id="e68456c6-b683-4ceb-bc7c-a0c9dd415462" class="numbered-list" start="1"><li>finds the k-closest (3) data points from the green unknown   </li></ol><ol type="1" id="7609855f-f9d1-4e09-80d0-164d2ebe6f92" class="numbered-list" start="2"><li>labels it based on the common most labels out of the 3</li></ol><p id="1ee1f9db-6d47-4704-bbf3-b12a4519c53e" class="">in this case the green unknown will be classified as a red triangle </p><figure id="669f46ae-a1e2-4931-80ea-426f380bfc7f" class="image"><a href="Untitled%2097.png"><img style="width:736px" src="Untitled%2097.png"/></a></figure></div></figure><h3 id="78b46873-225c-4514-84bb-466396d4cb13" class="">Changing k</h3><p id="241d22b3-5f46-4b5f-973a-a5c78eb11e7b" class="">if we increase k from 3 to 5, it will look for the 5 closest points, and thus change the resulting label</p><ol type="1" id="d20c50fd-c2c5-48b8-a7a8-56682ca8fd9c" class="numbered-list" start="1"><li>looks for the k-closest (5) data points</li></ol><ol type="1" id="b742c92c-86e5-4b73-84d1-669a76831ca7" class="numbered-list" start="2"><li>labels it based on the common most labels out of the 5</li></ol><p id="e4a88aa0-b624-4761-a8ef-059a96254b52" class="">in this case the 3/5 are blue squares which is the most common so the unknown is labeled as a blue square</p><figure id="b1dd7961-338d-4d6b-8f4e-784bcef0696f" class="image"><a href="Untitled%2097.png"><img style="width:736px" src="Untitled%2097.png"/></a></figure><h3 id="bc5b6fe4-871f-42b5-891b-6d2ac67c099a" class="">Weighted KNN</h3><blockquote id="cf427135-c2a9-44ef-bfe7-a5f7cc2f8832" class="">the issue is that the red triangles are closer to the unknown than the blue squares are, even tho there are more of them around it.<br/>to address this issue we can assign a weight to each data point. <br/>the weight of the data points is inversely proportional to the distnace from the data point to the unknown <br/></blockquote></details></li></ul><ul id="0518e460-8ab3-417a-b009-6afc8b4bd975" class="toggle"><li><details><summary>“Iris” dataset example</summary><h3 id="8856fbba-8edd-4612-82c3-5ece06f903ac" class="">k = 1</h3><p id="b7f188f8-8393-440e-a32b-fc288da2c856" class="">set k as a low</p><figure id="27218e4c-b7ab-4293-a9ff-ea4145926ad5" class="image"><a href="Untitled%2098.png"><img style="width:696px" src="Untitled%2098.png"/></a></figure><ul id="9597e307-84c9-4133-b5c5-3689b67a3b10" class="bulleted-list"><li style="list-style-type:disc">100% decision accuracy because k is a small value of 1</li></ul><ul id="131cbf05-cfc7-4306-a4bf-8fcc8c2a2919" class="bulleted-list"><li style="list-style-type:disc">decision boundaries are complex</li></ul><ul id="c7899373-d405-4a68-8708-30d77b18cab3" class="bulleted-list"><li style="list-style-type:disc">100% decision accuracy on the training set is not good because it will cause overfitting<ul id="7818a0c8-bfc0-4255-aada-753e43106bae" class="bulleted-list"><li style="list-style-type:circle">will be bad on unseen data</li></ul></li></ul><h3 id="0c277685-a8af-44c7-8b19-a1ef625804bf" class="">k = 3</h3><figure id="68227aa6-81c3-4bbd-adf4-06394858f4a2" class="image"><a href="Untitled%2099.png"><img style="width:670px" src="Untitled%2099.png"/></a></figure><h3 id="3a334a52-63fc-4b09-a497-a5d290dcd004" class="">k = 50</h3><p id="3f55cbfc-20ea-4920-a793-588d907e4185" class="">set k to a large number</p><figure id="a8ab07b5-8b04-480d-a99e-b991adfa39a1" class="image"><a href="Untitled%20100.png"><img style="width:1114px" src="Untitled%20100.png"/></a></figure><ul id="565f0e02-1257-4720-95c9-cd180df49310" class="bulleted-list"><li style="list-style-type:disc">large k value makes the decision boundaries and region more simple/smoother </li></ul><ul id="0f094d60-ab0b-416f-93b0-1da88ed319bd" class="bulleted-list"><li style="list-style-type:disc">training accuracy is not good because k is large and data points are in incorrect regions</li></ul><ul id="5c7e284e-f850-4441-9a49-309a4ea2004c" class="bulleted-list"><li style="list-style-type:disc">test accuracy will also not be good because training was poor&#x27;</li></ul><ul id="b5728a64-41a5-4861-a67e-b7d9c7e2694c" class="bulleted-list"><li style="list-style-type:disc">ran into issue of underfitting due to low level</li></ul></details></li></ul><ul id="6d5ff2ff-d6d0-43a7-9155-5cf9edd84618" class="toggle"><li><details><summary>K Value</summary><p id="d8188c15-ec82-47f4-b80d-95c4aca2ba58" class="">when K is <mark class="highlight-red">small </mark>we run into overfitting</p><ul id="966aecc9-9cea-4f78-a880-3cf1f513d926" class="bulleted-list"><li style="list-style-type:disc">just grabs the nearest point</li></ul><ul id="e1ccf91a-df37-40c4-acfe-b7e73e853d62" class="bulleted-list"><li style="list-style-type:disc">complexity will be very high</li></ul><p id="e7687e07-b599-4154-bd30-6e8b78b83ead" class="">when K is large, we run into underfitting</p><ul id="93da7029-21a7-4f3b-b32d-7b08e7c9db05" class="bulleted-list"><li style="list-style-type:disc">averages over lots of nearby examples</li></ul><ul id="ef56595f-e6f5-45d2-8f35-a6de367380ba" class="bulleted-list"><li style="list-style-type:disc">boundaries are too simple </li></ul></details></li></ul></details></li></ul><ul id="435a968b-777d-44cb-baac-1f584719c492" class="toggle"><li><details><summary>Radius Neighbors Classifier</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="eb436d83-0c40-455e-b578-a9bc6fb02178"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">similar to the KNN classifier<p id="a225d58d-3351-4aea-a715-a495eb400f72" class="">classification <mark class="highlight-blue_background">decision is based on the number of neighbors within a fixed radius</mark> of the subject data point we are trying to classify</p><ul id="03e164d2-1187-4e76-97ef-ddeeec4663d8" class="bulleted-list"><li style="list-style-type:disc"> radius is provided by us</li></ul><ul id="65c7d910-8e87-4a7b-8f60-8ebf76340826" class="bulleted-list"><li style="list-style-type:disc">method becomes less effective due to “curse of dimensionality ”</li></ul><figure id="94c634dc-e3b2-4030-8efe-878c98d9424c" class="image"><a href="Untitled%20101.png"><img style="width:724px" src="Untitled%20101.png"/></a></figure></div></figure></details></li></ul><ul id="50112e4e-4532-464c-9015-70aa27534f79" class="toggle"><li><details><summary>Perceptron + Logistic Regression classifier</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="3e161e04-348e-4add-a652-3aa0810e942a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">mathematical model of a single neuron<figure id="765e81ac-4163-49db-8ab9-ca1b2a3b104e" class="image"><a href="Untitled%20102.png"><img style="width:555px" src="Untitled%20102.png"/></a></figure><figure id="8be62133-8ccd-421d-93f1-7a422ccfb81d" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>F</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>s</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>∗</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">output = stepFunction(inputs[i]*weights[i]+bias)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">tp</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.13889em;">pF</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">in</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">bia</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="c000448b-9a99-4781-be29-5de32dbb1ca9" class="image"><a href="Untitled%20103.png"><img style="width:379px" src="Untitled%20103.png"/></a></figure><ul id="25c33958-29f0-4174-83ce-9e33522d361c" class="bulleted-list"><li style="list-style-type:disc">multiple inputs are from the same data point that has n different features (numerical)</li></ul><ul id="3a3850ca-bcd5-473b-9300-d60b2516a8ed" class="bulleted-list"><li style="list-style-type:disc">uses step function instead of sigmoid<p id="e99c25a7-1df8-4cf1-a7cd-b5d585bc4e76" class="">input ≤ 0 = 0</p><p id="35a9b5a8-e685-4860-810a-232c39c713f5" class="">input &gt; 0 = 1 </p></li></ul><ul id="650559c3-7912-497a-94c1-e8742c796fcb" class="bulleted-list"><li style="list-style-type:disc">output of perceptron is the final output.<ul id="8c6987ef-0be2-4aa5-be6f-9fa2ca1a23b5" class="bulleted-list"><li style="list-style-type:circle">used for binary classification  </li></ul></li></ul></div></figure><ul id="ba5b8058-dbd3-4914-a23d-e77d7744e50f" class="toggle"><li><details><summary>Logistic Regression classifier</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="85d408b4-f111-4942-b201-ad7f65e8a8cc"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">the same as Perceptron classifier but uses the sigmoid function instead of step as the activation function<ul id="55289603-fe87-45c6-a234-aa3b64253b39" class="bulleted-list"><li style="list-style-type:disc">output is a decimal point instead of 1 or 0</li></ul><figure id="56a5f33d-9419-4743-b811-f51d3b7f40cb" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>s</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>∗</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">output = sigmoid(inputs[i]*weights[i]+bias)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">tp</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">in</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">bia</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span></div></figure></div></figure></details></li></ul><ul id="54c578ee-96d8-4359-afc7-1fc688d5bf0b" class="toggle"><li><details><summary>hyperplane (aka logistic regression)</summary><h3 id="d1b7c5ec-0037-4738-bdc2-717768279fe7" class="">hyperplane (aka logistic regression)</h3><p id="a100c65d-6248-4ffe-92ea-a22bb8618f2a" class="">this model is used to draw a hyperplane in the search space by calculating the output for each data point. this plane separates different labels of data </p><figure id="e81355b5-cd21-4bfb-80b5-36486b1e3f58" class="image"><a href="Untitled%20104.png"><img style="width:240px" src="Untitled%20104.png"/></a></figure></details></li></ul><ul id="4b54f3eb-0dbb-4196-804d-c3add2a6c598" class="toggle"><li><details><summary>Regularisation</summary><h3 id="02507441-79c1-4865-bf98-a8d402fd5f34" class="">Regularisation</h3><blockquote id="d92869f0-7fca-4ee3-87ec-63f032bed622" class="">Problem: as you train this classifier, the scale of the weights (absolute value of weights) will get larger and larger<p id="1a8de35c-ce1a-48f4-8e12-f8dd4a873813" class="">this leads to overfitting.<br/>to solve this issue we can use add Complexity control aka regularisation<br/></p></blockquote><p id="a63a51c7-a532-40f9-a67e-7a2ed10e7107" class="">introduces a penalty which is proportional to the scale of the weight </p><ul id="b16bf52e-78cd-4374-af28-94161c32ec05" class="bulleted-list"><li style="list-style-type:disc">the higher the weight the higher the penalty. <ul id="ecd576c6-2c4a-45f2-b6ed-f9977e9214c8" class="bulleted-list"><li style="list-style-type:circle">this keeps them small and allows other weights to have more attention</li></ul></li></ul><h3 id="2e01c7d8-22d9-43b9-8ee7-e7b68c768fae" class="">hyperparameter C</h3><p id="008f6f7d-bbe6-470d-adb8-e4fa309ebdf9" class="">to control the penalty of weights, we can use the hyperparameter C, this is used in sklearn </p><ul id="dc04f216-253d-4dd9-a931-26f595e63348" class="bulleted-list"><li style="list-style-type:disc">`Inverse of regularization strength; must be a positive float. </li></ul><ul id="72bee148-5f70-4781-b1ff-908e5e215773" class="bulleted-list"><li style="list-style-type:disc">smaller the value of c, the stronger the regularisation penalty </li></ul></details></li></ul><ul id="25a19586-69d1-4d2c-a1f3-2f04ac53ea52" class="toggle"><li><details><summary>Multilayer Perceptron (MLP)</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="6c0f1d02-6e47-4e92-a328-0d83328eb067"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">network of perception for complicated boundary&#x27;s<figure id="e5c1f6dc-525c-4e9b-85d4-55112aadbae5" class="image"><a href="Untitled%20105.png"><img style="width:283px" src="Untitled%20105.png"/></a></figure><ul id="62685581-19a5-47ec-9234-2ed3ae491ae1" class="bulleted-list"><li style="list-style-type:disc">first layer is the input layer, the second is the hidden layer</li></ul><ul id="b2c76747-e5de-49f5-a7cf-df12358bffbe" class="bulleted-list"><li style="list-style-type:disc">for simple MLP: only has one hidden layer</li></ul><ul id="cb3962e3-5804-448a-8e6e-db6a5568ceb5" class="bulleted-list"><li style="list-style-type:disc">we need regression for each weight</li></ul></div></figure></details></li></ul><p id="01c92fac-4efe-4a20-9919-dcabdd40eb33" class="">
</p></details></li></ul><ul id="0032aa99-5056-40c7-99bd-c947f9957342" class="toggle"><li><details><summary>Decision Tree classifier</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="2f5c0b33-2c99-4991-967d-a1f290dd4375"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">classifies data using a decision tree<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bbfff69a-ea59-47e8-b677-3ac8b5be7858" class="code"><code class="language-C">Consider V2, is it &gt; 5.1?
		if yes: Consider V1, is it &lt; 2.4?
		if no: Consider V4, is it &gt; 0.3?</code></pre><ul id="57f4d61b-b576-46a8-875d-64bc39bbbe50" class="bulleted-list"><li style="list-style-type:disc">condition is based on the feature of the data point</li></ul><figure id="84f339a4-b5e1-4bcd-b8c7-9cb3e770837a" class="image"><a href="Untitled%20106.png"><img style="width:702px" src="Untitled%20106.png"/></a></figure><ul id="51b253fd-ae1f-4d1a-9c1f-d81edffd2bf1" class="bulleted-list"><li style="list-style-type:disc">binary tree for numerical data</li></ul><ul id="8d3d8692-8925-4bb4-895a-6d5413e8736a" class="bulleted-list"><li style="list-style-type:disc">general trees for categorical data</li></ul></div></figure><ul id="04fcb378-54a5-4242-8228-11fd9dd0f6cb" class="toggle"><li><details><summary>Depth</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="439b31df-d944-4b38-9630-828b3b1fd1d1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">depth is found by counting the edges from the root to the lowest node<ul id="4eaedf32-09cc-4352-89c3-d898c68c8112" class="bulleted-list"><li style="list-style-type:disc">if the depth is too large, then this leads to overfitting<p id="8395f5cd-2748-4eeb-bc33-5f166e443771" class="">to prevent this, we can set the max_depth in scikit learn</p></li></ul></div></figure></details></li></ul><ul id="3cb5798b-8263-4287-a66c-e2b9116f28b2" class="toggle"><li><details><summary>Human Interpretable</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="c54484d5-99f6-498a-8038-8ad6cd40940a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">we can easily understand how a decision tree works as traversing down takes logic and the output of the model is the leaf node it ends at<figure id="fc0418d7-1d27-42f7-a44a-894e57d7349b" class="image"><a href="Untitled%20107.png"><img style="width:508px" src="Untitled%20107.png"/></a></figure></div></figure></details></li></ul><ul id="6f493509-67ce-482b-9e99-565a819a630d" class="toggle"><li><details><summary>Learning Capability of Decision Tree</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="5b5b12dd-6921-4c64-8e3a-e788b4e37f21"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">the amount of different classification functions we can build using a decision tree on a training set <mark class="highlight-red">with n rows and 2 outputs</mark> is 2^n<figure id="bf858c6d-adf5-4ad3-bfd1-cb8ccfc71c55" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mn>2</mn><mo>∗</mo><mn>2</mn><mo>∗</mo><mn>2</mn><mo>∗</mo><mn>2...</mn><mo>=</mo><msup><mn>2</mn><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">2*2*2*2...=2^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2...</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7144em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></div></figure><figure id="1a1f6761-8d7b-467d-97b0-2b0f47860556" class="image"><a href="Untitled%20108.png"><img style="width:475px" src="Untitled%20108.png"/></a></figure></div></figure><h3 id="7c72e47c-bb55-4222-a018-5e07f17aed75" class="">What functions can be represented?</h3><p id="b12cf08a-0833-4c58-9af4-4726a360c583" class="">Decision trees can represent any function of the input features.</p><figure id="ec6d8915-f690-414b-abbc-6d9a34f4db12" class="image"><a href="Untitled%20109.png"><img style="width:393px" src="Untitled%20109.png"/></a></figure><ul id="8a83b564-09e2-475b-9641-3a1c0b729f0f" class="bulleted-list"><li style="list-style-type:disc">For Boolean functions, each path from root to leaf in a decision tree corresponds to one row of the truth table.</li></ul><ul id="55a0fafa-2855-4fb7-a97f-28353e52d19c" class="bulleted-list"><li style="list-style-type:disc">size of a decision tree can grow exponential number of<br/>nodes<br/></li></ul></details></li></ul><ul id="b92957af-2f6b-465a-86c3-15285ca309d5" class="toggle"><li><details><summary>Complexity of Learning Decision Trees</summary><blockquote id="25e284fb-0463-437a-8ecb-67cd6bb82e09" class="">a decision tree is not unique, meaning you can make multiple different decision trees for the same dataset. however different decision trees for the same dataset may have different complexities</blockquote><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="4a394457-7195-410b-a739-876cf58b744d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">we want the complexity of a decision tree to be simple so that its more interpretable and reduces overfitting </div></figure><h1 id="032b5498-5c2a-46ba-8f0f-853e2daab7fd" class="">Getting a simple tree</h1><p id="60f22d50-64de-4911-ae73-1f08620bfdb4" class=""><del>Getting the simplest deicion tree is a NP-complete problem, meaning that we don’t have a method to get it. </del></p><p id="c5b69368-7ec7-4113-8acf-71c46b8963d6" class="">we can get a reasonably simple tree by using the greedy heuristic method</p><ul id="20ab7bac-a07d-46d5-9ce2-26aba50dc6ba" class="toggle"><li><details><summary>greedy heuristic method</summary><h3 id="f6899847-d7b8-4600-952a-8375684d3567" class="">greedy heuristic method</h3><ol type="1" id="b15ccae0-dd14-4dcc-8d21-60a2ffea8870" class="numbered-list" start="1"><li>Start from an empty decision tree<figure id="c5d5961b-b62d-4d65-8d21-697d91825198" class="image"><a href="Untitled%20110.png"><img style="width:336px" src="Untitled%20110.png"/></a></figure></li></ol><ol type="1" id="ce1f3e97-3f58-4271-b9ff-f42acd342657" class="numbered-list" start="2"><li>Iteratively split on the best next feature<figure id="101cba65-4c5d-4fc8-a93e-f53b279c04bb" class="image"><a href="Untitled%20111.png"><img style="width:288px" src="Untitled%20111.png"/></a></figure></li></ol><ol type="1" id="e94f70fa-90e4-4851-82fa-53a134a75428" class="numbered-list" start="3"><li>repeat until node cannot be expanded further <figure id="bb5f0a2e-aecc-4b45-91c3-0adf85065c5d" class="image"><a href="Untitled%20112.png"><img style="width:336px" src="Untitled%20112.png"/></a></figure><figure id="5451d704-976a-440a-a097-1a6d4d6b5740" class="image"><a href="Untitled%20113.png"><img style="width:336px" src="Untitled%20113.png"/></a></figure><ul id="ddfea9cd-03b9-4350-a73b-3f0cb2f4844a" class="bulleted-list"><li style="list-style-type:disc">this is NOT the tree we want because it will lead to overfitting, we want to implement complexity control to make the tree simple when making it</li></ul><p id="cef47e16-298c-4283-89d1-19c502e82251" class="">
</p></li></ol></details></li></ul><ul id="a9e03c19-25d4-425b-abb8-e2e9d270dd02" class="toggle"><li><details><summary>Choose the Suitable Feature to Split</summary><ul id="8bf4eade-0026-4460-acef-efd7b6ab1f82" class="bulleted-list"><li style="list-style-type:disc">splitting on a feature should reduce uncertainty of the class label to be applied to each partition after splitting</li></ul><ul id="43d9bd63-9a68-479c-8102-d7e06cbd058e" class="bulleted-list"><li style="list-style-type:disc">using counts at leaves to define probability distributions, so we can measure uncertainty</li></ul><ul id="42f182f5-44b4-434a-86c2-6940bb725d1e" class="bulleted-list"><li style="list-style-type:disc">the more uneven distribution of even and uneven data points, the more certainty and vise-versa </li></ul><figure id="575610e1-fafc-4585-904f-cab537bb66c9" class="image"><a href="Untitled%20114.png"><img style="width:286px" src="Untitled%20114.png"/></a></figure><p id="5ed3dfc9-51d4-4957-bbe3-e41a97697fc0" class="">in this case we want to split x1 because each partitions have one partition to have all trues and other to have all false</p><figure id="a848c126-1837-4dfd-910a-6b7ea41a0431" class="image"><a href="Untitled%20115.png"><img style="width:303px" src="Untitled%20115.png"/></a></figure></details></li></ul><ul id="57652b3f-50a3-4f1f-89cd-c3d329bc537f" class="toggle"><li><details><summary>Measure Uncertainty</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffd7d478-f60e-49c4-99b4-3000c772e9b4"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">when we split our dataset into partitions, we want to increase our classification  certainty</div></figure><ul id="d83c4d93-faef-4e0c-9a61-5d0733c85dee" class="bulleted-list"><li style="list-style-type:disc">Deterministic (all true or all false) is a good split and will make classification more certain</li></ul><ul id="f7435133-84d8-44e4-b3c2-5f9c6ace2e2d" class="bulleted-list"><li style="list-style-type:disc">Uniform distribution (mix of both false and false) will make classification uncertain </li></ul><h3 id="8b0f6e69-19e1-4e76-9ab8-e461b4acee43" class="">Example</h3><figure id="70c9affe-aa35-4878-beb0-2bdcbcf56deb" class="image"><a href="Untitled%20116.png"><img style="width:303px" src="Untitled%20116.png"/></a></figure><p id="3bc778bc-7723-49a0-86fd-8f4900935dee" class="">first is better because it is more even, so its more certain</p><p id="cf37bf1c-8f7b-4dc5-b827-fe839d51a333" class="">but what if both partitions are uneven then how can we pick the better split?</p></details></li></ul><ul id="c7459a3b-b703-4359-b5f4-6923a6b57a58" class="toggle"><li><details><summary>Entropy</summary><p id="0be5c5f6-a8e2-4734-8551-39012848487d" class=""><mark class="highlight-orange_background">we can use Entropy to quantitively find a partition split which is more certain</mark></p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="c9d9b0a5-0f31-4411-9a1d-3211efdc8b9c"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Entropy measures the uncertainty over the value of a random variable<figure id="55523bc3-1944-4752-9a3e-ce4886f5fb78" class="image"><a href="Untitled%20117.png"><img style="width:672px" src="Untitled%20117.png"/></a></figure><figure id="ae50eb3f-316e-4cce-8a65-fa30f50baddf" class="image"><a href="Untitled%20118.png"><img style="width:360px" src="Untitled%20118.png"/></a></figure><ul id="799cc65c-6349-4e66-80fa-d23fce2492d2" class="bulleted-list"><li style="list-style-type:disc">higher the entropy, the more uncertain</li></ul></div></figure><h3 id="b78d2a0d-2608-4b12-8e9b-c378a51aa381" class="">High entropy</h3><p id="5bce9d28-8210-413a-b788-70880c32b875" class="">Y follows a uniform like (flat) distribution</p><p id="93ee509f-8886-435d-adc6-f57a5dcb111c" class="">Values sampled from Y are less predictable</p><h3 id="2d05e9aa-d14e-4bbe-925f-9957e338eb2e" class="">Low entropy</h3><p id="d70d6cd5-692f-459b-b231-9ae9b67be4a2" class="">Y follows a varied (peaks and valleys) distribution</p><p id="eb8b9cf0-05c1-4901-94a2-85af73ea5d8e" class="">Values sampled from Y are more predictable</p><h3 id="16a2d8c5-942f-4b56-aaab-1fe34545bb7b" class="">Entropy Calculation Example</h3><figure id="5b9eb082-3ee7-45d0-af0e-c58bf2e19162" class="image"><a href="Untitled%20119.png"><img style="width:912px" src="Untitled%20119.png"/></a></figure><h3 id="8678358a-8d8b-43c1-b45e-ee4978a5bacd" class="">Conditional Entropy</h3><figure id="d81bb074-3750-4e92-8cd3-bbc24ecac354" class="image"><a href="Untitled%20120.png"><img style="width:800px" src="Untitled%20120.png"/></a></figure><figure id="eecf00c0-994a-4533-b93b-3b8065c9958a" class="image"><a href="Untitled%20121.png"><img style="width:864px" src="Untitled%20121.png"/></a></figure><h3 id="09d6d3b3-7e09-4a6c-90e7-882954cb813f" class="">Information Gain</h3><figure id="532f6865-0b60-4e7c-ab56-d9500b910851" class="image"><a href="Untitled%20122.png"><img style="width:768px" src="Untitled%20122.png"/></a></figure></details></li></ul><ul id="2d7db79b-1e17-43cf-86af-f6cca1f7b93b" class="toggle"><li><details><summary>Information Gain</summary><figure id="d9c0ae1a-138b-4090-981b-1b0d42a42784" class="image"><a href="Untitled%20123.png"><img style="width:666px" src="Untitled%20123.png"/></a></figure></details></li></ul></details></li></ul><ul id="ce519f2c-3dc3-4501-bcbc-4aee38f19b43" class="toggle"><li><details><summary>Algorithm to Learn Decision Tree</summary><ol type="1" id="3bd93f21-d87e-4e60-916c-0ee2dff0e55e" class="numbered-list" start="1"><li>start from empty decision tree</li></ol><ol type="1" id="a2be2caf-9c13-48f1-be61-5e520a702e4d" class="numbered-list" start="2"><li>split on next best attribute (feature)<ul id="1002cc5a-005c-40c3-90d1-1fe960c43c0d" class="bulleted-list"><li style="list-style-type:disc">use information gain to select attribute<figure id="277c42b1-2276-413c-8506-b1083efb0903" class="image"><a href="Untitled%20124.png"><img style="width:512px" src="Untitled%20124.png"/></a></figure></li></ul></li></ol><ol type="1" id="22ee783c-7630-436d-94ec-dff8e4d0a5a6" class="numbered-list" start="3"><li>recurse/repeat</li></ol></details></li></ul><ul id="86411181-dc02-4fce-aa00-c5094f10d883" class="toggle"><li><details><summary>Decision Trees can Overfit</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="2d11714e-3678-44b6-a122-e9e164a7030d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">standard decision trees have learning bias, where the tree is large and the training accuracy is high, but the test accuracy is low<figure id="5fa5d051-025e-42b1-a4ed-bb85723c2249" class="image"><a href="Untitled%20125.png"><img style="width:271px" src="Untitled%20125.png"/></a></figure><p id="74224857-a4f9-49d3-b4c1-84ab5c862e8b" class="">To address overfitting, we must introduce a learning bias towards simple trees</p><ul id="96c2f670-30be-45c1-8c8b-5a8d7330cd52" class="bulleted-list"><li style="list-style-type:disc">Set the maximum tree depth</li></ul><ul id="f3584db6-deb9-43f4-b12f-69849c76db15" class="bulleted-list"><li style="list-style-type:disc">Set the minimum partition size of each node (including the leaf node)</li></ul></div></figure><h3 id="c990d4a2-29b0-49d0-bed7-f3467dd5662f" class="">Pruning</h3><ul id="f128c6de-2862-4966-87a1-dc371ebcc889" class="bulleted-list"><li style="list-style-type:disc">Pre-pruning: stops the tree from growing before it perfectly classifies the training data</li></ul><ul id="cd5f2b4f-ce7e-4324-8d7b-a22dec84db01" class="bulleted-list"><li style="list-style-type:disc">Post-pruning: grows a full tree first and then simplify the tree.</li></ul></details></li></ul></details></li></ul><ul id="264932ac-b4dc-4143-94d0-1777bcd31637" class="toggle"><li><details><summary>Support Vector Machine (SVM) / Kernel Machines</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1433ae61-a39e-42d3-ab39-7c78eb22cf2a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">classifies data by calculating the distance between data points to a hyperplane similar to K-NN and perception.<ul id="554e18e2-9b75-479e-98ae-5012aaaef691" class="bulleted-list"><li style="list-style-type:disc">Distance is calculated using a mathematical Kernel function.</li></ul><ul id="25330e81-5bdb-4db3-8a0d-03cd5379b417" class="bulleted-list"><li style="list-style-type:disc">Support vectors are data points closest to the hyperplane</li></ul><figure id="3584d1a6-416b-4155-8c99-8366aca52ba0" class="image"><a href="Untitled%20126.png"><img style="width:470px" src="Untitled%20126.png"/></a></figure></div></figure><ul id="6dc77b32-67be-45ea-8fbd-38c14793e5ba" class="bulleted-list"><li style="list-style-type:disc">uses a mathematical trick called a Kernel</li></ul><ul id="fb06d9c3-e7e5-4fd8-88d4-c2c7e2b72862" class="bulleted-list"><li style="list-style-type:disc">SVM is similar to K-NN as it classifies a data point based on other datapoints that are close to it<ul id="e7cf9d12-5718-42ae-b80a-5366d500f99f" class="bulleted-list"><li style="list-style-type:circle">however it uses a kernel instead of a distance formula</li></ul></li></ul><ul id="6313c5e8-7330-47cf-b520-57ff03a6e6d8" class="bulleted-list"><li style="list-style-type:disc">SVM is also similar to a perception as it draws a hyperplane in the search space to seperate data points based on classification labels</li></ul><ul id="d5346673-4e7c-45c9-b5e1-db9adff87b00" class="bulleted-list"><li style="list-style-type:disc">support vector are the data points closest to the hyperplane</li></ul><h3 id="f1f342b4-4fcb-4920-b17b-d0d505e06aa3" class="">Kernel Function</h3><ul id="013a5560-f47a-466e-91eb-798ca5a9b9af" class="bulleted-list"><li style="list-style-type:disc">linear function</li></ul><ul id="780c66c5-dfd2-42a4-9ebc-9c9c7ac61d53" class="bulleted-list"><li style="list-style-type:disc">RBF function</li></ul><ul id="33598b9a-17b7-43be-a32c-4209c0ae938a" class="bulleted-list"><li style="list-style-type:disc">poly function</li></ul><ul id="28c2bcb1-5ddb-4666-b7fa-f48f3d770e11" class="bulleted-list"><li style="list-style-type:disc">sigmoid function</li></ul><h3 id="396f86e5-1124-4a34-9660-b7c6a86f69d5" class="">SVM in High Dimensional search space  </h3><ul id="dd9ce0b4-e9fa-4743-9812-dd16be7a286b" class="bulleted-list"><li style="list-style-type:disc">SVM can solve complex problems (non linear) by mapping data points to high and low dimensional search space<figure id="a92ce023-b036-49d3-a441-f70f47cf97c5" class="image"><a href="Untitled%20127.png"><img style="width:692px" src="Untitled%20127.png"/></a></figure><ul id="b06996a0-a349-428d-bc2e-8e560f9c2ee1" class="bulleted-list"><li style="list-style-type:circle">mapping function is Θ which maps an input in 2d search space to a feature space</li></ul><ul id="e977398a-2e96-4263-a2c1-20ae78beeb35" class="bulleted-list"><li style="list-style-type:circle">this mapping function is expensive and the feature space can be too complex<ul id="939911e9-e98e-4cef-8457-d23fb79aa87f" class="bulleted-list"><li style="list-style-type:square">instead of explicitly using this function, we can still calculate the distance between two data points as if they were in high dimential space by using the kernal function trick</li></ul></li></ul></li></ul></details></li></ul><ul id="c09e682c-396f-4cea-a4cb-b975bf8e1e34" class="toggle"><li><details><summary>Ensembles of classifiers</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="29ea1dc1-ab1a-4409-a81e-d6fbd9eec90c"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">uses multiple different classifiers to classifiy a data point<ul id="992d7213-ccf5-4453-857f-fead28d4bc1e" class="bulleted-list"><li style="list-style-type:disc">this is done to reduce generalization error</li></ul><ul id="1c3555c7-5fa5-4a46-8c51-fbe26881276a" class="bulleted-list"><li style="list-style-type:disc">ensemble classifier assumes “independence”, meaning the error in one classifier is independent from the error in another. it is the best classifier if its true</li></ul><ul id="a6e3d276-f37e-40d3-83a2-89a69251981f" class="bulleted-list"><li style="list-style-type:disc">used because it can use the strengths and weaknesses of each of the classifiers that it uses<ul id="fd078f01-f074-4282-8700-dfd36e13b84a" class="bulleted-list"><li style="list-style-type:circle">each learning technique uses a different ‘hypothesis’ which is a different classifier in the Ensembles classifier</li></ul><ul id="98a63844-f0f2-4a63-aa33-d520f6fc617f" class="bulleted-list"><li style="list-style-type:circle">‘hypothesis is also called the base classifier or base learner</li></ul></li></ul></div></figure><ul id="8fd23b8d-4206-4b20-90b9-07482e638aa4" class="toggle"><li><details><summary>Types of Ensembles classifiers</summary><p id="0012e6e1-03d8-4f32-a3a1-3525e711b0b9" class="">there are two types of Ensembles classifiers</p><ul id="1a412bd0-db8b-412c-b797-1314cd3c15db" class="bulleted-list"><li style="list-style-type:disc">homogeneous <p id="c80adde7-cbec-4a0a-89c2-25fa97b8fcf6" class="">when all the classifiers used are the same </p><p id="313e751d-f23f-4e36-9809-5c2bec7bf211" class="">(eg all of them are decision trees)</p></li></ul><ul id="54cc3bdb-e061-463b-ae34-cabe5805d912" class="bulleted-list"><li style="list-style-type:disc">heterogeneous <p id="18a958c1-3997-492b-ba68-76e1c23534e4" class="">when the classifiers used are different from each other</p><p id="de96358f-9630-46e1-b836-be703b2975a6" class="">(eg one is NN, the other is K-NN and another is SVM)</p></li></ul></details></li></ul><ul id="6b699cb9-1780-4bd9-a8d7-b8bdc35a4b1f" class="toggle"><li><details><summary>Ensemble Learning</summary><h3 id="4810d953-acb2-4ce3-87d3-b5a3b01229ef" class="">Definition of Ensemble Learning</h3><p id="24a9d4d4-7626-4e76-ae5e-bdaab6e0ea5b" class="">method to select and combine an ensemble of classifiers into a better classifier</p><p id="eb5256d2-4a0f-442b-9285-1cfee2cbb1aa" class="">Can enlarge classification capability:</p><ul id="b4dcc8c1-c748-4c15-9de7-6ad56d8ce7b0" class="bulleted-list"><li style="list-style-type:disc">Perceptrons, logistic regression, support vector machines:<ul id="85e1470d-edae-4763-b28a-286960cfec22" class="bulleted-list"><li style="list-style-type:circle">linear separators</li></ul></li></ul><ul id="9994512d-4461-4a45-a619-0f49396caa68" class="bulleted-list"><li style="list-style-type:disc">Ensemble of linear seperators:<ul id="b8caba59-d6fc-4f4d-b2cd-3ce60e773e1b" class="bulleted-list"><li style="list-style-type:circle">polytope</li></ul></li></ul></details></li></ul><ul id="0b7d5a80-30a6-4a26-811f-a183a72f7c17" class="toggle"><li><details><summary>Learning Techniques </summary><ul id="400cab61-4560-477a-a4ee-4f9280fd5c3c" class="toggle"><li><details><summary>Bagging</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="aae8e3b7-9349-4c6f-9489-97367d9f09fa"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">decides the classification label of a data point by getting the output of all the classifiers and then using the majority vote/common answer as the classification<figure id="19aa634e-9805-455d-a210-a90d00d2adc8" class="image"><a href="Untitled%20128.png"><img style="width:881px" src="Untitled%20128.png"/></a></figure></div></figure><h3 id="f0ea9949-ccdc-46f2-84b5-adc257015417" class="">Random Forests Example</h3><figure id="5e464985-326a-4e70-bbd8-0ed1ad50fd0d" class="image"><a href="Untitled%20129.png"><img style="width:943px" src="Untitled%20129.png"/></a></figure></details></li></ul><ul id="e8e8e5ab-4a28-40c1-9958-741fcc9843d5" class="toggle"><li><details><summary>Boosting</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="09ada6c2-aa74-4cee-8eaf-2eefd769ba37"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">similar to the majority vote of bagging but instead uses a weighted majority<ul id="48d5800a-faaf-41da-bb92-57ace7cd2714" class="bulleted-list"><li style="list-style-type:disc">this is due to hypotheses rarely being independent </li></ul><p id="3ad61e9a-41ec-4c35-966c-c969ddbc94b4" class="">weights are assigned based on the accuracy of the classifier the vote comes from</p><ul id="7b6f8d10-e10c-411e-b54f-5e9ec039554f" class="bulleted-list"><li style="list-style-type:disc">more accurate classifiers will have higher weight to their vote</li></ul></div></figure><ul id="7b8f1fc4-85af-46b4-9fbb-fc26aec44af7" class="bulleted-list"><li style="list-style-type:disc">called boosting because it “boosts” weak learner classifiers used in the Ensembles</li></ul><h3 id="d27f5495-a21b-4c4b-988e-d13de1798cc4" class="">Boosting algorithm </h3><figure id="ccc1d4c2-d9f6-4a1e-b0f4-1d888fc5bb82" class="image"><a href="Untitled%20130.png"><img style="width:862px" src="Untitled%20130.png"/></a></figure><p id="8e37217a-5761-4abf-8e71-46223840b46b" class="">Example:</p><figure id="cd2d5197-1123-42bf-bee3-115043c7fa41" class="image"><a href="Untitled%20131.png"><img style="width:618px" src="Untitled%20131.png"/></a></figure></details></li></ul><p id="b2c8647f-c229-43da-ae06-59f73d13087f" class="">
</p></details></li></ul></details></li></ul></details></li></ul><ul id="f5a5477b-acbe-40b8-9c9f-ab2bfacf9b07" class="toggle"><li><details><summary>Overfitting vs. underfitting</summary><h3 id="55f7ff7d-7f46-4227-93e3-b8ae14517037" class="">Definition difference</h3><p id="504d3d36-0fb1-44a2-9d40-99d03d1d37e0" class="">overfitting is when model becomes too specific to the training set and isn’t flexible enough for unseen points </p><p id="a8b248ba-5c86-483e-9c7c-eb73a5098092" class="">underfitting is when the decision boundaries are too simple and can’t properly separate different points into different regions</p><h3 id="8e74020a-2367-4d17-afb6-600ddf476848" class="">Causes</h3><p id="1c480170-f893-4e2a-91a6-8b4ffa0999f3" class="">overfitting can be caused by</p><ul id="cb5b6680-a6a9-4301-808a-c9ce17030e41" class="bulleted-list"><li style="list-style-type:disc">complexity of data set</li></ul><ul id="bde6f263-3fa5-43d0-b196-7a2fb8e5383e" class="bulleted-list"><li style="list-style-type:disc">small data set</li></ul><ul id="3a1c7c0d-4739-4b54-b6fe-d07c75d7f7aa" class="bulleted-list"><li style="list-style-type:disc">train the model for too long on the same set<ul id="33926b66-921b-4ac3-8edc-0a65b270e341" class="bulleted-list"><li style="list-style-type:circle">eg: training NN for too long increases size of connection weights makes the model more sensitive: small change of input causes big change in output</li></ul></li></ul><p id="6f973ea4-37c3-4b6a-85aa-edea2795a8ba" class="">underfitting is caused by</p><ul id="8c9bc7ea-1bdc-4a81-9ef1-faec90881d18" class="bulleted-list"><li style="list-style-type:disc">simple data set</li></ul><ul id="d9e5058e-8679-4bb2-9f23-430f032615e7" class="bulleted-list"><li style="list-style-type:disc">train model for too short</li></ul><ul id="63142851-4a37-46e5-867d-c221aa654f1e" class="bulleted-list"><li style="list-style-type:disc">high evidentiality is high (too many features)</li></ul><h3 id="0bfa2d3e-2c43-450d-8248-8f4fdc488609" class="">Symptoms </h3><p id="652b59c3-6969-4fbd-88ef-b26a2e1c1ef0" class="">overfitting:</p><ul id="cba9c54a-b78e-4b64-9ce2-fefc6efc8ef6" class="bulleted-list"><li style="list-style-type:disc">high training accuracy and low test accuracy </li></ul><p id="5bb60912-caeb-495a-86b7-11500e910a09" class="">underfitting:</p><ul id="be9bbcaa-546a-4d81-b171-8ffa27271912" class="bulleted-list"><li style="list-style-type:disc">low training accuracy and low test accuracy.<p id="909e84bd-5513-48d9-b877-a75fd37f02c3" class="">haven’t picked up anything useful from training to distinguish anything</p></li></ul><h3 id="3a269f0d-9829-49e4-a923-ca8d2ebc5c40" class="">Solutions</h3><p id="8be91317-b4d6-4b54-aa6c-a8fadadc0f7c" class="">overfitting</p><ul id="3def37d5-128f-4373-a9d4-491285f41e8b" class="bulleted-list"><li style="list-style-type:disc">simplify the model</li></ul><ul id="a9611a0e-2d61-4dd9-a642-d60b3c50fe89" class="bulleted-list"><li style="list-style-type:disc">stop the training process when we see overfitting occurring </li></ul><ul id="62d1c2ad-7015-4562-a42e-9513a74f65e2" class="bulleted-list"><li style="list-style-type:disc">regularizing to stop sensitivity </li></ul><ul id="a4054db7-934f-49f0-9404-b5b7f3b94cba" class="bulleted-list"><li style="list-style-type:disc">increase data size</li></ul><p id="aa79fdea-325f-4de8-bbc7-d4e8efa86115" class="">underfitting</p><ul id="35d978d2-a153-42df-900d-578e01dad16d" class="bulleted-list"><li style="list-style-type:disc">use more complex/non linear training data</li></ul><ul id="ca3f8492-a675-44b4-93ef-9a897f934ec6" class="bulleted-list"><li style="list-style-type:disc">increase training time</li></ul><ul id="cd296fb1-8b0b-412e-9d56-efa693a3c67b" class="bulleted-list"><li style="list-style-type:disc">reduce the dimensionality</li></ul><p id="e90a8a5e-0451-42d7-bed8-95c6847438e0" class="">
</p><p id="d1559453-83f7-41f6-93c3-9588a9a287df" class="">
</p><p id="633d7577-16e7-4a66-b7d7-6444f228bc9d" class="">
</p><p id="8b82362a-3e01-4a2f-9189-c6990ecf3017" class="">
</p></details></li></ul><ul id="3fc19827-98d6-435a-847d-aebce2b2a3c6" class="toggle"><li><details><summary>Bias Variance trade off</summary><ul id="3ce797d1-daba-4335-9d11-7313c607b558" class="toggle"><li><details><summary>What is Bias and Variance? </summary><h3 id="0b0ca94a-84ad-4682-98e5-7159321eab01" class="">Bias</h3><p id="230fee65-9b76-42b3-b35b-f7cc7d4af360" class="">introducing assumptions to simplify the learning process in an algorithm</p><ul id="df1b924f-3559-4cc9-8eb9-b06752e7c7db" class="bulleted-list"><li style="list-style-type:disc">error from erroneous wrong assumptions causes learning to be simple but the result to be ineffective </li></ul><ul id="fc3c9142-4459-41e3-bed0-2057f4220a36" class="bulleted-list"><li style="list-style-type:disc">causes simple models that can’t capture inherent patterns</li></ul><ul id="cccc5aed-dc3c-4447-9ba4-cd9b2a2ed98c" class="bulleted-list"><li style="list-style-type:disc">causes underfitting</li></ul><h3 id="5704b7bb-d6d9-47c5-bdb3-0a57509f20ab" class="">Variance </h3><p id="3e5d1503-9ab0-4a76-ae09-ec519061dbf3" class="">error from sensitivity to small fluctuations in the training dataset</p><ul id="29804816-9301-4659-8f33-a6acba5f9c6c" class="bulleted-list"><li style="list-style-type:disc">caused by use of overly complex models</li></ul><ul id="6ca59d20-c279-4c1d-8060-aac3a9d965b0" class="bulleted-list"><li style="list-style-type:disc">causes overfitting</li></ul></details></li></ul><ul id="6dede131-8dc6-416e-abf5-315bd4534b0f" class="toggle"><li><details><summary>Bias-variance decomposition</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="bcf1a897-ba8a-40f9-812b-0f37629202be"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">mathematical thorium states that the expected generalization error can be decomposed to 3 parts<br/>error due to bias + variance + irreducible error<br/></div></figure><ul id="425c7df3-6637-419c-96a9-f05f4639b790" class="bulleted-list"><li style="list-style-type:disc"> irreducible error will always be present no mater the classifier </li></ul></details></li></ul><ul id="8e7e77bc-e5ba-4764-b2b5-f0fd4b477af4" class="toggle"><li><details><summary>Bias Variance Trade off</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="d54db17f-86a2-44f3-a812-5f3414d18cf2"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">we cannot reduce error from both bias and variance at the same time<ul id="8c9160d6-964c-4045-86a5-dd0289171842" class="bulleted-list"><li style="list-style-type:disc">reducing bias error will increase variance error viscera </li></ul><ul id="a74db63a-c199-4eb3-a57b-0b8c30eb20a2" class="bulleted-list"><li style="list-style-type:disc">different classifiers will have different Bias Variance Trade offs<p id="0733962a-4183-4d66-a4b3-58ab2d885186" class="">this can help us determine the best classifier for our task and the best trade off</p></li></ul></div></figure><h3 id="68a78189-431f-4edb-b0b4-8352608d48b2" class="">validation set</h3><p id="e77a659f-2fd2-4c55-9214-f3e7ed85d38b" class="">we can find the best classifier for our task based on its bias-var trade off by using the validation data set during training</p><ul id="6ff183e1-4973-49b9-82f9-cb9a035c2a3c" class="bulleted-list"><li style="list-style-type:disc">test accuracy determines the best classifier for our real world task, validation just helps us get there and we cannot rely on its accuracy <ul id="43d535b5-51ce-4234-ac2b-12bd36377d66" class="bulleted-list"><li style="list-style-type:circle">this is because validation set is used to validate the training set, it is not used to train the model</li></ul></li></ul></details></li></ul></details></li></ul><ul id="13806456-298d-4ede-a03c-5cdcc9fd7e60" class="toggle"><li><details><summary>Cross Validation/Holdout method</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="415c3c54-a656-403d-9b9a-cbe614d12520"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">method to find the classifier with the best bias-var trade off for our task<p id="3897cc43-e328-45a3-a072-e187e9d11345" class="">it preform a simple split of a data set into training and testing</p><figure id="4f1c24f2-31fb-4939-9d00-055cbc380659" class="image"><a href="Untitled%20132.png"><img style="width:467px" src="Untitled%20132.png"/></a></figure></div></figure><h3 id="ccb212a7-d7c6-4a05-89d9-16af4d92a222" class="">Limitations</h3><p id="1b742d9c-92b0-49df-a78f-0e717e035553" class="">the limitation of this method is when data set as a whole is too small, doing the split will make both sets small </p><ul id="43de5b4e-478e-4084-9c71-f6394254f39d" class="bulleted-list"><li style="list-style-type:disc">cause bias and inaccurate results</li></ul><p id="ad26bc08-df99-4227-9d75-dad8e1781636" class="">when the data set is small, we want to use one of the resampling methods instead which preforms multiple splits</p><p id="6ce75ae6-5839-4ebc-96c0-2e1ba6e2e236" class="">
</p></details></li></ul><ul id="40328aca-5695-45be-a4c2-92ac6d8aa6ec" class="toggle"><li><details><summary>Resampling methods</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="036f1083-8ce6-49d9-9b2f-904c0356beeb"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">preform a multiple splits of a data set for smaller data sets</div></figure><h3 id="382577bf-bf2b-47ed-9dc9-8af8d04f2d95" class="">Random Sub-sampling</h3><ol type="1" id="6f1bc9ef-d21c-404d-b8a4-3044e8949c7b" class="numbered-list" start="1"><li>splits data set 3 times</li></ol><figure id="489409c5-74fa-4137-8516-f49b287ac775" class="image"><a href="Untitled%20133.png"><img style="width:514px" src="Untitled%20133.png"/></a></figure><figure id="d3e6f125-9a29-40fc-bd3f-ad3d0563f36a" class="image"><a href="Untitled%20134.png"><img style="width:336px" src="Untitled%20134.png"/></a></figure><h3 id="fc259907-1233-4a37-b5b7-a83c8790eb2c" class="">K-Fold Cross-validation</h3><ul id="9dfde37e-8992-4e3c-ad61-8ca31d7810f8" class="bulleted-list"><li style="list-style-type:disc">k is the number of times we want to split the data set into different partitions </li></ul><ul id="91d30312-b481-4f71-8fdc-6d686f34dee7" class="bulleted-list"><li style="list-style-type:disc">guarantees every data point will be used in different experiments  </li></ul><figure id="a39f4701-bc6e-4457-9959-620bb0de9ee1" class="image"><a href="Untitled%20135.png"><img style="width:574px" src="Untitled%20135.png"/></a></figure><figure id="fc326153-3d9c-440c-8eb3-2d11ccf14f93" class="image"><a href="Untitled%20136.png"><img style="width:115px" src="Untitled%20136.png"/></a></figure><h3 id="580694d7-3026-4b01-9127-dffd83a1654e" class="">Leave one out Cross Validation</h3><ul id="597a32e0-e519-4cf9-b8a5-2f579c7f7f5d" class="bulleted-list"><li style="list-style-type:disc">used when the data set is VERY small (eg 10 data points)</li></ul><ul id="6bbd292a-873a-4db3-b864-77454b870449" class="bulleted-list"><li style="list-style-type:disc">k=dataset size</li></ul><ul id="025688f5-4baf-43fa-89e5-af35e8882e3b" class="bulleted-list"><li style="list-style-type:disc">single points are used for testing while the rest are used for training</li></ul><figure id="fb13d6f3-41b7-4779-8430-2d63d17dc71a" class="image"><a href="Untitled%20137.png"><img style="width:557px" src="Untitled%20137.png"/></a></figure><figure id="83804d84-966a-4f02-9489-c50a7267bc24" class="image"><a href="Untitled%20138.png"><img style="width:187px" src="Untitled%20138.png"/></a></figure></details></li></ul><ul id="85783a94-733a-42c0-a853-c1dbc9f6ecae" class="toggle"><li><details><summary>How Many Folds/Splits are Needed?</summary><p id="1cb9683f-8122-487b-9541-e6eaea3cc65d" class="">Large number of folds</p><ul id="8a49a1a4-1193-4197-b4eb-c5cdb56b6065" class="bulleted-list"><li style="list-style-type:disc">bias is small</li></ul><ul id="8c8c2dfa-6f23-4461-8d09-463bdb725ca6" class="bulleted-list"><li style="list-style-type:disc">computation costs is higher (more experiments)</li></ul><p id="b86144eb-e8ca-448f-adf9-b3d6493060f2" class="">Small number of folds</p><ul id="40e3bd05-b0a0-4f55-804f-119a1dcd9f02" class="bulleted-list"><li style="list-style-type:disc">less computation costs (less experiments)</li></ul><ul id="a5f12e32-2da7-44b5-af70-27370fa29836" class="bulleted-list"><li style="list-style-type:disc">but accuracy of validation result is worse</li></ul><h3 id="6568326d-8b8e-4f7f-8b8d-7bd45dd6a5fe" class="">Rules</h3><p id="40c1ff58-bc4d-48a4-bef8-e897a718b01b" class="">when the data set is large, we want a small value of K</p><p id="8b0a1a64-59c0-4e0f-aa0a-b2f2ed3a9803" class="">when data set is small, we want a large value for K</p><p id="4f863490-0852-4c84-be5b-cf55d44d7831" class="">when the data is is VERY small, then we use leave one out cross validation </p></details></li></ul><ul id="cddd0641-aafa-4b22-9fb9-9c9371b27760" class="toggle"><li><details><summary>Basic Performance Evaluation Metrics</summary><ul id="7ec5f7d8-c1bb-499e-a354-c8f4b9935963" class="toggle"><li><details><summary>Classification Accuracy </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="d1281010-8860-4f1f-a272-542c93308281"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">the number of objects which are correctly<br/>detected/classified as a percentage of the total number of desired<br/>objects in the data set.<br/><figure id="79ddee8d-12a5-41f6-a509-5292a2915713" class="image"><a href="Untitled%20139.png"><img style="width:495px" src="Untitled%20139.png"/></a></figure></div></figure><h3 id="463c1821-2b2f-455c-a7c5-ede30ca10cff" class="">Error Rate</h3><p id="0001383e-e156-4b7c-af39-334faf65f228" class="">number of objects incorrect classified as a percentage of number of objects</p><figure id="b7092c08-4864-4dba-90c5-9405cab1b3c4" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>=</mo><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">ErrorRate=Accuracy-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal" style="margin-right:0.02778em;">rror</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">cc</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">cy</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span></div></figure></details></li></ul><ul id="5c906543-304f-49ad-8416-d5cbde1a3c70" class="toggle"><li><details><summary>TPR, TNR, FPR and FNR</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="fcc94c37-1db8-41dc-98b1-c2e56c6e2de4"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><figure id="6af17234-4b73-4e4c-9370-a3a4b3a25458" class="image"><a href="Untitled%20140.png"><img style="width:592px" src="Untitled%20140.png"/></a></figure></div></figure><p id="50bb0853-f62a-4afa-bb90-59689fdffcbe" class="">TP = true positive</p><p id="b54cd82b-096a-4b62-814c-5464cd56588e" class="">FN = false negative</p><p id="1a624fb5-b1d7-4123-887d-875b8f9d6d34" class="">FP = false positive</p><p id="91c2d2d5-ffdd-4c71-aaec-a632ca0aed70" class="">TN = true negative</p><h3 id="ba9b00aa-7afe-4b7f-a290-0d03a405c3b8" class="">True Positive Rate (TPR) aka sensitivity</h3><p id="56d093f8-f8c9-4c37-8add-528a08f0b16f" class="">the fraction of desired objects in a dataset that are correctly classified by the model</p><figure id="20da6937-ffbe-465b-90ef-9a889eb2a1e0" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mi>T</mi><mi>P</mi><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">TPR=TP/(TP+FN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">TPR</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">TP</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">TP</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FN</span><span class="mclose">)</span></span></span></span></span></div></figure><h3 id="5c8b8af1-f22c-4d5f-84db-4c22b5b6d354" class="">True Negative Rate (TNR) aka specificity</h3><p id="ab949715-c095-4431-a26b-db80a2797820" class="">The fraction of non-objects in a database that are correctly classified/detected as non-objects/background</p><figure id="fc5be032-2a04-4ae0-8621-b0f7164011d2" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>T</mi><mi>N</mi><mi>R</mi><mo>=</mo><mi>T</mi><mi>N</mi><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">TNR = TN/(FP+TN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">TNR</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">TN</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">FP</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">TN</span><span class="mclose">)</span></span></span></span></span></div></figure><h3 id="d599ae74-f612-4d34-beb9-a1921a6e9367" class="">False Positive Rate</h3><p id="313a2ce7-e708-4d96-be38-d60ded88c677" class="">fraction of non objects in a dataset which are incorrectly classified as objects</p><figure id="00696a1f-2718-4bc5-ab69-52c5f549af4e" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mi>F</mi><mi>P</mi><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">FPR = FP/(FP+TN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">FPR</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">FP</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">FP</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">TN</span><span class="mclose">)</span></span></span></span></span></div></figure><h3 id="499b31a5-de76-48b4-9ffe-a9ed122c8509" class="">False Negative Rate</h3><p id="735ce32a-8144-4f0f-9b3f-5fb7f40302f6" class="">fraction of objects in a data set that are incorrectly classified by as non objects</p><figure id="4c194bf0-9e9e-4f08-b56b-570775111bf3" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mi>N</mi><mi>R</mi><mo>=</mo><mi>F</mi><mi>N</mi><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">FNR = FN/(TP+FN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">FNR</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FN</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">TP</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FN</span><span class="mclose">)</span></span></span></span></span></div></figure><h3 id="bffb799d-6ecc-4e38-a30b-1eca429a47c9" class="">Confusion Matrix</h3><p id="b20e0d05-b153-4e74-88ec-92f05f279ec1" class="">used when you have a multiclass classification problem </p><figure id="18f7ba12-9d36-4bdd-940d-9331ff6b6f09" class="image"><a href="Untitled%20141.png"><img style="width:488px" src="Untitled%20141.png"/></a></figure><ul id="06290047-defc-4929-b914-633173ccbec4" class="bulleted-list"><li style="list-style-type:disc">this is 3x3 matrix so its for a 3 class data set (iras)</li></ul><h3 id="27da7029-0643-4493-b68c-018b5ae8b036" class="">Adds to 1</h3><figure id="7e028bb4-f884-4b55-a9af-e17db9f3b160" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mi>N</mi><mi>R</mi><mo>+</mo><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">FNR+TPR=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">FNR</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">TPR</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span></div></figure><figure id="73b740bb-7ace-4a79-affb-548e2e583f1c" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mi>P</mi><mi>R</mi><mo>+</mo><mi>T</mi><mi>N</mi><mi>R</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">FPR+TNR=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">FPR</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">TNR</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span></div></figure></details></li></ul><ul id="655ef728-de2a-41ed-bce1-763a171b9c91" class="toggle"><li><details><summary>Receiver Operating Characteristic Curve </summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="d230ad24-b0f4-4a5f-b2e4-85a6019d6e80"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">standard ROC curves conventionally takes FPR as x axis and TPR as the y axis<figure id="093770d3-8672-4cc8-bd76-6cb05f0ad02a" class="image"><a href="Untitled%20142.png"><img style="width:584.045166015625px" src="Untitled%20142.png"/></a></figure><ul id="b51745b7-cfff-483d-9657-f2a751967251" class="bulleted-list"><li style="list-style-type:disc">there is trade off when increasing the TPR, it also increases the FPR</li></ul><ul id="ab380c54-a1d7-4b15-954c-017046837258" class="bulleted-list"><li style="list-style-type:disc">the side you want to focus on depends on the application</li></ul><ul id="2c8f3c3d-214b-47d1-9501-cfb36c08b0e7" class="bulleted-list"><li style="list-style-type:disc">when the ROC curve is close to the “ideal” point, near the top left where FPR is low and TPR is high, we have a good classifier</li></ul><ul id="01a3ca34-44a3-4591-bd7f-042dfbabd953" class="bulleted-list"><li style="list-style-type:disc">when the ROC curve is a straight line, the positive and negative curves overlap</li></ul></div></figure><ul id="03ffac99-fbbb-4cc0-94d3-38bb4ecf36e4" class="bulleted-list"><li style="list-style-type:disc">higher the ROC curve, the greater the discrimination capacity </li></ul><ul id="f3bd164e-7a80-459c-a160-602774ad42ce" class="bulleted-list"><li style="list-style-type:disc">lower the ROC curve, the weaker the discrimination capacity </li></ul><ul id="9c30dc23-848d-45a1-8a95-247a0b17aca0" class="bulleted-list"><li style="list-style-type:disc">idea point is where it has perfect interpretation, this doesn’t exist but we want to be as close as possible</li></ul><ul id="95eaab2f-f5c6-4bc2-bc92-068a8376f7d7" class="toggle"><li><details><summary>Threshold </summary><h3 id="597f98d8-0339-433e-9363-75964d01b3ff" class="">Threshold </h3><figure id="d3de40c6-4a3a-44f6-b0f9-75a438307238" class="image"><a href="Untitled%20143.png"><img style="width:555px" src="Untitled%20143.png"/></a></figure><h3 id="d678b32f-fa9d-45f3-9d1e-2d5e01ce558c" class="">Less aggressive Mindset</h3><figure id="df3b4345-797e-4500-b9b1-17155176f158" class="image"><a href="Untitled%20144.png"><img style="width:555px" src="Untitled%20144.png"/></a></figure><ul id="c459b7f3-68cc-4e16-be43-c2e71f6dfecb" class="bulleted-list"><li style="list-style-type:disc">red represents false positive</li></ul><ul id="346cd97c-8016-4713-830e-700fee9ec4e5" class="bulleted-list"><li style="list-style-type:disc">green represents true positive rate</li></ul><h3 id="1f42adb7-c7b4-4414-b556-5e46d23e717f" class="">Moderate Mindset</h3><p id="25868515-f7b3-4a6a-adb1-c8bdb1cf096a" class="">when we think that the false positive for the negative and positive are equal</p><ul id="e795e1d5-7258-48bb-bf75-10e55c8b31e7" class="bulleted-list"><li style="list-style-type:disc">both the red (false positive) and green (true positive) increases</li></ul><figure id="09343f5f-5a3a-421e-8031-91bf31bc2bc4" class="image"><a href="Untitled%20145.png"><img style="width:555px" src="Untitled%20145.png"/></a></figure><h3 id="d1bceb76-d195-44ed-835c-f8eb22159e3e" class="">More aggressive Mindset</h3><p id="0f1a75d9-607b-4b96-b870-6d1091de3034" class="">when we focus more on the positive data points</p><ul id="806ae4c0-d499-45a8-88bc-0061ba9081a1" class="bulleted-list"><li style="list-style-type:disc">further increases true positive rate</li></ul><ul id="a0710e8d-bf33-42c1-a24f-43da639cb57b" class="bulleted-list"><li style="list-style-type:disc">also increases false positive rate</li></ul><figure id="9b635e89-2147-499e-b050-9cf0c9a5f751" class="image"><a href="Untitled%20146.png"><img style="width:555px" src="Untitled%20146.png"/></a></figure><h3 id="43be54fc-c1eb-4e13-b58f-d5fbaef524b2" class="">ROC Curve</h3><p id="a0d3b550-7be6-4bf5-a882-1f9595bf144b" class="">an extreme point where the TPR is prefect, but also the FPR is also perfect</p><p id="fb1bcab7-9db3-4be0-86a7-857450bed0c4" class="">meaning that it is 1</p><figure id="4d039734-b07e-47f2-8192-8b9463e01d9a" class="image"><a href="Untitled%20147.png"><img style="width:555px" src="Untitled%20147.png"/></a></figure></details></li></ul><ul id="7238b621-9e1a-4326-bfd0-8fb8931fc160" class="toggle"><li><details><summary>AUC</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="95aef5e3-1166-432a-bd42-bf274991ac34"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">one way to convert the entire ROC curve into a single performance metric<br/><br/>it is the total area under the ROC curve <br/><figure id="14ddd863-3a33-43c7-afa2-37a05397210e" class="image"><a href="Untitled%20148.png"><img style="width:240px" src="Untitled%20148.png"/></a></figure><figure id="c1ba50be-35b8-4600-ac6f-09d79f836556" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mn>0.5</mn><mo>&lt;</mo><mi>A</mi><mi>U</mi><mi>C</mi><mo>&lt;</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">0.5&lt;AUC&lt;1.0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6835em;vertical-align:-0.0391em;"></span><span class="mord">0.5</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.0</span></span></span></span></span></div></figure><p id="3b43d815-24f4-4d01-a290-7692386c284b" class="">1.0 is ideal, 0.5 is worst case<br/>higher the AUC value the better<br/></p></div></figure><p id="83874c96-3535-4c4c-8c70-4811fb6866f0" class="">AUC can give a biased interpretation because it focuses on the left side/x axis</p><p id="e7107e7f-1ef8-44ac-bb24-35ae80ce9b6d" class="">
</p></details></li></ul></details></li></ul></details></li></ul></details></li></ul><ul id="863c1873-7aeb-4370-95e1-108b0f57ac74" class="toggle"><li><details><summary>Regression</summary><ul id="63301dc4-41f9-4a42-9f70-7ec17566fb95" class="toggle"><li><details><summary>Why Regression</summary><ul id="941fad5a-0ba6-4e44-9635-554e25720cbc" class="bulleted-list"><li style="list-style-type:disc">another way to supervised learning apart from classification <figure id="9f0d269c-a18a-4809-b836-887bb8ffa8c3" class="image"><a href="Untitled%20149.png"><img style="width:237px" src="Untitled%20149.png"/></a></figure></li></ul><ul id="67007c4b-4a37-4164-b453-db78ebd2145f" class="bulleted-list"><li style="list-style-type:disc">can be <mark class="highlight-purple_background">used to make predictions </mark>EG: finding house prices<figure id="cfbea5f5-5338-4135-9e15-d3c41f2cb8a1" class="image"><a href="Untitled%20150.png"><img style="width:596px" src="Untitled%20150.png"/></a></figure></li></ul><ul id="1c38c87f-cd79-4266-bb70-ce12f620e8aa" class="bulleted-list"><li style="list-style-type:disc">instead of the output being binary (class label is true or not) it can output a number</li></ul></details></li></ul><ul id="7cec7835-6d0c-4294-a412-0bdb496989e5" class="toggle"><li><details><summary>What is Regression Analysis</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="d7cd1962-1036-4cb4-9e60-a25e1e43403c"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">produces a regression equation <figure id="e0f852b5-18fd-4519-9b18-7824b1d13950" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Y_i=f(X_i,b)+e_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div></figure><ul id="4db664df-e8c5-4cba-81c7-960b2039d12d" class="bulleted-list"><li style="list-style-type:disc">widely used for prediction</li></ul><ul id="a9c8667d-6c74-4989-a520-4b183f948655" class="bulleted-list"><li style="list-style-type:disc">we try to reduce the error to make the output Y more accurate</li></ul><ul id="e50c4428-15e0-441a-81dc-101dfad9dd25" class="bulleted-list"><li style="list-style-type:disc">describe the relationships <mark class="highlight-purple_background">between a set of independent variables</mark> and the <mark class="highlight-purple_background">dependent variable</mark></li></ul><ul id="0d5de57a-22c8-4957-8495-e2467ee9d9d9" class="bulleted-list"><li style="list-style-type:disc">describe how the <mark class="highlight-purple_background">changes in each independent variable</mark> (Xi ) are<mark class="highlight-purple_background"> related to changes in the dependent variable</mark> (Y)</li></ul><ul id="5c8be143-538e-4483-a802-950347df732a" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-purple_background">outputs continuous quantify</mark> rather than discrete class label </li></ul></div></figure></details></li></ul><ul id="0ed139fd-fdc8-476f-b963-4797860e9507" class="toggle"><li><details><summary>Simple Linear Regression</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="6eeac3e8-ac95-45e3-a584-28492965c24a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a linear regression equation that takes into account only <mark class="highlight-purple_background">1 independent variable</mark><figure id="9b71cddf-e2a3-40cd-a9b8-a22e03ed0ad4" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>c</mi><mi>e</mi><mi>p</mi><mi>t</mi><mtext>  </mtext><mo>+</mo><mtext>  </mtext><mi>s</mi><mi>l</mi><mi>o</mi><mi>p</mi><mi>e</mi><mo>+</mo><mtext>  </mtext><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">Y_i=intercept~~+~~slope+~~error</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">in</span><span class="mord mathnormal">t</span><span class="mord mathnormal">erce</span><span class="mord mathnormal">pt</span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">error</span></span></span></span></span></div></figure><figure id="6fc29f9e-8780-4744-a00d-f722ea7fc7af" class="image"><a href="Untitled%20151.png"><img style="width:384px" src="Untitled%20151.png"/></a></figure></div></figure><h3 id="8f388720-02a9-49ad-aeb1-575790284bc5" class="">Reduce Total Error</h3><figure id="f9017f42-cac9-40f1-b977-f8b00e03fa01" class="image"><a href="Untitled%20152.png"><img style="width:1163px" src="Untitled%20152.png"/></a></figure></details></li></ul><ul id="8594f0fc-cc9d-476f-9460-715676a9fd0f" class="toggle"><li><details><summary>Multiple Linear Regression</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="9e563dc7-4adc-4c39-ba6b-d057f5d2a6ad"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a linear regression equation with multiple independent variables/features<figure id="20cf3cb1-ffba-438b-ba17-7eb3ca2f1ced" class="image"><a href="Untitled%20153.png"><img style="width:901px" src="Untitled%20153.png"/></a></figure><ul id="b0baa801-b34c-45b5-b9db-8b70ae28daa8" class="bulleted-list"><li style="list-style-type:disc">represented as a hyperplane rather than a straight line graph</li></ul><ul id="3495c8a1-4e29-4ed0-80c6-f59deba58ee0" class="bulleted-list"><li style="list-style-type:disc">one intercept, but many slopes (called coefficients/weights)</li></ul></div></figure><h3 id="750ff190-8753-4aaf-ad0d-732832488314" class="">Reduce Total Error</h3><p id="cd5a9053-4b7d-4e8d-a6a4-096b454afc82" class="">to reduce the total error, we need to find the optional values for the weight vector that minimizes the total square error</p><figure id="f894e26f-7115-4ef5-9c1e-eb3eab85ab69" class="image"><a href="Untitled%20154.png"><img style="width:1114px" src="Untitled%20154.png"/></a></figure><p id="238d2711-eb93-4e85-8b72-aa3b253b80f3" class="">
</p></details></li></ul><ul id="9b7f3bb4-cf80-4918-98bd-ba8d68419f4b" class="toggle"><li><details><summary>Regularisation</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="67305980-4590-4c7c-9f9f-a0674bd21889"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">if we only consider reducing the square error, then W will get very large and cause overfitting and complexity <ul id="b6d07bac-7797-43f0-8972-d6a14723834e" class="bulleted-list"><li style="list-style-type:disc">model tries to remember the training data points, this hurts its accuracy on unseen data </li></ul><p id="49c4ebfc-1bcc-425a-b508-d942f10a5eef" class="">Regularisation adds a penalty to control the size of W</p></div></figure><h3 id="2a6954a6-963c-43f7-89dc-4906a7fb634c" class="">Regularisation Techniques </h3><figure id="97de39ad-ef89-471d-aa11-8467f271b228" class="image"><a href="Untitled%20155.png"><img src="Untitled%20155.png"/></a></figure><ul id="453e3f3e-07b9-445a-8424-588429af091c" class="bulleted-list"><li style="list-style-type:disc">lasso can be used as embedded feature selection, as this method will assign weight of 0 to less important features</li></ul><h3 id="c7a1faa6-f167-474a-b5f1-87d14009ebfa" class="">Lasso Regression vs Ridge Regression</h3><figure id="e5b15d80-7ad1-4fd6-8ca6-ed3b4b3bb9f7" class="image"><a href="Untitled%20156.png"><img style="width:1214px" src="Untitled%20156.png"/></a></figure><ul id="b2b1ed1d-e1f7-4d15-98fc-57a1ed0ca7c2" class="bulleted-list"><li style="list-style-type:disc">for ridge regression, as the log lambda increases, all the features decreases towards 0. however they are not exactly 0</li></ul><ul id="1148082e-bb34-4234-bab5-d5fd958a8bcf" class="bulleted-list"><li style="list-style-type:disc">for Lasso regression, as log lambda increases, it removes the features (sets it to 0) at different rates and it is removed (considered irrelevant)</li></ul></details></li></ul><ul id="cdd07cff-fb89-4100-8405-a517b3a108ba" class="toggle"><li><details><summary>Non-linear Regression</summary><ul id="d073b0d3-0d16-417d-ae15-4006d3478bb0" class="bulleted-list"><li style="list-style-type:disc">Polynomial Regression</li></ul><ul id="092060e5-f056-4ad5-accc-d54f4f43406b" class="bulleted-list"><li style="list-style-type:disc">Gaussian Process Regression</li></ul><ul id="ea7524de-af61-4f96-9fb8-fc8fd6715c26" class="bulleted-list"><li style="list-style-type:disc">Exponential Growth Regression</li></ul><ul id="f15b9a0c-a1ad-47ef-8835-bd5163885cef" class="bulleted-list"><li style="list-style-type:disc">Logistic Growth Regression</li></ul><ul id="179edd1f-7f9a-4091-a51f-1e87fbd7b60e" class="bulleted-list"><li style="list-style-type:disc">Genetic Programming: no model assumption</li></ul></details></li></ul><ul id="18ecf4ca-00e2-4a65-a4b2-66b06ae44cfb" class="toggle"><li><details><summary>Evaluating Regression Model Performance </summary><ul id="38b472d3-6a8a-49db-9f2f-52a528529224" class="toggle"><li><details><summary>Mean Squared Error</summary><figure id="cbfa8a38-30ec-455a-ada4-8985dc911f7f" class="image"><a href="Untitled%20157.png"><img style="width:965px" src="Untitled%20157.png"/></a></figure><ul id="e466e3d7-3f63-40b8-93b8-774c8ca24988" class="bulleted-list"><li style="list-style-type:disc">not used as much because the value is large which makes it harder to understand and outliers that are very far away will make the value even bigger</li></ul></details></li></ul><ul id="b307821a-1ac4-4aff-9ca2-854f7a06214f" class="toggle"><li><details><summary>Root Mean Squared Error</summary><figure id="00829b9b-f701-4f31-b2d9-568d7861efef" class="image"><a href="Untitled%20158.png"><img style="width:596px" src="Untitled%20158.png"/></a></figure><ul id="14991cc4-3f09-409f-9a94-c33b7147f524" class="bulleted-list"><li style="list-style-type:disc">similar to the mean square error but its squared</li></ul><ul id="abab7a05-39f5-4bd3-858a-e0631e61a022" class="bulleted-list"><li style="list-style-type:disc">easier to interpreted </li></ul><ul id="3b8f5db4-4aa4-408e-8063-f7333fcccf85" class="bulleted-list"><li style="list-style-type:disc">its harder to optimize with a gradient based method because getting the differences is more complex <ul id="b6e91a90-81d3-4c91-8fec-01c8e80c7f3e" class="bulleted-list"><li style="list-style-type:circle">so its not used as much now adays </li></ul></li></ul></details></li></ul><ul id="8b62636a-3de7-4968-b013-7617e46104b7" class="toggle"><li><details><summary>Relative Squared Error</summary><figure id="336d5876-abc8-4e85-b680-c9e2ee70ebf4" class="image"><a href="Untitled%20159.png"><img style="width:971px" src="Untitled%20159.png"/></a></figure><ul id="aeb43578-aa24-4828-87b8-be0b49d19c15" class="bulleted-list"><li style="list-style-type:disc">more interpretable than mean square and root mean square error</li></ul><ul id="cf9889c0-85a9-4da4-931d-e28b81e4e26b" class="bulleted-list"><li style="list-style-type:disc">finds the ratio between the total square error and normalized </li></ul><ul id="0def7a6c-8d18-483f-a208-37abebcef934" class="bulleted-list"><li style="list-style-type:disc">most recommendable error to use</li></ul><ul id="c2be8a86-cc74-44f7-a3c7-7de1414670a0" class="bulleted-list"><li style="list-style-type:disc">when there is an outlier, it dominates the output of the error</li></ul></details></li></ul><ul id="2b79b7ff-2b01-4f74-ad25-e6eee63c5077" class="toggle"><li><details><summary>Mean Absolute Error</summary><figure id="d5e397c8-2720-46e2-9378-f7eaf5899340" class="image"><a href="Untitled%20160.png"><img style="width:965px" src="Untitled%20160.png"/></a></figure><ul id="6f73aa5f-d160-44be-8806-36043ab19293" class="bulleted-list"><li style="list-style-type:disc">minimizes the impact of outliers </li></ul></details></li></ul></details></li></ul></details></li></ul><ul id="d9eef1ca-a4fa-4cfb-8ff8-0bf9155a37ff" class="toggle"><li><details><summary>Clustering</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="c1131fe7-724f-4d05-ba30-986d4fc324ee"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">grouping set of objects that are most similar to each other<br/>such that objects in one cluster are more similar to each other than objects in another cluster<br/><figure id="2678211a-157f-42bc-b754-a453cb1fc52c" class="image"><a href="Untitled%20161.png"><img style="width:1167px" src="Untitled%20161.png"/></a></figure><ul id="ea197c36-be15-4222-813d-2744dc79ead5" class="bulleted-list"><li style="list-style-type:disc">unsurprised learning because no feedback from the label</li></ul><ul id="89c5dc37-8082-4b18-a614-1c258419009c" class="bulleted-list"><li style="list-style-type:disc">number of classes are unknown</li></ul><ul id="478d6c44-26a2-454f-aa39-20b202d8f0a7" class="bulleted-list"><li style="list-style-type:disc">no training data required</li></ul><ul id="2f46eeaf-2339-4a1d-b679-d330cd314939" class="bulleted-list"><li style="list-style-type:disc">aim is to work on existing data</li></ul></div></figure><ul id="df32dac1-4bf1-49d6-b575-722a55759334" class="toggle"><li><details><summary>Real world application for Clustering</summary><h3 id="33fb5581-c7dd-4908-97c3-5d0f207ac675" class="">Clustering in Wireless Networks</h3><figure id="606bb1d0-21bb-4df2-86f8-f3669fe6e9c0" class="image"><a href="Untitled%20162.png"><img style="width:624px" src="Untitled%20162.png"/></a></figure><p id="9030423a-b233-4636-b34f-f0c7ab6d80a5" class="">sensers are grouped in clusters, these clusters are connected to the cluster head which is able to remove any irrelevant data and reduce energy consumption . the output of the cluster head is then sent to the base station </p><h3 id="d5965f48-929d-4dcb-a031-2a80b5eab00d" class="">Customer Segmentation</h3><p id="92fcd067-b67a-4dd1-a488-6487544a3a0f" class="">can cluster customers that are similar to each other</p><figure id="64f21ce2-cf0a-4b9c-9531-18600e6590cd" class="image"><a href="Untitled%20163.png"><img style="width:624px" src="Untitled%20163.png"/></a></figure><h3 id="e00d31ee-c3d6-40e1-a63c-0c76952bf333" class="">Other Applications</h3><ul id="34d1d99d-0328-40d4-9445-47a7ea0ee73c" class="bulleted-list"><li style="list-style-type:disc">cluster different dogs</li></ul><ul id="c3de5329-070f-40cf-938a-24c6118ae0c7" class="bulleted-list"><li style="list-style-type:disc">cluster brain activities when doing different tasks</li></ul><figure id="e75e9ab6-3f50-4214-8af9-8212d3ce1ee0" class="image"><a href="Untitled%20164.png"><img style="width:850px" src="Untitled%20164.png"/></a></figure></details></li></ul><ul id="fcc7787b-c347-43c6-82fc-b85afe548975" class="toggle"><li><details><summary>What is similarity?</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="92368311-9cef-4508-97ab-20d9dc1b3e50"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">similarity is hard to define, making clustering subjective<ul id="d1dd62b3-effc-40f4-9489-e407dba89046" class="bulleted-list"><li style="list-style-type:disc">Typically measured by a distance or similarity measure</li></ul><ul id="2a38386f-f440-4704-be15-3f36a360024d" class="bulleted-list"><li style="list-style-type:disc">because the output can be different, when doing clustering we should run the clustering algorithm multiple times so we know the pattern is correct</li></ul><ul id="4b149189-445b-4996-a4d8-c0240375cebb" class="bulleted-list"><li style="list-style-type:disc">Different measures lead to different clusters -&gt; clustering is subjective</li></ul></div></figure></details></li></ul><ul id="ab003f79-a910-4c91-9204-1fbc55abfc85" class="toggle"><li><details><summary>Distance measures</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="68404f39-0ecb-4a3c-bfe1-944e88a59ea9"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">takes 2 objects from a universe of possible objects and calculate a real number distance/dissimilarity between the objects</div></figure><ul id="29177d3e-7227-4c23-9df5-3beb96caf235" class="bulleted-list"><li style="list-style-type:disc">the distance metric used depends on the data type<ul id="30203f49-0255-45b0-b968-834d44df6499" class="bulleted-list"><li style="list-style-type:circle">for Numerical features: Euclidian distance, Manhattan distance, Cosine distance</li></ul><ul id="120e4ac6-b2c0-46ba-ac1e-8e0da45a6f87" class="bulleted-list"><li style="list-style-type:circle">for Categorical features: Hamming distance</li></ul></li></ul></details></li></ul><ul id="c994740f-623c-486f-90d0-1228b38a7876" class="toggle"><li><details><summary>Clustering methods</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="487e2540-a9a4-4787-9cb1-179afdca446f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">two types of clustering methods, Hierarchical and Partitional<figure id="c66b9802-6fbf-4be5-8e17-01745aa06038" class="image"><a href="Untitled%20165.png"><img style="width:564px" src="Untitled%20165.png"/></a></figure></div></figure><ul id="fc8034c6-8f1a-4338-af6c-f75099f1ece1" class="toggle"><li><details><summary>Hierarchical algorithms</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ee98faea-37e9-4912-bbaa-02c692268e38"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Create a hierarchical decomposition of the set of objects using some criterion</div></figure><ul id="f5f79579-c2a1-4777-a5dc-69bea7e60aa5" class="toggle"><li><details><summary>Hierarchical Clustering (1)</summary><figure id="f8a57c73-5a0b-4494-9010-7a6fcb46e1d3" class="image"><a href="Untitled%20166.png"><img style="width:1072px" src="Untitled%20166.png"/></a></figure></details></li></ul><ul id="d2d5512a-fb44-49e0-9c34-46509e7d6455" class="toggle"><li><details><summary>Dendrogram</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="9bd53eaf-f066-4c00-ab4d-05d256d2f0b6"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">hierarchy of clusters is represented as a tree/dendrogram<br/><br/>The dissimilarity between two observations is related to the vertical height at which they first get merged into the same cluster. The greater the height, the greater the dissimilarity<br/><br/><figure id="7e2f416e-62d6-4ab3-9fe2-284d27035215" class="image"><a href="Untitled%20167.png"><img style="width:508px" src="Untitled%20167.png"/></a></figure></div></figure><h3 id="72350542-d4c7-4aa3-8097-abcffa2052ae" class="">“Cutting” a dendrogram</h3><p id="dba3ae36-5021-48b0-9c0c-f967f941b712" class="">Cutting a dendrogram horizontally gives a natural clustering. The<br/>height of the cut determines the number of clusters<br/></p><ul id="b98f0e7b-220b-4e06-8ebe-2b63eb2fa648" class="bulleted-list"><li style="list-style-type:disc">No need to re-run to get different number of clusters</li></ul><figure id="05d3a1de-5a33-4f9e-8d62-ac0553bef5df" class="image"><a href="Untitled%20168.png"><img style="width:568px" src="Untitled%20168.png"/></a></figure><p id="d7fa71c0-05bc-4d9d-9fd6-c7fc59916fee" class="">with 4 clusters</p><figure id="dca95a6a-5286-4345-82e6-5817e009a0ef" class="image"><a href="Untitled%20169.png"><img style="width:568px" src="Untitled%20169.png"/></a></figure></details></li></ul><ul id="afa343cf-c02f-4733-8599-707f336424b0" class="toggle"><li><details><summary>Building Dendrogram</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="0bd20d7d-682c-4b30-976c-1b5255ed983b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">there are 2 way to build a dendrogram<figure id="89e6757b-485e-4fce-952a-aebd99a906e9" class="image"><a href="Untitled%20170.png"><img style="width:720px" src="Untitled%20170.png"/></a></figure></div></figure><ul id="5c787092-2314-44ff-a938-ad529098729e" class="toggle"><li><details><summary>Agglomerative or bottom-up</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="353b170a-8038-4481-a511-e2f6195fb92a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">clustering where we start with the observations in n clusters – the leaves of the tree – and then merge clusters – forming branches – until there is only 1 cluster, the trunk of the tree</div></figure><ul id="24b18aad-8418-47c0-89d2-ada23ee74809" class="toggle"><li><details><summary>Steps</summary><ol type="1" id="a7ebd4fc-034e-4135-a578-9dd135a418c5" class="numbered-list" start="1"><li>we begin by defining a distance matrix which contains the distances between every pair of objects in our database<figure id="9eebbc00-85cc-40c2-a315-85f60621a417" class="image"><a href="Untitled%20171.png"><img style="width:793px" src="Untitled%20171.png"/></a></figure></li></ol><ol type="1" id="c15404ab-2344-4d3b-9968-0990de436dcc" class="numbered-list" start="2"><li>Starting with each item in its own cluster, find the best pair to merge into a new cluster. <mark class="highlight-purple">Repeat until all clusters are fused together</mark><figure id="a3d3896e-e996-45c9-a2b6-a30bd21f1a13" class="image"><a href="Untitled%20172.png"><img style="width:512px" src="Untitled%20172.png"/></a></figure></li></ol></details></li></ul><ul id="de530b86-ad03-4de9-917f-bfcab19750f3" class="toggle"><li><details><summary>Linkage Methods</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="483da69b-0950-4fa9-9ba3-72f739a7f65b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Agglomerative Clustering merges clusters based on distance<br/>between clusters – defined by linkage method<br/><figure id="a40e6d66-16b1-4136-85b0-985a58ed63d1" class="image"><a href="Untitled%20173.png"><img style="width:452px" src="Untitled%20173.png"/></a></figure><figure id="a31d6a6e-c211-45e5-82ee-f10574d4e815" class="image"><a href="Untitled%20174.png"><img style="width:627px" src="Untitled%20174.png"/></a></figure></div></figure></details></li></ul><ul id="42fb28b7-9cc5-4749-b7a5-54254d0c3bac" class="toggle"><li><details><summary>Agglomerative clustering algorithm</summary><figure id="efb04db5-9093-4f77-96eb-e7036d875203" class="image"><a href="Untitled%20175.png"><img style="width:512px" src="Untitled%20175.png"/></a></figure><figure id="fc7e61f8-b986-41c9-8b05-ed8eea4a4946" class="image"><a href="Untitled%20176.png"><img style="width:783px" src="Untitled%20176.png"/></a></figure></details></li></ul></details></li></ul><ul id="985b256d-b933-429f-9fea-97a27466b184" class="toggle"><li><details><summary>Divisive or top-down</summary></details></li></ul></details></li></ul></details></li></ul><ul id="070497f2-3ac8-4900-ac98-b95e50707477" class="toggle"><li><details><summary>Partitional algorithms</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="7906fe10-9646-4cda-ac36-875bd9f883d1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Construct various partitions and then evaluate them by some criterion</div></figure><ul id="4a9037c1-f6d2-4c58-acab-116b37590145" class="toggle"><li><details><summary>K-Means</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="aef99809-5b7c-4333-9250-eecaf5a04b58"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a partitional method that that clusters data using k number</div></figure><table id="ac5a2e58-0787-4678-9720-eab940c196a1" class="simple-table"><tbody><tr id="7d09325c-198a-4ee4-935d-958be9597bff"><td id="ybHs" class="" style="width:275px">pros</td><td id="_rsp" class="" style="width:269px">cons</td></tr><tr id="94a3719a-219f-4fb4-9a0f-0b47b659ca18"><td id="ybHs" class="" style="width:275px">Very simple and flexible algorithms</td><td id="_rsp" class="" style="width:269px">Need to specify K in advance</td></tr><tr id="0787bf02-89f1-429b-9d55-7970f5312a44"><td id="ybHs" class="" style="width:275px">Scale well with large numbers of samples and features</td><td id="_rsp" class="" style="width:269px">Need to re-run to obtain clustering with different numbers of clusters</td></tr><tr id="fe711ef4-4a64-4aed-b137-af077d1c7143"><td id="ybHs" class="" style="width:275px"></td><td id="_rsp" class="" style="width:269px">Applicable when mean is defined, what about categorical data?</td></tr><tr id="ec39d371-ecf2-4405-928c-d59abdc959ba"><td id="ybHs" class="" style="width:275px"></td><td id="_rsp" class="" style="width:269px">Stochastic algorithm: different initialised centroids -&gt; different clusters<br/>have to run it multiple time<br/></td></tr><tr id="9b63ccba-a5af-4fa8-b601-0874e335d7e8"><td id="ybHs" class="" style="width:275px"></td><td id="_rsp" class="" style="width:269px">Usually convert to local optima</td></tr></tbody></table><h3 id="654a4d6d-2b2a-4c84-8fea-1b82cba4a2cf" class="">Steps</h3><ol type="1" id="778287fd-a181-4458-b6c2-66629644b64c" class="numbered-list" start="1"><li>Start with <mark class="highlight-purple">K</mark> <mark class="highlight-purple">random </mark>cluster centres, aka centroids<figure id="19687618-3231-478a-b8d1-3e6f9df1a223" class="image"><a href="Untitled%20177.png"><img style="width:778px" src="Untitled%20177.png"/></a></figure></li></ol><ol type="1" id="e434781d-5050-4d5c-8d5b-11af46d4a061" class="numbered-list" start="2"><li>Reassign: <mark class="highlight-purple">assign </mark>each instance/object <mark class="highlight-purple">to the nearest centroids</mark><figure id="9bf2858a-e724-457c-b223-92eabb3c911a" class="image"><a href="Untitled%20178.png"><img style="width:794px" src="Untitled%20178.png"/></a></figure></li></ol><ol type="1" id="3add5d9c-c474-4056-90b7-b9172a0d3883" class="numbered-list" start="3"><li>Updating: compute the new centroid for each cluster as the mean of the objects assigned to the cluster<figure id="0842c7fc-0093-433f-a83a-5f723f1c0588" class="image"><a href="Untitled%20179.png"><img style="width:561px" src="Untitled%20179.png"/></a></figure></li></ol><ol type="1" id="ee0e5507-893e-4438-afb1-026fb8c85238" class="numbered-list" start="4"><li>Repeat step 2 until no change to the centroids<figure id="4a8f7d74-9a8c-4601-8c50-3cc992735ba1" class="image"><a href="Untitled%20180.png"><img style="width:771px" src="Untitled%20180.png"/></a></figure></li></ol></details></li></ul></details></li></ul></details></li></ul><ul id="f1873448-087f-48ba-b24a-c1aade93fe7f" class="toggle"><li><details><summary>Clustering performance</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="8371e94c-d61b-4118-b65e-30d2401ee39e"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><figure id="f564120c-bc66-425e-afd2-dfccd7cf442f" class="image"><a href="Untitled%20181.png"><img style="width:564px" src="Untitled%20181.png"/></a></figure></div></figure><h3 id="ad800bfb-b258-4828-856c-58c90166e72e" class="">Silhouette Score</h3><figure id="18de92ff-13c6-4471-802e-54045319a391" class="image"><a href="Untitled%20182.png"><img style="width:979px" src="Untitled%20182.png"/></a></figure><h3 id="281c7c12-a40d-48a0-9fd5-de22f34965a1" class="">Other metrics</h3><ul id="b132882f-bcc4-488c-b6bd-2c675ddab568" class="bulleted-list"><li style="list-style-type:disc">Davies-Bouldin index</li></ul><ul id="587800ba-b6d8-42bd-8cef-fe8b2744b17f" class="bulleted-list"><li style="list-style-type:disc">Dunn index</li></ul><ul id="1f64cf22-f4c0-443a-8e23-f5ad7aacc9e2" class="bulleted-list"><li style="list-style-type:disc">Calinski-Harabasz Index</li></ul><figure id="6e8c93e9-1514-4c6c-a274-17799a5cb11a"><a href="https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">2.3. Clustering</div><div class="bookmark-description">Clustering of unlabeled data can be performed with the module sklearn.cluster. Each clustering algorithm comes in two variants: a class, that implements the fit method to learn the clusters on trai...</div></div><div class="bookmark-href"><img src="https://scikit-learn.org/stable/_static/favicon.ico" class="icon bookmark-icon"/>https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation</div></div></a></figure></details></li></ul></details></li></ul><ul id="6183919c-ad73-46aa-8e41-07b856ff93e3" class="toggle"><li><details><summary>Reinforcement Learning</summary><ul id="dfb1cd81-2c26-4f18-b9f6-18c62e4f8d4b" class="toggle"><li><details><summary>“Robot in a room” toy problem</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="6d9e4e6a-e28e-495d-865b-2bd56cca16bd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">robot is in a grid based room, the machine learning model has to make a decision (to go up, down, left, right) with its given state (position)</div></figure><figure id="c5a67a43-8bfa-405a-8480-01e1744212f1" class="image"><a href="Untitled%20183.png"><img style="width:1429px" src="Untitled%20183.png"/></a></figure><ul id="7f10b781-750d-4c2a-b6b7-b8f25b1f5064" class="bulleted-list"><li style="list-style-type:disc">a toy problem to introduce an example of reinforming learning</li></ul><ul id="f24955c3-75d9-470e-b009-ddd3713b4e2e" class="bulleted-list"><li style="list-style-type:disc">the ML agent is the robot and the environment is the room </li></ul><ul id="20d415f5-e21a-498f-9592-1fd427ba9cba" class="bulleted-list"><li style="list-style-type:disc">when the robot does an action the state is updated</li></ul><ul id="41348fb7-3aad-4a4d-909e-5a516e94f704" class="bulleted-list"><li style="list-style-type:disc">after the <mark class="highlight-yellow_background">state transition</mark>, the robot gets <mark class="highlight-yellow_background">incident feedback</mark> (whether it is doing a good thing or bad thing)<ul id="7004b3b1-a5f8-477b-aa1d-be407dbb7e50" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-yellow_background">this feedback is called a reward</mark></li></ul><ul id="58fc7069-b692-45e7-b278-420917f7f2e0" class="bulleted-list"><li style="list-style-type:circle">each step/decision the robot gives gives the model a reward</li></ul><ul id="4ddc9f0a-9cd4-48fb-8704-4b85983a80b3" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-yellow_background">we can add all the rewards together to get the “total reward”</mark></li></ul><ul id="b811d480-dbc1-4994-8e93-e7b071068c57" class="bulleted-list"><li style="list-style-type:circle">our goal for RL is to maximize the total reward</li></ul></li></ul><ul id="404ca33d-2374-4d0c-9894-f0d50a8a3c18" class="bulleted-list"><li style="list-style-type:disc">+1 and -1 are<mark class="highlight-yellow_background"> terminal states</mark> which causes the game to end<ul id="1edffe5b-ff3c-4540-8576-d8ee7e620f77" class="bulleted-list"><li style="list-style-type:circle">the value of the state determines the rewards the robot gets</li></ul></li></ul><ul id="8b68cf75-d8eb-44aa-a4bb-d5e0e3f3f3ad" class="bulleted-list"><li style="list-style-type:disc">gray box is an obstacle the robot cannot go to</li></ul><ul id="a9a49d3a-92e2-4d06-a2c2-25df062182d3" class="bulleted-list"><li style="list-style-type:disc">this is called a sequential decision problem <ul id="53e5c314-45b3-4921-906e-e119310d56e1" class="bulleted-list"><li style="list-style-type:circle">series of inter-dependent decisions </li></ul></li></ul><h1 id="6481f4e2-b018-4ef5-9643-ac803553458a" class="">Finding the Solution</h1><p id="958e401d-2dcb-4aca-b2e0-716f3aad470f" class="">this is one of the possible solution, but it is not the full solution nor the optimal solution</p><p id="5b5f5dfa-1813-476c-85da-9196a951230e" class="">this is a partial solution  </p><figure id="f4937a3d-1b86-493f-8848-f7c7b50f54bd" class="image"><a href="Untitled%20184.png"><img style="width:1069px" src="Untitled%20184.png"/></a></figure><p id="f7465352-888e-4810-8aee-5be24a0f7aae" class=""><mark class="highlight-yellow_background">name for the full solution is called the policy </mark></p><p id="d917d652-d5bf-4971-ba50-583f2c0ec7f1" class="">policy is the mapping function from each state to an action</p><p id="7997c745-a6d8-4a39-990e-4c737f40b34e" class="">the input is the robot’s current state while the output is the action</p><figure id="dbb6e2fb-a015-423a-84ab-2eb27eb6e0a7" class="image"><a href="Untitled%20185.png"><img style="width:287px" src="Untitled%20185.png"/></a></figure><h3 id="f215ce74-d557-48a8-b65b-f8b7fff6c63d" class="">Optimal policy</h3><p id="ff0600ce-7fbb-475b-9836-148e9b9ac81b" class="">full solution/policy is not unique, it is dependent on what actions you take</p><p id="a26674ab-d63e-4d6a-ad91-11fc029b461c" class="">we want to find a special policy called the “optional policy” which is the true solution</p><p id="133efc4b-dc38-4d9c-9053-84e13e54d936" class="">we can use it to follow the policy from any state to the terminal state that maximize the reward</p><figure id="86022610-eda6-4175-9a49-4f96859fa384" class="image"><a href="Untitled%20186.png"><img style="width:288px" src="Untitled%20186.png"/></a></figure><p id="5cec2834-31a7-4f88-8ac5-ac49e7100d64" class="">the optimal policy can change based on the reward you give to the model each time it makes a decision/state change</p><figure id="363be858-a420-4eb4-9136-e37217dcdfc9" class="image"><a href="Untitled%20187.png"><img style="width:336px" src="Untitled%20187.png"/></a></figure><p id="ecc76249-ffff-4714-a02b-bf11290d7114" class="">because of this we should set the reward carefully when modeling a problem</p><h1 id="3ea5cf66-4c88-45b8-bb48-41be70a23766" class="">Other examples</h1><figure id="035b419d-3ae7-4919-9b6a-e3660540c83e" class="image"><a href="Untitled%20188.png"><img style="width:624px" src="Untitled%20188.png"/></a></figure><p id="0876d72e-a5a9-447d-8bbc-d85b2a8bff9d" class="">
</p></details></li></ul><ul id="eff1933d-2876-4275-862f-30c180749c28" class="toggle"><li><details><summary>Real World Successful stories</summary><figure id="914ad0ae-a0b9-44f7-bab5-84c34908da0c" class="image"><a href="Untitled%20189.png"><img style="width:892px" src="Untitled%20189.png"/></a></figure></details></li></ul><ul id="a86669a6-9cdf-4572-a6e3-e9ddf3275811" class="toggle"><li><details><summary>Markov Decision Process (MDP)</summary><figure id="6346e214-1e18-431e-9736-604e0efb8b4f" class="image"><a href="Untitled%20190.png"><img style="width:541px" src="Untitled%20190.png"/></a></figure><ul id="662f1353-07d1-4a58-ac98-290814003da2" class="bulleted-list"><li style="list-style-type:disc">has a <mark class="highlight-yellow_background">set of states</mark>, <mark class="highlight-yellow_background">set of actions</mark> and an <mark class="highlight-yellow_background">initial state s0</mark></li></ul><ul id="a6972792-051a-4963-9aa4-132a28def972" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow_background">transition model</mark> P(s,as’)<figure id="f1f782f1-41fc-4c85-9b33-7463439b34b7" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mi>u</mi><mi>p</mi><mo separator="true">,</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo>=</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">P( [1,1], up, [1,2] ) = 0.8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">([</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mclose">])</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.8</span></span></span></span></span></div></figure></li></ul><ul id="a4ed40e0-2a86-4fba-a5c2-8632365d9a15" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow_background">Reward function </mark>r(s,a)<figure id="3281fa43-bf66-41fa-ad74-176c888984c1" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>r</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mn>4</mn><mo separator="true">,</mo><mn>3</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mi>u</mi><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r( [4,3],up) = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">([</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">3</span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">+</span><span class="mord">1</span></span></span></span></span></div></figure></li></ul><ul id="83bfdd9d-960c-40db-8ec8-70fa37653dbc" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow_background">Goal</mark>: maximize cumulative reward in the long run</li></ul><ul id="41a79b43-a66b-4325-aede-aedcff971a2c" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow_background">Policy</mark>: mapping from state to action<ul id="116ec213-a601-469d-826f-78974b51d1c2" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-purple_background">𝝿(s)</mark> or <mark class="highlight-blue_background">𝝿(s,a)</mark> (<mark class="highlight-purple_background">deterministic </mark>policy vs. <mark class="highlight-blue_background">stochastic </mark>policy)</li></ul><ul id="90080f62-1fa5-43c9-ab70-f502277f5df0" class="bulleted-list"><li style="list-style-type:circle">pi symbol means policy in RL</li></ul><ul id="83c98b02-4128-4e1e-9d5a-515608d0fa33" class="bulleted-list"><li style="list-style-type:circle">policy function that accepts only a state parameter produces an action, this is called a deterministic policy</li></ul><ul id="23281fb5-bdb6-4c39-ae33-390935391d00" class="bulleted-list"><li style="list-style-type:circle">policy function that accepts a state and action as a parameter produces the probability, this is called a stochastic policy</li></ul></li></ul><ul id="85880c5c-0087-4c21-9412-b873d1c2627b" class="bulleted-list"><li style="list-style-type:disc">Reinforcement learning<ul id="d8c577a7-1ba0-4f32-b454-40de24631d5f" class="bulleted-list"><li style="list-style-type:circle">transitions and rewards usually not available</li></ul><ul id="29ae3ba5-780a-4f7f-87a5-4748813e58bb" class="bulleted-list"><li style="list-style-type:circle">how to change the policy based on experience</li></ul><ul id="77589c3f-f13e-430d-9d58-49e4a256d7a5" class="bulleted-list"><li style="list-style-type:circle">how to explore the environment</li></ul><ul id="df38ce3b-38a3-41cc-9b96-b0ac96169c45" class="bulleted-list"><li style="list-style-type:circle">not everything is own to the agent, it has to learn it </li></ul></li></ul><h3 id="c242903f-e9f2-4500-8d89-cd09f5be56c6" class="">Example: student MDP</h3><figure id="15468d16-02e8-46ea-85ca-db7f5b39ef2c" class="image"><a href="Untitled%20191.png"><img style="width:624px" src="Untitled%20191.png"/></a></figure></details></li></ul><ul id="89a9506f-9b37-41c6-93b8-b0b296d94212" class="toggle"><li><details><summary>Exercise question</summary><figure class="block-color-red callout" style="white-space:pre-wrap;display:flex" id="548d588d-73be-447b-8489-6939bbced853"><div style="font-size:1.5em"><span class="icon">❓</span></div><div style="width:100%">How to define the MDP of the Super Mario game?<figure id="49f37d84-129c-401e-9b46-3bc0fe3caf01" class="image"><a href="Untitled%20192.png"><img style="width:890px" src="Untitled%20192.png"/></a></figure></div></figure></details></li></ul><ul id="021d1158-2f55-4a38-89b7-7341161dc6cf" class="toggle"><li><details><summary>Value functions</summary><ul id="a5e714ac-6494-4e06-a8b5-ada5d699235c" class="bulleted-list"><li style="list-style-type:disc">State value function: Vp(s)<p id="1e6c88e0-3ced-4c91-be07-b159c3109163" class="">expected return when starting in s and following π (policy)</p></li></ul><ul id="ab00a7ec-80b0-43e1-9a66-168f5b6b3d23" class="bulleted-list"><li style="list-style-type:disc">State-action value function: Q^π(s,a)<p id="165bea80-ccc8-4e86-8c6a-94d13bebf8c4" class="">expected return when starting in state, performing action, and following π (policy)</p></li></ul><ul id="0ca095e5-c13e-4e37-be37-1bfb509bac99" class="bulleted-list"><li style="list-style-type:disc">Useful for finding the optimal policy<ul id="9bbc5267-e529-4f0d-9302-84cda81cc940" class="bulleted-list"><li style="list-style-type:circle">can estimate from experience</li></ul><ul id="f766b940-2d80-4ad9-97ae-fb93cb689364" class="bulleted-list"><li style="list-style-type:circle">pick the best action using Q^π(s,a)</li></ul><ul id="a52fbcde-fef4-45b9-bb4f-b7ba7befc99d" class="bulleted-list"><li style="list-style-type:circle">value function is important because it helps you to improve your current policy </li></ul><figure id="cfe72b1e-3f6a-4b17-86bc-0b4ff2c3a752" class="image"><a href="Untitled%20193.png"><img style="width:240px" src="Untitled%20193.png"/></a></figure></li></ul><ul id="d6c209fe-f75b-47eb-a5c0-0f43c7f3d9ec" class="bulleted-list"><li style="list-style-type:disc">Bellman equation<p id="4fd02632-8c69-4086-99ec-f9f01088d6fc" class="">we can find the value function by solving the Bellman equation </p><figure id="f8f70688-4724-4802-aca0-e123eee26e1b" class="image"><a href="Untitled%20194.png"><img style="width:596px" src="Untitled%20194.png"/></a></figure><p id="8ef4dc00-e7bf-4030-8943-c0a5c0a32587" class="">its very complex so instead we can use learning techniques to find approximate solution for the equation </p><p id="bed6baa3-4b82-467e-875c-f3040b815627" class="">this approximate solution will be the value function for our policy </p></li></ul></details></li></ul><ul id="412f9ba2-8d11-487f-aae3-aac7167aa568" class="toggle"><li><details><summary>Iterative policy improvement</summary><p id="1d3a0681-e9f8-48b4-a8c0-e53e5c559962" class="">Two main components</p><ul id="7c9078cf-e316-4a2c-86c9-ae5e9b99922f" class="bulleted-list"><li style="list-style-type:disc">Policy evaluation<p id="d9217143-4fd5-49c2-b772-11e588e87993" class="">aim to learn the value function give a given policy pi</p></li></ul><ul id="7d1f5dae-b6b5-425a-9e33-2f8b08411eae" class="bulleted-list"><li style="list-style-type:disc">Policy improvement<p id="10d17270-df46-4eae-aa3a-2c70342c5953" class="">once we find the value function, we can use the policy improvement component to construct a better policy pipeline </p></li></ul><figure id="9abc8fec-4f84-4205-b303-1d5d7b38a284" class="image"><a href="Untitled%20195.png"><img style="width:624px" src="Untitled%20195.png"/></a></figure></details></li></ul></details></li></ul><ul id="82a36a79-1f1a-47aa-8f83-b93b735fa961" class="toggle"><li><details><summary>Neural Networks</summary><ul id="7eb207b6-399b-4e2f-93de-03dcc1cbe242" class="toggle"><li><details><summary>Why NN</summary><p id="3fd7a6aa-4e6a-468c-8f37-72c97c06405a" class="">a lot of the new machine learning applications and break through techniques in ai are driven by new techniques in NN</p><figure id="0d21f06d-85ca-472e-a0c8-1fd2c1bf845a" class="image"><a href="Untitled%20196.png"><img style="width:1150px" src="Untitled%20196.png"/></a></figure></details></li></ul><ul id="cb0d5bdc-d0e6-4a92-86d4-9c23640d772e" class="toggle"><li><details><summary>What is NN</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="aad6682d-fc27-4f88-8040-032a90d85380"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">NN is a machine learning model designed to recognize patterns and solve complex problems <ul id="716cfb6c-0810-4d1a-82ed-85e5f6777cc4" class="bulleted-list"><li style="list-style-type:disc">structure is inspired by our understanding of the human brain</li></ul><ul id="b4e61259-af1a-48df-9a87-e0f9df9f1ae2" class="bulleted-list"><li style="list-style-type:disc">consists of layers of interconnected nodes, or &quot;neurons,&quot; each of which performs simple computations</li></ul><figure id="e73cdea9-b49f-40a5-8bb1-079866eea8db" class="image"><a href="Untitled%20197.png"><img style="width:926px" src="Untitled%20197.png"/></a></figure></div></figure></details></li></ul><ul id="2a3c6e73-364e-4200-a1e4-73ccc102e2a3" class="toggle"><li><details><summary>Neurons </summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="8440316f-9e9a-4a40-b00d-f95b8f19af71"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">NN is a network of neurons<p id="ce99e9c2-4c3b-4dfe-b510-c584d504bcc3" class="">neurons are the basic component/unit of neural network</p><ul id="1c721c95-f38c-410e-b3f3-bac5f38f4289" class="bulleted-list"><li style="list-style-type:disc">also also referred to as a node or a unit,</li></ul><ul id="fe6d8125-744d-40e7-b2a2-0a02f88d9683" class="bulleted-list"><li style="list-style-type:disc">the basic unit of computation, as a processing point for carrying out specific calculations</li></ul><ul id="710b3ff5-d1ec-4ea8-9722-6d1fb45e5a5a" class="bulleted-list"><li style="list-style-type:disc">The primary function of a neuron in a neural network is to receive input, processes it, and generates output</li></ul><ul id="f345f1ca-7bd8-40e6-8d2e-684c2470a350" class="bulleted-list"><li style="list-style-type:disc">the input x is transformed into an output � using weights W and a bias � as: � = � �� + � where � is the activation function</li></ul></div></figure><h3 id="5c6adf86-a3e3-4c49-816a-94f830597939" class="">Weights and Bias</h3><p id="2eba3eb3-e857-437c-91e7-5b5a4c73d8f4" class="">neurons have 2 main parameters, weights and bias</p><p id="7146bc88-1d25-4524-8c4e-d1ca8f510235" class="">weights are multiplied with the input of the neuron. this signifies how important the an input is to the output of the neuron</p><p id="2d7921fc-e3f7-4b76-88c5-16c447ceb72b" class="">bias is an additive parameter that is added to the neuron to adjust/offset its output. bias tells us how big the weighted sum needs to be in order for the neuron to be active </p><figure id="c8dd62b1-6a52-44cd-9d6d-134b9b58c838" class="image"><a href="Untitled%20198.png"><img style="width:624px" src="Untitled%20198.png"/></a></figure><h3 id="6b90985f-e3ec-44fa-a0b3-375c9b050604" class="">Activation Function</h3><p id="ab786314-25f2-4318-add5-551a1fdd8e8c" class="">a mathematical function that takes the weighted sum of the neuron and regulate the output to a number between 0 and 1</p><p id="df42dc76-1442-4574-a943-43420143c73f" class="">an important role this plays is to introduce non-linearity into neural networks. allows it to solve complex data</p><figure id="d22af84e-39e7-4ff1-b715-be4bd2177c93" class="image"><a href="Untitled%20199.png"><img style="width:963px" src="Untitled%20199.png"/></a></figure></details></li></ul><ul id="0c9ccf79-e6dd-4685-8236-e697f7844bc5" class="toggle"><li><details><summary>Perceptron</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="bb25afbf-b076-4041-91fb-230699968c20"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">perceptron is the simplest neural network <br/>it is a binary linear classifier, based on a linear threshold unit.<br/>it calculates the weighted sum and applies the step function to output 1 or 0<br/><figure id="16619399-00db-45f6-8a10-d1b298477070" class="image"><a href="Untitled%20200.png"><img style="width:1007px" src="Untitled%20200.png"/></a></figure></div></figure><h3 id="47cca188-2149-413a-b26c-19fbe8d399a4" class="">Problem with Perceptron</h3><p id="88eecb4b-bcbe-4983-8d47-f9f1f44ebf7c" class="">there are problems with perceptron</p><p id="378f0672-3a82-4c88-895b-f35458e6530c" class="">one of the most famous ones is that because perceptron’s are linear, it cannot solve non linear problems such as XOR</p><p id="c8dcf2b7-bf6b-44a9-875c-1aa50ba4cbdd" class="">XOR is non linearly separable so you cannot separate it by one line</p><figure id="4b597506-75d5-4a93-9c3d-46465aacd6db" class="image"><a href="Untitled%20201.png"><img style="width:750px" src="Untitled%20201.png"/></a></figure><h3 id="f7e6251c-1d84-41a8-b7ff-bdf5b1443858" class="">Multi-layer Perceptron (MLP)</h3><p id="e38ec080-33b3-4798-98b6-edd9e0881cb8" class="">to solve the problem of perceptron being linear, we can use multiple perceptron’s that are linked together in a multi layered network, </p><ul id="2cb3a23a-a797-4fd3-81a4-62539aceacdf" class="bulleted-list"><li style="list-style-type:disc">where the output of one perceptron can be the one of the input of another</li></ul><ul id="6904728d-cf78-472d-8433-b19153f274ce" class="bulleted-list"><li style="list-style-type:disc">has an input layer, output layer and multiple hidden layers</li></ul><ul id="085e1d33-34ce-4d1b-aaac-dc56729159a6" class="bulleted-list"><li style="list-style-type:disc">&quot;universal approximation theorem” states that an MLP can theoretically approximate any continuous functions </li></ul><figure id="23a6f84a-9cc7-4c97-8fd9-8362ac4a777b" class="image"><a href="Untitled%20202.png"><img style="width:624px" src="Untitled%20202.png"/></a></figure><p id="7b77b5fb-e4c6-4536-9d39-ef3842a1517d" class="">
</p></details></li></ul><ul id="45bf7b69-8a12-4e38-99d0-4b8e787bc4f5" class="toggle"><li><details><summary>Layers of Neural Networks</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="25bbb03b-b956-443c-8a2d-da47f0585795"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">NN has multiple layers categories as 3 different types of layers<p id="80edb635-441c-40e4-ac0d-a4b2bca03f5d" class=""><mark class="highlight-red">Input Layer</mark>: The first layer that receives the input signal to be process</p><p id="6573be33-820b-4973-ab84-669c9c2d0b2b" class=""><mark class="highlight-red">Hidden Layers</mark>: One or more layers that perform computations through neurons and are not exposed to the input or output directly</p><p id="41951142-a0b8-4951-9684-a4616d72686c" class=""><mark class="highlight-red">Output Layer:</mark> The final layer that produces the output of the model</p><figure id="e89144cb-3199-4bc8-bd9f-d946f89e44c4" class="image"><a href="Untitled%20203.png"><img style="width:562px" src="Untitled%20203.png"/></a></figure><ul id="6583dadb-3617-497f-b03b-257b1e78e00b" class="bulleted-list"><li style="list-style-type:disc">deep learn means there many many hidden layers</li></ul><ul id="968c36e0-3b0e-4278-a78b-23c2ed5e544c" class="bulleted-list"><li style="list-style-type:disc">Why the network is organized in layers?: layers break problems into bite-sized pieces</li></ul></div></figure><h3 id="2be6192f-abad-44eb-8db1-0101ac7d5cd9" class="">Designing layers</h3><p id="7877c917-1b24-4940-a2be-b28dd68c01b7" class=""><mark class="highlight-red">Input layer </mark>mirrors the format and structure of input data</p><ul id="6793641b-1425-464a-a6df-cda3c8eeba7a" class="bulleted-list"><li style="list-style-type:disc">number of neurons in the input layer is typically equal to the number of features in the input data</li></ul><ul id="ce041363-b1d5-4b64-88e1-d5c2eda3ad5c" class="bulleted-list"><li style="list-style-type:disc">e.g., with images of size 28x28 pixels, need 784 (28x28) neurons, each representing one pixel value</li></ul><p id="2a94acfe-da8b-4022-8583-2219d06d06bc" class=""><mark class="highlight-red">Output layer</mark> is closely tied to the specific task</p><ul id="29d5f534-351c-4e1b-9a1d-7c3807a192bd" class="bulleted-list"><li style="list-style-type:disc">for classification tasks, the number of neurons typically corresponds to the number of classes, e.g., in a task to identify digits from 0 to 9, the output layer would have 10 neurons,</li></ul><ul id="7705797b-974a-4da2-baa7-f28d3d178fe9" class="bulleted-list"><li style="list-style-type:disc">For regression tasks, the output layer usually contains a single neuro</li></ul><p id="7c17f9a9-c417-4e59-8c2c-6e9b20149ef1" class="">it can be quite an art to the design of the <mark class="highlight-red">hidden layers</mark></p><ul id="4db377c9-1cbd-4325-a1f5-a7aa66068682" class="bulleted-list"><li style="list-style-type:disc">determining #hidden layers and #neuron in each layer</li></ul><ul id="d2cea69d-54d3-44e5-b5e5-9fbb447d85e0" class="bulleted-list"><li style="list-style-type:disc">increasing the number of hidden layers can enable the network to learn more complex patterns and features in the data</li></ul><ul id="604307e9-bdb1-451f-aab8-dfab2f58756b" class="bulleted-list"><li style="list-style-type:disc">use heuristics-how to trade-off the number of hidden layers against the time required to train the network, trade-off between underfitting and overfitting</li></ul></details></li></ul><ul id="418887d4-2126-4313-876f-77f032d64dfc" class="toggle"><li><details><summary>Numbers of Weights and Bias in MLP</summary><figure id="b16d5f55-4a6b-467c-930e-09867b803207" class="image"><a href="Untitled%20204.png"><img style="width:873px" src="Untitled%20204.png"/></a></figure></details></li></ul><ul id="d61d3c2f-3c17-4678-80e3-617c833be22a" class="toggle"><li><details><summary>Neural Network Architectures</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="7ead9855-4bdf-4aea-98f2-99a695c65b6b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">there are various NN architectures designed for specific types of tasks<figure id="f304db94-ec58-46e4-8d3e-350c06a95da2" class="image"><a href="Untitled%20205.png"><img style="width:562px" src="Untitled%20205.png"/></a></figure><ul id="4e499191-6706-40a8-bb87-4c9b41c0cd16" class="bulleted-list"><li style="list-style-type:disc">Recurrent NN is for sequential data such as natural language processing<p id="c3aa28be-eae7-4e27-be05-258c27065b8d" class="">the recurrent connection allows it to remember the input from previous layers </p></li></ul><ul id="7b29009d-ae1e-42f9-a2e4-dc23edc39b2d" class="bulleted-list"><li style="list-style-type:disc">Convolutional NN is for great data such as images<br/>has a convolution layer<br/></li></ul></div></figure></details></li></ul><ul id="aa213b83-9008-4930-8b7e-c2bbc048e7b7" class="toggle"><li><details><summary>Forward Propagation</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="a64200b7-12c7-4912-9c3e-8b2198479cb1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">input data is passed forward from the input layer through the hidden layer towards the output layer to generate an output/prediction<ul id="e10e49ff-dab9-40eb-aaa7-8412d3b2e020" class="bulleted-list"><li style="list-style-type:disc">This sequence of operations is crucial for both training the network and making predictions/inference</li></ul><figure id="5e33941e-4f49-4b09-a6b0-3185e984ce05" class="image"><a href="Untitled%20206.png"><img style="width:562px" src="Untitled%20206.png"/></a></figure></div></figure></details></li></ul><ul id="41cca934-145c-45b4-9682-fc43db754bae" class="toggle"><li><details><summary>Tensor</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="39ca0403-6e10-4581-9db6-94e84cd74a59"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">data structure that represents multi dimensional arrays of numerical values<ul id="bf3fd18f-962c-445b-9711-1287bb132382" class="bulleted-list"><li style="list-style-type:disc">its a block of data with a given number of dimensions and a size in each dimension</li></ul><ul id="5723e3ee-a16c-4152-9f72-e1a197836a1a" class="bulleted-list"><li style="list-style-type:disc">used in ML because our data from task to task can have different shapes, so tenser can be used for most of them</li></ul><ul id="76090c34-5664-4220-835a-1c7a477bca66" class="bulleted-list"><li style="list-style-type:disc">the fundamental data structure used to store and operate on data in various machine learning frameworks and libraries, such as TensorFlow, PyTorch, and NumPy</li></ul><figure id="de96bc65-a2d0-4bd3-96f4-2675c399f142" class="image"><a href="Untitled%20207.png"><img style="width:1165px" src="Untitled%20207.png"/></a></figure></div></figure><figure id="7c326f32-c133-480a-b825-392f1b40fd3f" class="image"><a href="Untitled%20208.png"><img style="width:624px" src="Untitled%20208.png"/></a></figure></details></li></ul><ul id="35d25536-25e0-4407-93f6-969ac362f211" class="toggle"><li><details><summary>Activation Functions</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="86962816-b606-4250-a226-941e2b4492ab"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">there are multiple different activation functions a neuron can use<figure id="e1d17177-5dd7-4c66-9dd3-68f432a7a5bc" class="image"><a href="Untitled%20209.png"><img style="width:877px" src="Untitled%20209.png"/></a></figure></div></figure><h3 id="f8b4d3be-f1a8-42d2-a7c5-34de1ab632e5" class="">Sigmoid Function (AKA the Logistic function)</h3><p id="86cf7aad-203a-4ea8-b904-6110cd4e9025" class="">maps input values to the range (0, 1) using a smooth S-shaped curve, making it suitable for binary classification tasks</p><ul id="2ff72c2a-1f55-4fe0-8035-98ada85de7db" class="bulleted-list"><li style="list-style-type:disc">output can be interpreted as a probability</li></ul><ul id="8f01e8b2-2116-4b84-8ea4-2c97cdef00eb" class="bulleted-list"><li style="list-style-type:disc">nonlinear activation function</li></ul><figure id="7d329c69-b05e-4a86-844b-db135821548e" class="image"><a href="Untitled%20210.png"><img style="width:336px" src="Untitled%20210.png"/></a></figure><h3 id="64ab9b25-13c0-48d6-8f25-a24cd596a5be" class="">Tanh (Hyperbolic Tangent) Function</h3><p id="e793e761-00aa-4f8e-9172-facba46effd6" class="">similar to the sigmoid but maps input values to (-1, 1)</p><ul id="62ef5fbf-c5a0-443e-b0c5-a568dc9b678e" class="bulleted-list"><li style="list-style-type:disc">providing a better symmetry around zero</li></ul><figure id="67e4bb06-f82a-45b2-ad94-574697ea9832" class="image"><a href="Untitled%20211.png"><img style="width:395px" src="Untitled%20211.png"/></a></figure><h3 id="811301aa-a5e7-49ce-8e7e-37e04fa7146b" class="">ReLU (Rectified Linear Unit) Function</h3><p id="a72a3517-3aff-45ca-ad00-fbec08909fe7" class="">a nonlinear function that returns the input if it is positive, and zero otherwise, max(0, �)</p><ul id="f5735c18-7900-4b48-ad95-2ab6aebbc419" class="bulleted-list"><li style="list-style-type:disc">one of the most widely used activation functions due to simplicity and effectiveness</li></ul><ul id="a6e1a58b-0dc9-4a71-ace1-a3938eb39cd9" class="bulleted-list"><li style="list-style-type:disc">encourages sparse representations by activating only a subset of neurons for any given input</li></ul><figure id="6030ec1d-0b6f-4de8-ab3a-873c2ddb58da" class="image"><a href="Untitled%20212.png"><img style="width:288px" src="Untitled%20212.png"/></a></figure><h3 id="d3abe4bc-d682-4170-9efd-60ded001c44b" class="">Softmax Function</h3><p id="588e292c-c3d3-4fed-8752-ddf936c23ae8" class="">maps the input to a probability distribution over a set of possible outcomes, using an exponential function</p><ul id="a60f5f13-9a2a-407a-86c2-9d82062fb88c" class="bulleted-list"><li style="list-style-type:disc">used in the output layer when are doing multiclass classification and can be mapped to a classification probabilities </li></ul><ul id="17820891-5fa3-4f36-b601-e43cd0b57b32" class="bulleted-list"><li style="list-style-type:disc">convert a vector of numbers into a new vector that reflects the probability distribution of the original vector&#x27;s values</li></ul><ul id="a4cafe73-7b56-4b72-a42e-a50988c86b6e" class="bulleted-list"><li style="list-style-type:disc">commonly used in the output layer of a neural network for multi-class classification tasks</li></ul><ul id="6c09685b-b479-433f-aefe-9cc5a1573e52" class="bulleted-list"><li style="list-style-type:disc">computationally expensive</li></ul><figure id="1c81bf83-3751-4adf-b9ae-e11db58c9518" class="image"><a href="Untitled%20213.png"><img style="width:889px" src="Untitled%20213.png"/></a></figure><h1 id="dae26fcb-90b7-4481-a16c-90b13ae452d7" class="">Which Activation function to use</h1><figure id="ddb47672-6d18-414f-8f10-711b02b21ddc" class="image"><a href="Untitled%20214.png"><img style="width:721px" src="Untitled%20214.png"/></a></figure></details></li></ul><ul id="5c95d965-a95e-4f0a-89e6-32f319875b21" class="toggle"><li><details><summary>History of Neural Networks</summary><figure id="cbb202b6-1fbd-4c94-aef9-0e7b934cbc36" class="image"><a href="Untitled%20215.png"><img style="width:624px" src="Untitled%20215.png"/></a></figure><figure id="8fe5f66c-ce89-4d3a-8854-55324b8687c4" class="image"><a href="Untitled%20216.png"><img style="width:624px" src="Untitled%20216.png"/></a></figure></details></li></ul><ul id="02271091-f663-4c38-ab3a-f2b7ac58b44d" class="toggle"><li><details><summary>Training NN</summary><ul id="023ba537-172e-45e2-884d-49037a227683" class="toggle"><li><details><summary>NN Before Training</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="c96d7133-3174-4f76-a6e3-3308d0b63fc0"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">for the NN output to mean anything we need to train it with data<br/>before it is trained, all it has been done is that the NN has been initalized with random weights and bias values<br/>so the output is just random numbers that means nothing<br/><figure id="87103dcf-1e8f-4022-ab46-31a7a3d352a5" class="image"><a href="Untitled%20217.png"><img style="width:916px" src="Untitled%20217.png"/></a></figure></div></figure></details></li></ul><ul id="025f58de-eed7-490f-a16c-3cdde8db7fd7" class="toggle"><li><details><summary>Perceptron Learning</summary><figure id="5e521437-1cd0-4d65-b97f-cf3629a2f1b8" class="image"><a href="Untitled%20218.png"><img style="width:970px" src="Untitled%20218.png"/></a></figure></details></li></ul><ul id="d1f5117a-54a1-43ef-a922-70d3b6e9e5fa" class="toggle"><li><details><summary>Overview-Training a NN</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="d908efe9-1006-4b48-80bf-902273412b44"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">teach a neural network to learn from mistakes thus making correct predictions<ul id="bd27229c-3669-4871-9680-c54ecac76408" class="bulleted-list"><li style="list-style-type:disc">adjusting weights either up or down so the error is reduced<br/>by comparing actual output is compared to target values using a loss function<br/></li></ul><figure id="ad6011f8-5282-4962-9dc3-52a9dc6887d0" class="image"><a href="Untitled%20219.png"><img style="width:816px" src="Untitled%20219.png"/></a></figure></div></figure></details></li></ul><ul id="1a10b7bd-f5fd-446f-9061-86b68380a259" class="toggle"><li><details><summary>Training a NN</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="cecc2b37-01b9-419b-b95c-72eee8a1765f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">A <mark class="highlight-blue_background">process to find the optimal set of weights W* and bias</mark> that results in the smallest cumulative <mark class="highlight-blue_background">loss </mark>across all the training data<ul id="b5eb7e52-28f2-422e-a135-71fa27bbb6c8" class="bulleted-list"><li style="list-style-type:disc">A complex optimisation problem: often involving high dimensional search space, non-linear transformations</li></ul><p id="76c006a9-8022-493a-8b62-a665332ff703" class="">Loss function:</p><figure id="3df71911-0a1a-44c8-bb78-34feaf41eb46" class="image"><a href="Untitled%20220.png"><img style="width:534px" src="Untitled%20220.png"/></a></figure><ul id="e9673cd6-687d-490b-a6e3-a3f502ab5a09" class="bulleted-list"><li style="list-style-type:disc">W represents the weights of the neural network</li></ul><ul id="dbf6fa2e-0d33-4d5a-8459-144898e9bf18" class="bulleted-list"><li style="list-style-type:disc">L represents the loss/cost/error function, measures the difference between the predicted output Yi and the actual target Yi</li></ul><ul id="d8b28917-8dfe-4721-806b-ba9aa1e86061" class="bulleted-list"><li style="list-style-type:disc">N is the total number of training samples</li></ul></div></figure></details></li></ul><ul id="53bf780a-570a-4208-9998-e04c210c5658" class="toggle"><li><details><summary>Training Loop</summary><figure id="7220266c-5e1e-4d74-8d49-b92bf0d7ed26" class="image"><a href="Untitled%20221.png"><img style="width:1022px" src="Untitled%20221.png"/></a></figure></details></li></ul><ul id="7992139e-440d-4674-9738-52afb34a3f92" class="toggle"><li><details><summary>Forward Pass</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="0670341c-a77a-45e9-a722-ecc103c326dd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">in order to calculate the loss at each instance we need to get the prediction for the training set inputs<br/>forward passing is when we input the training set into the neural network to get an output<br/><figure id="14eee50d-24de-4c1f-a86b-67a619b04b8f" class="image"><a href="Untitled%20222.png"><img style="width:1003px" src="Untitled%20222.png"/></a></figure><ul id="16248b89-ec56-4269-8bd9-8a76c240c41d" class="bulleted-list"><li style="list-style-type:disc">input layer passes the input data without processing the data into the hidden layer</li></ul><ul id="dfc41a69-706e-4aeb-a031-8a0cd462c3c0" class="bulleted-list"><li style="list-style-type:disc">neurons in the hidden layer will process the input data using the weights and biases and activation function<p id="4fc2f3bb-8dda-47ff-ae92-04b5f73e2568" class="">output of the hidden neuron then goes to the next layer which is either another hidden layer or the output layer</p></li></ul><ul id="69a35064-5d0c-4167-bccb-f49be808cf15" class="bulleted-list"><li style="list-style-type:disc">output layer does what the hidden layer does but its output is the output of the whole NN<p id="4b3b078f-0c7b-4834-b267-32e503269328" class="">the activation function may be different for the output layer </p></li></ul></div></figure></details></li></ul><ul id="fc3c10bc-b132-4f29-a808-a55d2cb922f2" class="toggle"><li><details><summary>Loss function</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="0ef10010-e00a-4a15-8fcc-6e47e39f1261"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">measures the difference between the target output vs actual output of NN<ul id="60cac750-6fb8-4bd8-aee2-98da5a2765dd" class="bulleted-list"><li style="list-style-type:disc">important function in NN training</li></ul><ul id="c4cd4992-50d2-4e81-8eb2-2a336b57c487" class="bulleted-list"><li style="list-style-type:disc">primary function is to give feedback to the NN during back propagation on how accurate it is and to adjust the weights and bias values</li></ul><figure id="b2062ff2-531f-4b25-9847-948ff2148084" class="image"><a href="Untitled%20223.png"><img style="width:949px" src="Untitled%20223.png"/></a></figure></div></figure><p id="a3b5af4b-9400-4724-8de1-d2f063dd205f" class="">Common Loss Functions</p><h3 id="9f6a8019-8284-46c9-83a5-69df07f054b1" class="">Cross-entropy Loss (for Classification)</h3><p id="fd11befd-2d0b-43fb-b883-37534e087cb6" class="">total cross-entropy loss over a dataset of N examples</p><p id="e00edac8-5203-4b68-84be-3312a425f579" class="">cross entropy measures the difference between the NN’s output (probability) from the target labels</p><figure id="08dc8246-5de5-4d90-802a-ffe4d7ad6c9e" class="image"><a href="Untitled%20224.png"><img style="width:1036px" src="Untitled%20224.png"/></a></figure><ul id="dc7da3d9-143e-4a68-8f0f-94aae3934ebb" class="bulleted-list"><li style="list-style-type:disc">y hat is a negative value</li></ul><h3 id="56a1769f-8d58-4e16-a6de-84befb4b66ed" class="">Mean Squared Error for Regression</h3><figure id="e4e08951-ede8-4986-a424-a7b400daa7b6" class="image"><a href="Untitled%20225.png"><img style="width:596px" src="Untitled%20225.png"/></a></figure><p id="9a06b626-4ca1-422c-98e0-236cb1e5971b" class="">
</p></details></li></ul><ul id="0f6c0b10-2df3-4819-8f60-b997e60081cb" class="toggle"><li><details><summary>Gradient Descent</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="2a6411ab-cb79-49ad-b09c-6fc94d4f9751"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">optimization algorithm that iteratively adjusts the network’s weights in the direction opposite to the gradient, minimizing the loss function<figure id="a1ff293d-43a1-40fa-b139-f6bb514afd92" class="image"><a href="Untitled%20226.png"><img style="width:1140px" src="Untitled%20226.png"/></a></figure><ul id="95ee4f02-dd02-49de-865b-34f5daf8f644" class="bulleted-list"><li style="list-style-type:disc">gradient points in the most steep direction</li></ul><ul id="dccc89e1-9222-4b5b-abac-a2f5bb79272c" class="bulleted-list"><li style="list-style-type:disc">gradient is a <mark class="highlight-red">vector of partial derivatives of the loss function with respect to each weight</mark><p id="ae771076-ae1d-4e70-aa23-512ab891a078" class="">each element represents how much loss will be reduced by changing the weight</p></li></ul></div></figure></details></li></ul><ul id="be3f629b-d742-4e41-ad7d-03387d2924eb" class="toggle"><li><details><summary>Back Propagation</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="8c015ded-07bf-453d-9a39-0a07a3548b41"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">process of going through the network backwards, adjusting the values of weights and biases by propagating the error found from the loss function</div></figure><ul id="6468eb88-7068-44e9-b043-3e2f2f87dbce" class="toggle"><li><details><summary>what is it?</summary><ul id="4236afcf-9494-458b-9c5f-5ee560d79333" class="bulleted-list"><li style="list-style-type:disc">gradients are computed backwards<ul id="2dd1d794-670e-40d3-aeb9-e37c3c4b7fe6" class="bulleted-list"><li style="list-style-type:circle">start at the outer layer and compute the gradient of the loss function with respect to each output</li></ul><ul id="1e66bf3c-d6c4-4987-93d8-6239a622e1e4" class="bulleted-list"><li style="list-style-type:circle">this rypically involves finding how much change in each output values affects the overall loss</li></ul><ul id="14966f6c-0872-4d9d-8a14-5325f3b7e2ce" class="bulleted-list"><li style="list-style-type:circle"></li></ul></li></ul><ul id="9350409f-7494-4c9a-ae4e-4029e89f8834" class="bulleted-list"><li style="list-style-type:disc">calculate the contribution of each weight to the loss function</li></ul><ul id="0d83862b-6e44-4c5e-82f4-0f2a39a63430" class="bulleted-list"><li style="list-style-type:disc">backward propagateing the gradeint of the error  <figure id="a0efa78f-956f-4e7a-80dd-124c394c228d" class="image"><a href="Untitled%20227.png"><img style="width:1170px" src="Untitled%20227.png"/></a></figure></li></ul><h3 id="0b985652-63d0-4092-b717-8bef59675c29" class="">Notes on BP algorithm</h3><figure id="95df8db5-0cc8-4757-89b6-1145d731ebce" class="image"><a href="Untitled%20228.png"><img style="width:596px" src="Untitled%20228.png"/></a></figure></details></li></ul><ul id="54ef6bb6-9723-403d-bb51-99e63ec193b1" class="toggle"><li><details><summary>Backpropagation Algorithm</summary><h3 id="e41e007d-2902-4e17-9703-be25f05626e5" class="">Backpropagation Algorithm</h3><p id="c91a11dd-ab7b-4f3e-94c7-3e7f1e7ecbdf" class="">the central algorithm in network learning</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e5410256-4388-46e5-a8ee-3f78855c0601" class="code"><code class="language-C++">1. Let 𝜂 be the learning rate
2. Set all weights to smaller random values
3. Until total error is small enough, repeat
	-For each input example
			- Feed forward pass to get predicted outputs
			- Compute 𝛽𝑧 = 𝑑𝑧 − 𝑜𝑧 for each output node
			- Compute 𝛽𝑗 = σ𝑘 𝑤𝑗→𝑘𝑜𝑘 1 − 𝑜𝑘 𝛽k
			- Compute the weight changes Δ𝑤𝑖→𝑗 = 𝜂𝑜𝑖𝑜𝑗 1 − 𝑜𝑗 𝛽j
			
	- Add up weight changes for all input examples
	Change weights according to the update rule of GD</code></pre></details></li></ul><ul id="4d2460a6-eb0e-49cf-877c-edf2b848d47b" class="toggle"><li><details><summary>Variants of Gradient Descents</summary><figure id="e0d1ce3b-dd9f-441f-ac71-a1cdf8dcf5bb" class="image"><a href="Untitled%20229.png"><img style="width:596px" src="Untitled%20229.png"/></a></figure><figure id="64896bc1-019f-49e7-86fd-735a4b13a6f0" class="image"><a href="Untitled%20230.png"><img style="width:568px" src="Untitled%20230.png"/></a></figure><p id="5e0c9c30-88ca-4687-898f-86cdbfdea250" class="">
</p></details></li></ul><ul id="0faf0c9c-0016-47bd-adfe-b98c5a16d651" class="toggle"><li><details><summary>Chain Rule</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="bbf21a77-da65-4248-9439-199823f58e35"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">used to calculate the partial derivative of the loss function (C) with respect to a weight (W)<br/>this is apart of the gradient<br/><figure id="f7e7f85c-f132-4b08-acac-e2200389cb8f" class="image"><a href="Untitled%20231.png"><img style="width:506px" src="Untitled%20231.png"/></a></figure><ul id="a1f0ad58-d5f7-42d7-969e-a2166761fe68" class="bulleted-list"><li style="list-style-type:disc">the chain rule states how to compute the derivative of a composite function</li></ul><ul id="00dbad14-ca81-4ef8-9f1e-116b1151d7f1" class="bulleted-list"><li style="list-style-type:disc">the chain rule allows to efficiently compute how small changes in the weight of one layer affect the loss function, by breaking down the computation into smaller, more manageable steps</li></ul></div></figure><h3 id="9e561ffe-fa75-4453-a480-c795e930b124" class="">Breaking down the 3 steps of the chain rule for partial derivatives</h3><figure id="174811b0-583b-4829-a20f-a387bb06452b" class="image"><a href="Untitled%20232.png"><img style="width:976px" src="Untitled%20232.png"/></a></figure><h3 id="4674c06f-298e-46cb-b33e-e603ea98b096" class="">Putting it all together</h3><p id="56d51f98-122e-4b02-bf54-8a2f0c73b58b" class="">Putting constituent derivatives together</p><figure id="9bc5bdce-f389-479b-9e6c-8e1ecb50a06d" class="image"><a href="Untitled%20233.png"><img style="width:478px" src="Untitled%20233.png"/></a></figure><p id="575932fe-6420-4d0f-93f0-fd57dbb02057" class="">This formula tells us how a change to that <mark class="highlight-blue_background">one particular</mark> <mark class="highlight-blue_background">weight </mark>in the last layer will affect the loss for that <mark class="highlight-blue_background">one particular training example</mark></p><p id="59d36069-d175-439c-ae9e-29136a261df7" class="">
</p></details></li></ul><ul id="b0194c07-4c9e-4170-8ed0-10e285d6e76c" class="toggle"><li><details><summary>Partial Derivatives</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="3640ad08-4e44-4e77-abc3-95f6dc6e27bd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">equation for calculating partial derivatives for each weights in the network<figure id="8d2042cf-3cc3-42b2-a661-3b9c170212f5" class="image"><a href="Untitled%20234.png"><img style="width:568px" src="Untitled%20234.png"/></a></figure><ul id="40759dd5-99eb-4175-a38d-4294733b9062" class="bulleted-list"><li style="list-style-type:disc">application of the chain rule</li></ul></div></figure><ul id="649365d6-7687-40d4-8c12-742f5de320e6" class="toggle"><li><details><summary>How we get here?</summary><h3 id="76bc13e6-d488-466d-bf88-b4af375d9421" class="">Simplify Notation</h3><p id="ffd0b9cb-f1e9-4bee-93f1-0b355add8333" class="">we can simplify the calculation of a neuron output by letting bias be an input with the weight of 1</p><p id="fe1a3fa5-a5fb-46d9-a199-9604b08871c9" class="">this will result in the same output as the previous but lets simplify each step</p><figure id="f4607c6c-167e-4cfa-a042-380f16da34d7" class="image"><a href="Untitled%20235.png"><img style="width:568px" src="Untitled%20235.png"/></a></figure><h3 id="ffda7dd8-66f4-4915-9076-279e840dbbe0" class="">How to calculate contribution of 𝐰(𝐋) to the loss function C?</h3><figure id="7a20cd74-2c4c-4093-a0a5-1e158448c552" class="image"><a href="Untitled%20236.png"><img style="width:568px" src="Untitled%20236.png"/></a></figure><ul id="1ce6a3e2-2ce2-488e-99cd-7aa9fe36a3d6" class="bulleted-list"><li style="list-style-type:disc">(L) signifies which layer the neuron is on</li></ul><ul id="1c29322d-2641-4f7a-81d1-1fbb253976f7" class="bulleted-list"><li style="list-style-type:disc">z(L) is the output of a neuron at a certain layer without the activation function</li></ul><ul id="2bf8721c-720d-4f5d-9d96-6087c848dc63" class="bulleted-list"><li style="list-style-type:disc">a(L) is the actual output of a neuron at a certain layer</li></ul><h3 id="963be492-3769-4d3b-a1ea-bf9b187555bf" class="">How sensitive the loss Ci is to small change in the weight W(L)</h3><figure id="47d0a82c-b29a-4087-bd90-db4e70760016" class="image"><a href="Untitled%20237.png"><img style="width:1247px" src="Untitled%20237.png"/></a></figure><p id="a79bbfb9-9265-4876-966d-ad4d49ac4ca1" class="">small change in w(L) will cause some change in z(L)</p><p id="b16ee688-a4c7-4102-8565-66619ec9c44d" class="">the sensitivity of ci (the rate of change ei derivative) determines the amount of change</p><h3 id="901d3e07-0289-4614-9f0a-be075682e2f0" class="">More …</h3><figure id="4520df9f-1b1d-43e5-8a52-e4a224ad206a" class="image"><a href="Untitled%20238.png"><img style="width:568px" src="Untitled%20238.png"/></a></figure><h3 id="f2a6bada-6bd9-4c70-b709-50c8a7f5e170" class="">Previous Layers’ Weights</h3><figure id="a897c5be-d5fa-4b60-8019-e5df0e94a605" class="image"><a href="Untitled%20239.png"><img style="width:924px" src="Untitled%20239.png"/></a></figure><h3 id="ba971369-b10e-43f9-b28a-cce5fbc3397f" class="">More Complicated Networks</h3><p id="291db9fd-635c-4710-ad1e-c8e678d3d505" class="">what we’ve seen so far is the process of calculating the gradient for a network with only 1 neuron per layer. </p><p id="ed852b87-3891-4dad-9482-f5f4d7ccd152" class="">actual networks have many neurons per layer</p><figure id="3a125fc4-eb17-43e8-9378-f74e1c7a1798" class="image"><a href="Untitled%20240.png"><img style="width:568px" src="Untitled%20240.png"/></a></figure><p id="c6d04051-aa1e-4b81-93a8-555dcb6b45fd" class="">the loss of the NN now (Ci) is the sum of all the neuron’s loss values</p><figure id="b84f4e83-504b-41cd-becd-9433a4bbfd6e" class="image"><a href="Untitled%20241.png"><img style="width:995px" src="Untitled%20241.png"/></a></figure><p id="29bcebc9-c928-4a83-8683-2491f3c0e5b7" class="">the formula is the same still, but we need to calculate the weighted sum of all the neurons in a layer together </p></details></li></ul><p id="6deadf1b-23a7-437a-b426-31e50bea41ad" class="">To get the derivative of L with respect to the weight, take<br/>the average over all training data<br/></p><figure id="b4d82e20-9806-47ec-91e9-6cee66d1c036" class="image"><a href="Untitled%20242.png"><img style="width:375px" src="Untitled%20242.png"/></a></figure><p id="5fcc6974-e8b2-4740-b1f7-502fd3eb26aa" class="">also need all the other derivatives with respect to all the<br/>other weights in the entire network<br/></p><figure id="389ea220-b4d4-430d-8662-c4011a826308" class="image"><a href="Untitled%20243.png"><img style="width:589px" src="Untitled%20243.png"/></a></figure><p id="5a49d962-3d24-47ae-bfaa-44d1d8f20eee" class="">Then update weights with 𝑾𝒊+𝟏 = 𝑾𝒊 − η𝜵𝑳(𝑾𝒊)</p></details></li></ul><ul id="9cbb965c-4666-48ee-a702-1d5c27ba20cd" class="toggle"><li><details><summary>Summary</summary><figure id="e5645c72-8f83-4e96-87cc-dd71060cd109" class="image"><a href="Untitled%20244.png"><img style="width:568px" src="Untitled%20244.png"/></a></figure></details></li></ul></details></li></ul><ul id="79919365-cc43-4293-87d8-ad051d342e9c" class="toggle"><li><details><summary>Adjusting Learning Rate</summary><figure id="fc3882f7-2c3e-4c0c-b999-01c80a6e0763" class="image"><a href="Untitled%20245.png"><img style="width:596px" src="Untitled%20245.png"/></a></figure><ul id="78382742-0552-4e8d-afa8-74465d965a75" class="bulleted-list"><li style="list-style-type:disc">too high learning will cause overshoot, missing the target</li></ul><ul id="d08cfd67-e099-4c32-a730-a9c6e1973609" class="bulleted-list"><li style="list-style-type:disc">too low learning rate will be slow and take too long</li></ul><p id="14682846-d101-4f80-9289-98d96e17e26d" class="">we need to find the middle ground, this is problem specific so we should start a some point and adjust the learning rate hyper parameter ourselves until we find a value that is good enough</p><h3 id="723a9e3c-154e-4b33-acca-4b98dc0d58da" class="">Adaptive learning rate methods</h3><figure id="3003b1a9-1584-4926-9820-2a39acc9e47e" class="image"><a href="Untitled%20246.png"><img style="width:929px" src="Untitled%20246.png"/></a></figure></details></li></ul></details></li></ul><ul id="8c3d2973-4048-4a67-b286-6a1f01ce84db" class="toggle"><li><details><summary>Tools and Libraries </summary><figure id="fea6e755-6064-49de-b212-5ec0cb82182e" class="image"><a href="Untitled%20247.png"><img style="width:1167px" src="Untitled%20247.png"/></a></figure></details></li></ul><ul id="892d188c-f2ad-496b-913c-be4d93f4870a" class="toggle"><li><details><summary>Avoid NN Overfitting</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="cd7c02ff-41e9-496c-9752-141d3e81bba9"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">NN’s flexibility to detect complex patterns also means its prone to overfitting<p id="c667a594-2940-4213-9625-b120af3f6537" class="">to avoid overfitting we can use <mark class="highlight-red">regularizations </mark>such as</p><ul id="6949600b-bc9c-4136-9259-0e09a0377e2e" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-red">early stop</mark>: interrupt training when validation set performance starts to drop</li></ul><ul id="eceec483-d8ed-4781-9104-005faf3ede2c" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-red">l1 &amp; l2 regulization</mark>: modify the cost function by adding λ|w_t| or λ||w_t||</li></ul><ul id="0015019c-0c3a-4cb1-8cd8-883028ddbaba" class="bulleted-list"><li style="list-style-type:disc">data <mark class="highlight-red">augmentation</mark>: add more data entries to the training set </li></ul></div></figure><ul id="ef638dae-8413-4340-b268-56c61ca2de33" class="toggle"><li><details><summary>l1 &amp; l2 regulization to add penalty to the loss function during NN learning</summary><figure id="cec6913d-a456-4488-8839-8a1954b8d157" class="image"><a href="Untitled%20248.png"><img style="width:1412px" src="Untitled%20248.png"/></a></figure></details></li></ul></details></li></ul><ul id="f1c355b2-6b03-479b-bf70-2d8284a70965" class="toggle"><li><details><summary>Automatic Differentiation</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="a86c2357-e04d-413d-9c23-a27dbafbea71"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">algorithmic/computational way to calculate derivatives using a Computational Graph<figure id="708225dc-ffff-4822-81b6-6b8ca3c9e54a" class="image" style="text-align:center"><a href="Untitled%20249.png"><img style="width:240px" src="Untitled%20249.png"/></a></figure></div></figure><ul id="9d124997-f0fa-4043-b4a9-50c898d94310" class="toggle"><li><details><summary>Traditional Differentiation Methods vs Auto Differentiation</summary><h3 id="65b7b337-934c-459f-b49f-68d14fe3b28f" class="">Numerical Differentiation</h3><p id="5bc22275-c419-4d42-bb7f-0f7e9fec4c70" class="">simply way to approximate derivatives of a function (by finite differences)</p><figure id="f5ad6643-1552-4ec3-bb59-fa97df7cb4ac" class="image"><a href="Untitled%20250.png"><img style="width:596px" src="Untitled%20250.png"/></a></figure><p id="19fdc106-1315-41cc-b7c6-8059a44aecc9" class=""><mark class="highlight-gray">straightforward but can lead to significant rounding errors and inefficiencies, especially in high-dimensional space</mark></p><ul id="8c9ce90a-fe87-4d49-a304-14fe7e013ebe" class="bulleted-list"><li style="list-style-type:disc">issue is that h needs to be as small as possible, else can lead to rounding errors</li></ul><h3 id="7111d997-dd19-4589-bd87-19830b1489d4" class="">Symbolic Differentiation</h3><p id="2bc9936f-c1c1-4565-b138-e4c7499fafeb" class="">calculates derivatives symbolically/analytically using rules</p><figure id="f8ad0800-4712-449c-ac43-371bf2e42df4" class="image"><a href="Untitled%20251.png"><img style="width:411px" src="Untitled%20251.png"/></a></figure><ul id="5f706c03-c126-454e-9456-811651439572" class="bulleted-list"><li style="list-style-type:disc">can handle complex expressions</li></ul><ul id="ba417409-1fae-47f0-801e-ccaa8c6dd992" class="bulleted-list"><li style="list-style-type:disc">often lead to inefficient code and suffers from expression swell, making it impractical</li></ul><ul id="76b40d7c-10db-4156-b8ed-16870359288d" class="bulleted-list"><li style="list-style-type:disc">simplifies functions </li></ul><ul id="98963ebe-eb1e-49b7-86a8-ac05d7cb87d3" class="bulleted-list"><li style="list-style-type:disc">although its easier, it isn’t as effective on complex functions found in neural network training such as the loss function</li></ul><h1 id="2f85874f-335e-4885-8aaa-9b8db12f6429" class="">Automatic Differentiation</h1><p id="f12816fd-5191-40fe-84ac-48af44d53633" class=""><mark class="highlight-gray">AKA, algorithmic differentiation, computational differentiation, Autodif</mark></p><p id="02b7b0e0-551e-4927-b1f9-b9257bdd1dc7" class="">computational technique that accurately evaluating derivatives of functions expressed as computer programs</p><ul id="ece5f91e-1543-40e8-8e3f-8713ff367c5a" class="bulleted-list"><li style="list-style-type:disc">tries to generate a numerical derivative rather than symbolic </li></ul><p id="5f589f66-5bd6-44d8-9ca3-d989b64fd7ec" class="">it works by building a data structure to represent the derivative and then uses it to calculate the derivative </p><figure id="eef12e43-a90f-4fe1-9467-2850db2bef6f" class="image"><a href="Untitled%20249.png"><img style="width:329px" src="Untitled%20249.png"/></a></figure><ul id="4e34cf9f-aa91-44db-b6ba-53869674f5d0" class="bulleted-list"><li style="list-style-type:disc">generate numerical derivative evaluations rather than derivative</li></ul><ul id="96eacce7-6e62-4e16-adc3-cf7304318e25" class="bulleted-list"><li style="list-style-type:disc">build up data structures to represent derivative computations, and then can simply execute the expression to compute the derivative</li></ul><ul id="cb7fe389-22e1-4585-853a-327f0773d07a" class="bulleted-list"><li style="list-style-type:disc">efficient and optimizes derivative computation</li></ul><ul id="04b0b3ab-77ce-4319-b058-083759f0c5ed" class="bulleted-list"><li style="list-style-type:disc">“autograd” is the name of a particular package for “autodiff”</li></ul></details></li></ul><ul id="b1edaff7-b443-473b-8966-23b0f0b8853e" class="toggle"><li><details><summary>How Autodiff works for NN</summary><p id="62516208-7aa3-4797-ad90-48d8254d8e23" class="">NN functions can be complex, so we can use auto diff to break down the complex functions into smaller functions and calculate the derivative for those </p><ul id="7804605c-69c7-4015-b799-0b8ca4e5941c" class="bulleted-list"><li style="list-style-type:disc">Autodiff facilitates NN training by break down complex functions into simpler ones to compute derivatives efficiently<ul id="8bb7eea7-290c-456a-9f59-6e2983649f70" class="bulleted-list"><li style="list-style-type:circle">construct a computational graph</li></ul><ul id="fa31cab6-968f-4f11-ab8c-e214aa7667ed" class="bulleted-list"><li style="list-style-type:circle">leverage the chain rule to compute derivatives efficiently<p id="7bd64081-f341-4d4b-a108-27e31d47672f" class="">a composite function f(x)=h(g(x)), the derivative of f with respect to x is df/dx=dh/dg dg/dx</p></li></ul><ul id="15b0e5e7-2fa1-4229-a58c-be1c4c3166b5" class="bulleted-list"><li style="list-style-type:circle">during backpropagation, compute derivatives through accumulation of values during code execution</li></ul></li></ul><figure id="056699fc-2e59-4e94-b393-523b267b8b78" class="image"><a href="Untitled%20252.png"><img style="width:957px" src="Untitled%20252.png"/></a></figure></details></li></ul><ul id="c6a43a30-2335-490c-bb7d-64e59339e023" class="toggle"><li><details><summary>Computational Graph</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="8c4b1131-15e1-46de-affc-0bd19e6b373d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a conceptual representation to break down calculations into individual operations that are easier to analyze and manipulate<ul id="c4204c50-d973-4b1a-97ef-cc849fbdb49c" class="bulleted-list"><li style="list-style-type:disc">Nodes: each <mark class="highlight-red">node represents</mark> an <mark class="highlight-red">operation </mark>or a <mark class="highlight-red">variable</mark>.</li></ul><ul id="9d773e24-cc11-4537-b2bf-5de3ff6fd50e" class="bulleted-list"><li style="list-style-type:disc">Edges: <mark class="highlight-red">directed </mark>arrows <mark class="highlight-red">connecting nodes</mark>, indicating the <mark class="highlight-red">flow of data</mark><p id="b2d71811-4285-4889-a90e-2e97a0fc3eeb" class="">represent the dependencies between operations, specifying which operations must be completed before others can begin.</p></li></ul><figure id="83ecde8e-8e55-4530-9ed3-60b30d51d7ac" class="image"><a href="Untitled%20253.png"><img style="width:1009px" src="Untitled%20253.png"/></a></figure></div></figure><ul id="9dc7013d-dfdf-423c-b0e6-7d61241d4fa8" class="toggle"><li><details><summary>The role of Computational Graph</summary><p id="5d639d4f-e8c3-43b1-8882-757ab4a3b59a" class="">to evaluate the computational graph, we can feed a variable/data instance into the graph. going through the graph, each node will apply operations to the input value</p><figure id="8c146bb6-63bd-411d-8452-1ce7adb124a2" class="image"><a href="Untitled%20254.png"><img style="width:568px" src="Untitled%20254.png"/></a></figure><p id="f85c0958-380b-460e-9ca2-7d9af9fe3dfc" class="">to calculate the derivative, we calculate the partial derivatives as the data goes through each edge </p><figure id="5fad72cf-f72f-4483-a94b-f25c07472ac2" class="image"><a href="Untitled%20255.png"><img style="width:358px" src="Untitled%20255.png"/></a></figure><p id="9bef4bb0-e7df-4ebd-8105-72a8d2cb1eb1" class="">to make this process more efficient we can use a reverse mode</p><figure id="144518f7-d6ab-4c14-b7ba-bade65c96a07" class="image"><a href="Untitled%20256.png"><img style="width:568px" src="Untitled%20256.png"/></a></figure><figure id="dae2471e-d734-4962-80ae-35ceddc1deba" class="image"><a href="Untitled%20257.png"><img style="width:958px" src="Untitled%20257.png"/></a></figure></details></li></ul><ul id="f7bfa23e-9fe8-481c-884d-599a47a74e45" class="toggle"><li><details><summary>Two Modes</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="90c5c8bb-2ec0-4927-aabf-ca8f5853322f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Two primary modes: forward mode and reverse mode differentiation</div></figure><ul id="064c721a-7979-4dff-9755-f2805c6ff634" class="bulleted-list"><li style="list-style-type:disc">forward-mode starts at an input to the graph and moves towards the end, gives us the derivatives of all outputs with respect to one input</li></ul><ul id="5cfea967-e4ce-43a1-84d7-f9190cee0bdd" class="bulleted-list"><li style="list-style-type:disc">reverse-mode starts at an output of the graph and moves towards the beginning, gives the derivatives of one output with respect to all inputs</li></ul><figure id="f5e6c344-fb4b-413d-9602-9e4d448b8b39" class="image"><a href="Untitled%20258.png"><img style="width:568px" src="Untitled%20258.png"/></a></figure><p id="deaaee0e-7162-424d-b865-e3470a476cc5" class="">for NN, if we use the forward mode, it calculates derivatives for all the weight values. this can be inefficient</p><p id="61664429-db7d-40be-801c-9b39e9a29c95" class="">if we use the reverse mode, Z can be the output of the loss function, and we can find the partial derivative of the loss function which calculates the loss with respect to each node. </p><p id="76af915c-ee9a-46df-a2b3-d6007f6e1fe7" class="">this means we only have to calculate once rather than for all nodes</p><p id="ca208ca5-3ee0-4e28-95d9-3c8a7c20b1c2" class="">reverse mode is refereed to as back propagation </p></details></li></ul></details></li></ul><ul id="c07810cd-983e-4db9-98a7-88f1ea4589b4" class="toggle"><li><details><summary>AutoDiff Algorithm</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e268985b-fd26-4e4a-badf-b374aba4246c" class="code"><code class="language-Python">def graident(out):
		node_to_grad[out] = 1
		nodes = get_node_list(out)
		for each node in reverse_topo_order(nodes):
				grad = sum partial adjoints from output edges
				input_grad = node.op.gradient(input, grad) for input in node.inputs
				add input_grads to node_to_grad
			return node_to_grad
			
#Sum the partial derivative from output edges
#Compute gradients of the operation with respect to its inputs
# Accumulate gradients for each input node</code></pre><h3 id="96459609-bc79-4014-a03d-186e8e90f09e" class="">Example</h3><figure id="0a9bbeff-71d2-4052-997b-f588fb630241" class="image"><a href="Untitled%20259.png"><img style="width:568px" src="Untitled%20259.png"/></a></figure><figure id="1a5239b1-7a54-40c9-be2e-6e50dec71c3f" class="image"><a href="Untitled%20260.png"><img style="width:568px" src="Untitled%20260.png"/></a></figure><figure id="f9b6b5af-d50b-4388-b1cd-76962dcdc8e4" class="image"><a href="Untitled%20261.png"><img style="width:568px" src="Untitled%20261.png"/></a></figure><figure id="224405c0-ffcc-4bdc-adc2-6c7cc6ae13f6" class="image"><a href="Untitled%20262.png"><img style="width:568px" src="Untitled%20262.png"/></a></figure><figure id="b0ac22be-f08b-4c3b-8e98-a9b558ab9f05" class="image"><a href="Untitled%20263.png"><img style="width:1058px" src="Untitled%20263.png"/></a></figure><figure id="754f6309-5fd7-434b-a748-f342882c8621" class="image"><a href="Untitled%20264.png"><img style="width:1065px" src="Untitled%20264.png"/></a></figure><figure id="e9688089-f04c-4f87-aa47-d973b86ab754" class="image"><a href="Untitled%20265.png"><img style="width:1047px" src="Untitled%20265.png"/></a></figure><figure id="25a7c143-647a-4849-8ec5-b0c5bf7dd9e0" class="image"><a href="Untitled%20266.png"><img style="width:1058px" src="Untitled%20266.png"/></a></figure><figure id="55eefbb4-7471-4bd0-85e9-68a329f4b8dd" class="image"><a href="Untitled%20267.png"><img style="width:1072px" src="Untitled%20267.png"/></a></figure><figure id="cfae0076-dd11-4359-9c27-2363958ce288" class="image"><a href="Untitled%20268.png"><img style="width:1073px" src="Untitled%20268.png"/></a></figure><figure id="07ebac8b-a987-4bda-9d08-a5fa817cea24" class="image"><a href="Untitled%20269.png"><img style="width:793px" src="Untitled%20269.png"/></a></figure></details></li></ul><ul id="eb677947-efeb-47d7-bf87-4f77a6df38e6" class="toggle"><li><details><summary>More complicated functions</summary><figure id="3852f349-e439-4944-ac4d-fc2213a56adc" class="image"><a href="Untitled%20270.png"><img style="width:713px" src="Untitled%20270.png"/></a></figure><figure id="fbee84a3-78f4-403e-a856-a63c77a81850" class="image"><a href="Untitled%20271.png"><img style="width:743px" src="Untitled%20271.png"/></a></figure></details></li></ul><ul id="bd2c65b3-e55d-42be-aae1-0344f30cccd1" class="toggle"><li><details><summary>Autograd in Pytorch</summary><figure id="477ca3f0-567b-4775-a0cd-af79ce6481aa" class="image"><a href="Untitled%20272.png"><img style="width:740px" src="Untitled%20272.png"/></a></figure></details></li></ul><ul id="ed867758-e07b-4475-bf8d-32a0ba424e24" class="toggle"><li><details><summary>Summary</summary><figure id="dae4eeaf-8aee-4f3a-8275-e4f6ddd93e87" class="image"><a href="Untitled%20273.png"><img style="width:568px" src="Untitled%20273.png"/></a></figure></details></li></ul></details></li></ul></details></li></ul><ul id="e9d5a44c-47f0-4ba1-8eea-14c153ba82c5" class="toggle"><li><details><summary>Search Methods</summary><ul id="c6c77304-007f-4c3f-874f-fd15235af36b" class="toggle"><li><details><summary>Search in AI/ML</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="71777de7-fb6f-4466-afa2-35a005b46989"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">search refers to the <mark class="highlight-blue_background">process of exploring</mark> a vast space of <mark class="highlight-blue_background">possible solutions</mark>/states/configuration <mark class="highlight-blue_background">to find an optimal</mark> or near-optimal solutions/states/configuration <mark class="highlight-blue_background">for a given problem</mark></div></figure><h3 id="9459b0d3-fb5b-40f3-acfd-8ef00da207e5" class="">explore possible states</h3><figure id="5f553525-e2d4-4038-ae85-0d2c6be88e26" class="image"><a href="Untitled%20274.png"><img style="width:562px" src="Untitled%20274.png"/></a></figure><h3 id="5d936761-668e-44f8-b256-740a0b3e22d3" class="">find the best solution</h3><figure id="78f46675-e444-406a-97e9-1a771090bcb0" class="image"><a href="Untitled%20275.png"><img style="width:1110px" src="Untitled%20275.png"/></a></figure></details></li></ul><ul id="84fcbe37-f134-4bba-a5f9-45d4c46caed2" class="toggle"><li><details><summary>General Search Algorithm</summary><ol type="1" id="c1608de1-d9a9-4f9c-b94e-61932a078f51" class="numbered-list" start="1"><li>Initialization: start with an initial state or node</li></ol><ol type="1" id="3c085f41-de6d-40be-b253-177209f56f73" class="numbered-list" start="2"><li>Node Expansion: expand the node by generating successors</li></ol><ol type="1" id="2efa7cdc-9aee-4384-af3b-6ac1b24fb9b1" class="numbered-list" start="3"><li>Goal Test: each expanded node is checked against the goal criteria. If the goal is met, the algorithm terminates and returns the solution</li></ol><ol type="1" id="4c00c1ec-f04d-410d-9e29-a2acd454026d" class="numbered-list" start="4"><li>Strategy for Node Selection: use a specific strategy to decide which of the available nodes to explore next</li></ol><ol type="1" id="615c3dbe-cf8a-4144-9348-1dab7e77b6ad" class="numbered-list" start="5"><li>Loop: The process repeats with the new current node until the goal is reached or no more nodes are available for expansion</li></ol><figure id="d4bc3af3-1f8a-4945-aae5-c983a965992f" class="image"><a href="Untitled%20276.png"><img style="width:567px" src="Untitled%20276.png"/></a></figure></details></li></ul><ul id="4a036bd0-b33e-4954-9aa0-42399bc0618d" class="toggle"><li><details><summary>Search strategies</summary><p id="0364b92c-8672-4b77-9915-25468faf49cb" class="">there are 2 categories of search based on if we have pre-information (heuristic) or not</p><h3 id="8c7ea56b-c64e-4e82-a207-5a9d29d60784" class="">Uninformed search(blind search)</h3><ul id="a001e591-55d7-4382-aeed-5a3aed18407f" class="bulleted-list"><li style="list-style-type:disc">Breadth first</li></ul><ul id="2e5253cc-0b34-4dbf-851a-40a00ce6e19c" class="bulleted-list"><li style="list-style-type:disc">Uniform cost</li></ul><ul id="f1bf8ffa-2086-40cf-882f-3117d7b40fad" class="bulleted-list"><li style="list-style-type:disc">Depth first</li></ul><ul id="c284c9e1-214a-44bc-bfea-fe2796a2fe60" class="bulleted-list"><li style="list-style-type:disc">Depth limited</li></ul><ul id="fb3acb6b-4727-49c4-b45f-d0a29b42b341" class="bulleted-list"><li style="list-style-type:disc">Iterative deepening</li></ul><ul id="fff7a749-5117-43dc-905f-18ba5b667093" class="bulleted-list"><li style="list-style-type:disc">Bidirectional</li></ul><h3 id="4e511dca-cd9e-4dc0-85f5-486e79763653" class="">Informed (Heuristic) search</h3><ul id="4aa9fe23-d8d7-4409-8345-a738076f047b" class="bulleted-list"><li style="list-style-type:disc">Greedy(best-first) search</li></ul><ul id="91ae9322-7a45-4ba8-a7d4-e5fb15e92394" class="bulleted-list"><li style="list-style-type:disc">A* search</li></ul><h3 id="9003577e-7b08-4e70-81c7-60db904fab11" class="">Beyond classic search (subgroup of heuristic search)</h3><ul id="27f8ac6a-acb1-4153-87fe-4ff25aa73f57" class="bulleted-list"><li style="list-style-type:disc">Hill climbing</li></ul><ul id="491758ac-26b2-4742-8683-44678e6c853f" class="bulleted-list"><li style="list-style-type:disc">Gradient descent</li></ul><ul id="b2eb6e06-c8be-4695-98a8-e8f70ad1a1ac" class="bulleted-list"><li style="list-style-type:disc">Simulated Annealing</li></ul><ul id="a3ca30be-829e-4da2-a706-bfc3526283f0" class="bulleted-list"><li style="list-style-type:disc">Beam search</li></ul><ul id="655b9216-40f0-4f27-a229-8e84adc2f3a1" class="bulleted-list"><li style="list-style-type:disc">Bound and bound</li></ul><ul id="053891c2-1db5-4359-b62c-b46ad66fe288" class="bulleted-list"><li style="list-style-type:disc">dynamic programming</li></ul></details></li></ul><ul id="117c1f47-44f9-4079-8e28-4bc419ac2ab1" class="toggle"><li><details><summary>Uninformed (Blind) Search</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="69bff280-5787-46f4-9fe8-b554c62ac8ff"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">operate <mark class="highlight-blue_background">without any information about</mark> the <mark class="highlight-blue_background">number of steps</mark> or the <mark class="highlight-blue_background">path cost </mark>from any node in the search tree to the goal state<ul id="8f582118-9d11-4ce9-b431-fe06c6dfa3b5" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-blue_background">only rely on </mark>the problem&#x27;s <mark class="highlight-blue_background">inherent configuration</mark> to make decisions about which nodes to expand</li></ul><figure id="32c27dcc-0c0b-4718-bc09-e2f7cbaaf5fe" class="image"><a href="Untitled%20277.png"><img style="width:1272px" src="Untitled%20277.png"/></a></figure><ul id="5ee1e953-c815-4311-9872-b6a6f923a474" class="bulleted-list"><li style="list-style-type:disc">explores all state spaces</li></ul><ul id="a3517ceb-7e0c-42aa-874d-37e73f29fde5" class="bulleted-list"><li style="list-style-type:disc">will find solution if it exits, some algorithms are designed to find the optional one</li></ul><ul id="37cb50f0-f1e5-4a4a-926c-d341d20acb16" class="bulleted-list"><li style="list-style-type:disc">has high space time complexity</li></ul><ul id="d789b4bc-d453-421f-939a-eae2fb00750d" class="bulleted-list"><li style="list-style-type:disc">foundational technique in AI when little to no heuristic information</li></ul></div></figure></details></li></ul><ul id="8ac12b4d-4b23-4691-8c7b-a5381153df86" class="toggle"><li><details><summary>Informed (Heuristic) search</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="97f5f9a9-ec74-4af8-a306-337470628f93"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><mark class="highlight-blue_background">uses problem-specific knowledge</mark> (heuristic function) to <mark class="highlight-blue_background">guide </mark>the search towards the goal<ul id="ab92e5b4-eee8-448a-ba82-056928e51dee" class="bulleted-list"><li style="list-style-type:disc">more efficient than blind search</li></ul><ul id="64472e55-2d6f-4341-aaed-a87f0c916453" class="bulleted-list"><li style="list-style-type:disc">heuristic function ℎ(n) estimates cost from start to goal</li></ul><ul id="c31b4ede-6182-44eb-a35e-384d90ce5b11" class="bulleted-list"><li style="list-style-type:disc">example problem: for estimating cost or distance to goal, heuristic prioritize paths that are likely to lead to the solution</li></ul></div></figure><h3 id="ea917002-9931-46ba-b228-aabe6176ef77" class="">Greedy Best-First Search</h3><p id="5da3f24e-da04-4834-b857-051c8e8bd36d" class="">uses a heuristic function that just choses the next node that appears the closest to the goal</p><ul id="46d7e7e5-8f49-4910-a3de-693e206014ac" class="bulleted-list"><li style="list-style-type:disc">doesn’t guarantee the optional or a valid solution </li></ul><figure id="d8ed7752-e43f-477d-8a24-df10e12e38a2" class="image"><a href="Untitled%20278.png"><img style="width:725px" src="Untitled%20278.png"/></a></figure><ul id="ae632cc4-9a6c-4d5e-9f9b-3e15b95b5456" class="bulleted-list"><li style="list-style-type:disc">used when speed is better than accuracy </li></ul><ul id="b5fa16df-c556-4856-9882-9118e75400ba" class="bulleted-list"><li style="list-style-type:disc">can also be used when  goal is clearly defined</li></ul><ul id="024e49a3-4f57-4d1e-92c6-5454a312b057" class="bulleted-list"><li style="list-style-type:disc">effectiveness depends on quality of the heuristic</li></ul><h3 id="48edae96-cd99-46e1-aa80-defa20570242" class="">A* Search</h3><p id="1d157d14-81c0-4be4-8162-b779d58c47b1" class="">finds the most efficient path from the start to goal states</p><ul id="55fe42a0-c0be-4835-8497-3cedd2e49c43" class="bulleted-list"><li style="list-style-type:disc">estimates the cost from start to goal from the actual path traveled so far + remaining path using the heuristic </li></ul><figure id="448e0f3e-ab1e-4103-ba89-213dfd371fbb" class="image"><a href="Untitled%20279.png"><img style="width:1101px" src="Untitled%20279.png"/></a></figure><ul id="d83fad96-bda2-49eb-ac6f-1f9255244524" class="bulleted-list"><li style="list-style-type:disc">heuristic has an impact on performance of A*</li></ul><ul id="0d86ae70-f14d-45a9-b408-a150a0a3247c" class="bulleted-list"><li style="list-style-type:disc">guarantees optional solution if heuristic is good</li></ul><ul id="be336690-ed4f-400d-b999-b599aa94d25f" class="bulleted-list"><li style="list-style-type:disc">used in games, robotics and planning times</li></ul><h3 id="d4ca2115-f303-4a25-82cb-e6402db6165b" class="">Hill Climbing</h3><p id="d298c096-1acb-4757-a23e-044a7f3c6e8e" class="">iteratively improves a single solution based on neighbouring solutions </p><ul id="b53d23c4-49e7-4045-ae26-dd640d214c5e" class="bulleted-list"><li style="list-style-type:disc">if a neighbouring solution is better than the candidate solution, then it will move to the neighbouring solution </li></ul><ul id="e5c7ce18-4a8d-4f50-943c-082eee1b3e3e" class="bulleted-list"><li style="list-style-type:disc">issue is that it can get stuck on a local optimum, and never get to the actual optional solution</li></ul><figure id="ef239eb9-6874-4005-9b7f-9c756d337718" class="image"><a href="Untitled%20280.png"><img style="width:1054px" src="Untitled%20280.png"/></a></figure><ul id="f47f83c8-63b3-4e42-a08f-662f6fc80d6a" class="bulleted-list"><li style="list-style-type:disc">a local search technique that aims to find the optional solution by making small changes to a candidate solution and evaluating whether the new solution is better</li></ul><ul id="c5832b01-de51-46c5-88d9-a26e1e2cd674" class="bulleted-list"><li style="list-style-type:disc">hill climbing is a greedy approach </li></ul><ul id="492b2bed-bd06-4b55-bd07-268054355ba2" class="bulleted-list"><li style="list-style-type:disc">use a heuristic or objective function to evaluate the solution</li></ul><ul id="67e9910c-88b8-43d7-aa95-32551a52e36f" class="bulleted-list"><li style="list-style-type:disc">well-suited for optimizing over surfaces with only one maximum</li></ul><ul id="49fea5b5-3017-49ce-859a-f2783a4057a3" class="bulleted-list"><li style="list-style-type:disc">this is the bases of gradient descent</li></ul><h3 id="1729ca48-bb40-4d0b-820b-41ec389a74d7" class="">Simulated Annealing</h3><p id="49a39308-b403-4bcd-9a2f-10d4942655c9" class="">iteratively improves the current solution by randomly perturbing it and accepting the perturbation with a certain probability</p><ul id="73092100-314c-44fb-b0c1-266f1ab06b9f" class="bulleted-list"><li style="list-style-type:disc">improvement over hill climbing</li></ul><figure id="757f5cbe-4667-425f-ad09-2c13ca328071" class="image"><a href="Untitled%20281.png"><img style="width:486px" src="Untitled%20281.png"/></a></figure><ul id="a3481ac9-4ce4-4fad-bec5-55d31b4715fd" class="bulleted-list"><li style="list-style-type:disc">inspired by annealing process in metal/glass</li></ul><ul id="7fd81003-c8a1-4bbe-a34c-90b0875b4586" class="bulleted-list"><li style="list-style-type:disc">randomly perturbing lets us get out of the local optimum </li></ul><ul id="d20b43aa-3834-4ba3-9b31-4aa78774c17c" class="bulleted-list"><li style="list-style-type:disc">depends on a “temperature parameter” and cooling schedule <ul id="7ef74c49-ba7c-4f91-9130-57290203c497" class="bulleted-list"><li style="list-style-type:circle">decreases over time. at the beginning , we have a higher probability of accepting a worse solution. at the end we decrease this probability </li></ul></li></ul><figure id="bc2510d7-c646-4a37-b746-fe818a386950" class="image"><a href="Untitled%20282.png"><img style="width:624px" src="Untitled%20282.png"/></a></figure><h3 id="c2f24d23-16f9-4a43-82da-b12afef2f857" class="">Beam search</h3><ul id="6225b9d5-f97c-4f05-9284-81abf35280be" class="bulleted-list"><li style="list-style-type:disc">extends breath first search</li></ul><p id="b040549d-d6ef-4064-ba69-69f5dc713ac2" class="">generate all possible successor states and keep only the “best” k candidates, i.e., “beam”</p><ul id="13412cb7-4d09-4cbc-a137-a0660c50dfa0" class="bulleted-list"><li style="list-style-type:disc">reduce the search space significantly while still allowing a broad exploration of paths</li></ul><ul id="cdfa00d1-f954-4dce-b1f5-ea22c8cc09a2" class="bulleted-list"><li style="list-style-type:disc">the beam width k is configurable</li></ul><ul id="bf8d27c6-a473-4da1-ba6d-a4d467f2719c" class="bulleted-list"><li style="list-style-type:disc">applications in robot navigating, a maps app searching for the best route, game-playing systems</li></ul><figure id="eccabe1d-708d-4888-9fca-e1d32f1901f9" class="image"><a href="Untitled%20283.png"><img style="width:1060px" src="Untitled%20283.png"/></a></figure></details></li></ul><ul id="9764e6dc-6418-4482-95ca-72384fde9559" class="toggle"><li><details><summary>Summary</summary><figure id="747e1dcc-8923-4742-8b1b-dab67f00bd0f" class="image"><a href="Untitled%20284.png"><img style="width:1057px" src="Untitled%20284.png"/></a></figure></details></li></ul></details></li></ul><ul id="0a466fef-8e0a-4982-8343-f3871dccea80" class="toggle"><li><details><summary>Evolutionary Computation: Genetic Algorithm</summary><ul id="af49217d-e978-47fa-80d1-8722dac539dd" class="toggle"><li><details><summary>Why Evolutionary Computation?</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="e6672a0e-d99e-4e31-b076-60bfd5906a83"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Evolutionary Computation (EC) is one technique that <mark class="highlight-blue_background">can avoid some problems/limitations </mark>with the previous discussed <mark class="highlight-blue_background">methods and algorithms in ML</mark></div></figure><h3 id="a62c1f8b-8192-4ab7-902c-c777e807405d" class="">Limitations/Problems EC can solve</h3><ul id="cdb24b85-528c-46b2-b559-a21660f8ecde" class="bulleted-list"><li style="list-style-type:disc">Local optima</li></ul><ul id="db11a775-01be-4021-a3a9-fd1f5b7aa31a" class="bulleted-list"><li style="list-style-type:disc">Unreasonable assumptions</li></ul><ul id="c1baf006-eb23-40e7-81f1-444bc47e1770" class="bulleted-list"><li style="list-style-type:disc">Needs to predefine/fix the structure/model of the solution, and only learns the parameters/coefficients</li></ul><ul id="d22e5cc9-a454-4f8b-90d9-2e0e6624c354" class="bulleted-list"><li style="list-style-type:disc">Many parameters to learn (high-dimensional optimisation)</li></ul></details></li></ul><ul id="6cf6889e-d1c2-4cb9-96b0-4d17c18dbc16" class="toggle"><li><details><summary>what is Evolutionary Computation and Learning</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="264855a6-c79f-46ec-adfb-e16ce1fef428"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">evolutionary computation is a family of “nature inspired” AI algorithms for global optimisation</div></figure><ul id="7c4e6b1f-c022-4aa3-b4c1-a59ef0a651f7" class="bulleted-list"><li style="list-style-type:disc">• In technical terminology, they are a family of population-based trial-and-error problem solvers with a metaheuristic or stochastic optimisation character.</li></ul><ul id="afe1a39e-6712-4d67-8276-96da97d42134" class="bulleted-list"><li style="list-style-type:disc">Evolutionary Learning is the use of evolutionary computation methods for tackling machine learning tasks</li></ul><figure id="3d003159-43f2-4834-921c-0822d015c9a3" class="image"><a href="Untitled%20285.png"><img style="width:1310px" src="Untitled%20285.png"/></a></figure><figure id="c9f18453-9d3b-4189-9e9e-3f80551bb4df"><a href="https://en.wikipedia.org/wiki/Evolutionary_computation" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Evolutionary computation</div><div class="bookmark-description">In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.</div></div><div class="bookmark-href"><img src="https://en.wikipedia.org/static/apple-touch/wikipedia.png" class="icon bookmark-icon"/>https://en.wikipedia.org/wiki/Evolutionary_computation</div></div><img src="https://upload.wikimedia.org/wikipedia/commons/f/fb/Darwin_image_evolution_from_random_patches.gif" class="bookmark-image"/></a></figure></details></li></ul><ul id="a5f92e80-0dd2-404e-bbf9-535263702b07" class="toggle"><li><details><summary>EC Techniques</summary><ul id="620df56b-bb70-451a-b6c1-cdcf51308626" class="bulleted-list"><li style="list-style-type:disc">Evolutionary algorithms (EAs)<p id="cf68ba2f-f88a-48d9-bb18-9207594c2642" class="">inspired by nature</p><ul id="c12a9060-8080-458c-bf4b-8556a70b680c" class="bulleted-list"><li style="list-style-type:circle">Genetic algorithms (the biggest branch)</li></ul><ul id="70e7fe81-2421-45db-a4f3-b26a8c2fdf87" class="bulleted-list"><li style="list-style-type:circle">Evolutionary programming</li></ul><ul id="123dbec2-6c07-4532-9ca6-71965614b285" class="bulleted-list"><li style="list-style-type:circle">Evolutionary strategies</li></ul><ul id="034d33e1-31c7-4638-8856-6eae69c89cd0" class="bulleted-list"><li style="list-style-type:circle">Genetic Programming (Koza, 1990s, fast growing area) </li></ul></li></ul><ul id="8d9c3ca1-9fed-4b2e-93fb-2f212eb6d844" class="bulleted-list"><li style="list-style-type:disc">Swarm intelligence (SI)<p id="6fb1a100-8491-4b95-8751-8e4c4f430156" class="">inspired by social insects</p><ul id="765cbb92-45f9-473d-98f6-4f2d68949622" class="bulleted-list"><li style="list-style-type:circle">Ant colony optimisation</li></ul><ul id="689f24db-38c3-4e8b-acfb-9a3b152ab5b1" class="bulleted-list"><li style="list-style-type:circle">Particle swarm optimisation (PSO) </li></ul><ul id="5e7156dc-d733-4c83-a5f7-11b0d5d5468d" class="bulleted-list"><li style="list-style-type:circle">Artificial immune systems</li></ul></li></ul><ul id="bbe560b3-a7a5-435b-b45d-9ae262b963b8" class="bulleted-list"><li style="list-style-type:disc">Other techniques<ul id="0251da12-ed57-4d18-af45-8c1a30a8ca64" class="bulleted-list"><li style="list-style-type:circle">Differential evolution</li></ul><ul id="2f3983a8-39d4-4f35-bf54-15cbb85d3742" class="bulleted-list"><li style="list-style-type:circle">Estimation of distribution algorithms</li></ul></li></ul></details></li></ul><ul id="10f24f8b-ca3d-4899-9027-cfaa2f9b776e" class="toggle"><li><details><summary>Evolutionary Algorithms</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="104719f4-3b7a-4a83-a2de-ddff642a8b9d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">search for the best solution by evolving a population with some reproduction<figure id="a29ddf37-d7a4-453e-bf49-26472a63a802" class="image"><a href="Untitled%20286.png"><img style="width:879px" src="Untitled%20286.png"/></a></figure><ul id="0cdce916-ffe1-4c32-812a-6a5872af3c64" class="bulleted-list"><li style="list-style-type:disc">starts with randomly initialized solutions</li></ul><ul id="cea52d04-3c16-4502-bd9f-7d071dc27a82" class="bulleted-list"><li style="list-style-type:disc">based on the good individual solutions, we will select the best ones which will become parents  by going into a mating pool and creating children solutions</li></ul><ul id="15fb3d28-efcf-4962-ad35-5c7007b3af96" class="bulleted-list"><li style="list-style-type:disc">this process keeps repeating until some stopping criteria is meet</li></ul></div></figure></details></li></ul><ul id="ca979d2d-b953-457c-be31-9ab9ce947975" class="toggle"><li><details><summary>Evolutionary Search</summary><figure id="7d769e45-bcac-4704-8339-8b3aaaa31f31" class="image"><a href="Untitled%20287.png"><img style="width:624px" src="Untitled%20287.png"/></a></figure></details></li></ul><ul id="96234ed8-d487-40a6-abf9-4389895ffd57" class="toggle"><li><details><summary>Key Characteristics</summary><ul id="a5e37567-295c-41f5-9ded-98daad6e04b2" class="bulleted-list"><li style="list-style-type:disc">One (or more) <mark class="highlight-yellow">populations of individuals</mark></li></ul><ul id="d097d2b5-415f-45c5-aca4-e7991357c12f" class="bulleted-list"><li style="list-style-type:disc">Dynamically changing populations due to the <mark class="highlight-yellow">birth and death of individuals </mark>(through crossover, mutation, …)</li></ul><ul id="b9d80bbb-8496-425b-abfc-1e17dcab4ce2" class="bulleted-list"><li style="list-style-type:disc">A <mark class="highlight-yellow">fitness function</mark> which reflects the ability of an individual to survive and reproduce (<mark class="highlight-yellow">“survival of the fittest”</mark>)</li></ul><ul id="287a06dc-22cb-47fc-beda-82b636b1c937" class="bulleted-list"><li style="list-style-type:disc">Variational inheritance: <mark class="highlight-yellow">offspring closely resemble their parents</mark>, but are not identical</li></ul><p id="7aa34b43-56ba-43c3-af95-957eb47a6048" class="">
</p><ul id="251fa7e8-e200-4c61-a516-aa08441b272d" class="bulleted-list"><li style="list-style-type:disc">Final solution (individual): the one with the best fitness</li></ul><ul id="3d25bb83-f68e-4689-998a-f2178314d190" class="bulleted-list"><li style="list-style-type:disc">Fitness could be accuracy, cost, error, …</li></ul></details></li></ul><ul id="a1aea772-4462-4577-b188-f704dbb75e08" class="toggle"><li><details><summary>Key Design Questions</summary><h3 id="026e9051-c30d-488b-8a12-78b4adf3fbfd" class="">Representation</h3><p id="af4990f5-c555-44d1-bec6-cf108ddf498a" class="">How can we represent individuals (solutions)?</p><ul id="0a177de1-92e2-4044-bff9-2b9333d9c99b" class="bulleted-list"><li style="list-style-type:disc">could have a array or list to represent if a solution has been selected or not</li></ul><h3 id="4482b7df-e3a4-4e23-b70b-3f722c8f0e55" class="">Evaluation</h3><p id="99d800cc-3f85-4af0-91da-291109405c2d" class="">How can we evaluate individuals (fitness function)?</p><ul id="20fbf077-6c9f-4211-84b7-dc912ece8db5" class="bulleted-list"><li style="list-style-type:disc">A fitter individual should have a better objective value (e.g. smaller error)</li></ul><ul id="751b9cb5-8737-4248-b088-c4d702e37464" class="bulleted-list"><li style="list-style-type:disc">we have to define a fitness function to measure how close an individual solution is to a desired solution </li></ul><h3 id="21c7cbb3-c42a-4a27-aaa5-88cce25539cc" class="">Selection</h3><p id="e15a6a50-268b-4c44-b9d9-536cf6abedcf" class="">How to select individuals into the mating pool (selection scheme)?</p><ul id="fcdc9eef-eebc-44c9-a28e-786679b7ba6f" class="bulleted-list"><li style="list-style-type:disc">Selection pressure</li></ul><ul id="284c6a3a-a04f-434d-89c0-a2b2536ed3a3" class="bulleted-list"><li style="list-style-type:disc">Fitter individuals should be more likely to survive/reproduce</li></ul><h3 id="35922494-035b-4ed9-bafe-8fdc28d56479" class="">Genetic Operators</h3><p id="9cc4be22-a654-4355-bd02-00a59ccd5036" class="">How to generate new individuals (crossover, mutation operators)?</p><ul id="4da60e49-3bae-4a65-b5af-6fcd3855db2d" class="bulleted-list"><li style="list-style-type:disc">Children inherit strong parts of parents</li></ul><ul id="b36eab6c-adba-4790-8b6b-b70521bf5bcd" class="bulleted-list"><li style="list-style-type:disc">Maintain diversity (jump out of local optima)</li></ul><h3 id="5ed7e75f-1dd7-4df1-af1c-b35f0adc9fe1" class="">Other parameters</h3><p id="a2a92e1c-dd1a-49c2-ac55-f1870420d1b2" class="">population size, mating pool size, stopping criteria</p></details></li></ul><ul id="79294df2-d94d-4400-96b9-61aec0fbf3d4" class="toggle"><li><details><summary>Individual Solution Representation</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="e5f56bb1-0663-4cd1-a12d-b2a299a5b539"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">representation for an Individual Solution in an evolutionary algorithm is <mark class="highlight-yellow">Problem dependent</mark></div></figure><h3 id="d07e5f0e-9eb7-4caa-8127-72300e91404f" class="">Example Representations</h3><figure id="ed35f62c-9e6d-421a-8faa-b0c88f97ce56" class="image"><a href="Untitled%20288.png"><img style="width:624px" src="Untitled%20288.png"/></a></figure></details></li></ul><ul id="48ac97de-cc20-4bf4-bc34-dfd7ec469b6d" class="toggle"><li><details><summary>Fitness Function</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="5fc498a8-bf13-4a53-92f8-852d2131b5c7"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">computes the <mark class="highlight-yellow">quality </mark>of <mark class="highlight-yellow">an individual</mark> solution</div></figure><ul id="24dfd7d3-e431-458f-9349-ffe244ee5e88" class="bulleted-list"><li style="list-style-type:disc">Must correspond to optimality property</li></ul><ul id="4d7f3ebc-ea0d-490c-b8d7-169860e0f35d" class="bulleted-list"><li style="list-style-type:disc">Must be computable</li></ul><ul id="53e87585-861e-4ad9-a5a9-b4ff7e86d5bf" class="bulleted-list"><li style="list-style-type:disc">Must be smooth (change in input is proportional to the size of the output)<ul id="b7724fd4-6848-4188-ab6d-e4866c3d1f68" class="bulleted-list"><li style="list-style-type:circle">Small changes to candidate -&gt; small changes to quality/fitness</li></ul><ul id="b6a21088-36d5-4e3a-88bf-d2adbc3a5ede" class="bulleted-list"><li style="list-style-type:circle">Large changes to candidate -&gt; large changes?</li></ul></li></ul><ul id="8a212fab-bac8-4624-84fe-2a27d96dc5ff" class="bulleted-list"><li style="list-style-type:disc">Depending on the problem, the fitness function could be:<ul id="97fc4685-f1b5-4309-9d65-fddd5ed884e0" class="bulleted-list"><li style="list-style-type:circle">the larger, the better --- maximization</li></ul><ul id="9c6e3a6c-4f89-425c-9938-6830617bcbf6" class="bulleted-list"><li style="list-style-type:circle">the smaller, the better --- minimisation</li></ul></li></ul></details></li></ul><ul id="066897b9-2fab-4d64-9965-4024036947d3" class="toggle"><li><details><summary>Solution Selection Operator</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="5c6a1b75-7264-4204-adb3-e0d2c5161460"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">a selection operator that selects which solutions from a generation get added to the mating pool</div></figure><h3 id="40e22302-bd11-4c57-8252-7c43865b9ec9" class="">Uniform selection</h3><p id="595560e1-6602-490c-839f-9fbd587b236e" class="">Each individual has the same chance to be selected</p><h3 id="4e1bd66d-251d-4cf9-ad42-ae5045186a78" class="">Roulette wheel selection</h3><p id="0a08252e-6e8b-4d9a-913d-689903326d15" class="">The <mark class="highlight-yellow">probability </mark>of being <mark class="highlight-yellow">selected </mark>is <mark class="highlight-yellow">proportional to </mark>the <mark class="highlight-yellow">fitness</mark></p><ul id="25d66755-14f0-4dad-9b2e-73e727f6e340" class="bulleted-list"><li style="list-style-type:disc">Assume fitness is maximised</li></ul><figure id="ab5e2da7-1728-4d8f-ab47-7efad60b0bb1" class="image"><a href="Untitled%20289.png"><img style="width:489px" src="Untitled%20289.png"/></a></figure><h3 id="7b7ce619-5561-4bf3-8f64-3d1d00adffdb" class="">K-tournament selection</h3><p id="a3e168f5-b9d6-4f1b-a032-c837a319eedf" class="">from the whole set of solutions, we randomly chose k solutions</p><p id="3ae413ab-0c61-4344-9e2f-acba4f9118df" class="">among these k solutions, we select/pick the best one based on the fitness solution </p><ul id="29b95a5e-ac2f-4c2f-99c8-074fe8e15814" class="bulleted-list"><li style="list-style-type:disc">the point of this is that we give less fit solutions the chance of reproduce</li></ul><ul id="be6c42e6-ce01-4b6a-8903-54c7237950f9" class="bulleted-list"><li style="list-style-type:disc">if you’re unlucky you might only get sub optional solutions </li></ul><figure id="3ed0896d-fa9c-4acc-90a6-83f155da7415" class="image"><a href="Untitled%20290.png"><img style="width:624px" src="Untitled%20290.png"/></a></figure><h3 id="2ffeb677-eb8a-4690-9418-bfab2aa28917" class="">Truncate selection</h3><p id="2d2f081e-44ce-470d-93b7-0184910f718e" class="">when we only care about the top best/most fit solutions </p><p id="4479e058-ed96-4aef-bcbb-78747437671b" class="">
</p></details></li></ul><ul id="1a688cf5-7cbb-41a2-aaf6-a8f9b30439a0" class="toggle"><li><details><summary>Genetic Operators</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="fd554e27-58be-400b-9a10-1a3f30572cdd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">Depends on the problem – individual representation</div></figure><ul id="2caf6fb1-0427-405d-9fcf-b4f1a91609c3" class="bulleted-list"><li style="list-style-type:disc">Swap a bit of a binary vector</li></ul><ul id="82d93049-7169-4f36-b5b3-cb492cd4336e" class="bulleted-list"><li style="list-style-type:disc">Resample an element of a continuous vector</li></ul><ul id="7e2b288a-44aa-44d2-9f58-4b1ad7593d09" class="bulleted-list"><li style="list-style-type:disc">Shuffle a part of a sequence</li></ul><p id="6d9fd1f8-05bf-4afa-bc5b-298b2f1dfcf5" class="">
</p><p id="c41e6152-7e04-4140-9c8b-a99404c14fa1" class="">
</p></details></li></ul><ul id="2471f7ed-5d3f-4794-808c-40de3438fa31" class="toggle"><li><details><summary>Genetic Algorithms</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="5f7c9462-4010-471d-ad40-3b6aead402a7"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><ul id="89756f03-8598-46ec-a1e3-38671a256157" class="bulleted-list"><li style="list-style-type:disc">popular optimization method</li></ul><ul id="61ed2ca7-8f5e-4037-b0bf-1ecdf8e9bf71" class="bulleted-list"><li style="list-style-type:disc">used often in feature selection or designing in engineering</li></ul></div></figure><ul id="5c400f85-764a-4c9c-8f82-d16f67aceecf" class="toggle"><li><details><summary>Representation: binary string</summary><figure id="b1e6ee72-773a-489f-a9ac-843b7a388d59" class="image"><a href="Untitled%20291.png"><img style="width:624px" src="Untitled%20291.png"/></a></figure></details></li></ul><ul id="9e392289-6aa2-4424-8a7d-c454aad920ac" class="toggle"><li><details><summary>A Basic Genetic Algorithm </summary><p id="454ed0e3-4651-4852-9337-b25a9d9229f1" class="">Randomly initialise a population of chromosomes</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="41da37bd-bf72-4f70-9eef-50cc61f0d037" class="code"><code class="language-C++">Repeat until stopping criteria are met:
		• Construct an empty new population
		• Repeat until the new population is full:
				• Select two parents from the population by roulette wheel
				selection
				• Apply crossover to the two parents to generate two children
				• Each child has a probability (mutation rate) to undergo mutation
				• Put the two children into the new population
		• End Repeat
		• Move to the new population (new generation)
• End Repeat
• Output the best individual from the final population</code></pre><figure id="3852a5c4-3c16-44ee-877c-93228de00a70" class="image"><a href="Untitled%20292.png"><img style="width:1141px" src="Untitled%20292.png"/></a></figure></details></li></ul><ul id="80960a6d-f8b2-4cfe-962b-1b5b27cb2d91" class="toggle"><li><details><summary>A Simple GA Example</summary><figure id="fd7fab52-bcbc-4061-a389-f73cfa190b6d" class="image"><a href="Untitled%20293.png"><img style="width:596px" src="Untitled%20293.png"/></a></figure><figure id="2693cbac-3d4c-4a20-aa3f-fd5a679511b1" class="image"><a href="Untitled%20294.png"><img style="width:801px" src="Untitled%20294.png"/></a></figure></details></li></ul></details></li></ul><ul id="466c8c26-8df6-432a-a1b1-305c3a65cb42" class="toggle"><li><details><summary>Genetic Programming</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="5d6b93b5-6ae8-4d71-9724-a902a26719e6"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">inherits properties from EC techniques and automatic programming<ul id="2c89fb66-4368-4380-b2b9-a6f920d84c13" class="bulleted-list"><li style="list-style-type:disc">similar evolutionary process to the general evolutionary algorithms</li></ul><ul id="533b30f6-7963-4ccc-bbba-90f70d164a24" class="bulleted-list"><li style="list-style-type:disc">genetic algorithm uses bit strings to represent solutions while genetic programming uses tree-like structures to represent programs</li></ul><figure id="c8b8737f-1bb5-479d-9f62-90dca3d3865f" class="image"><a href="Untitled%20295.png"><img style="width:240px" src="Untitled%20295.png"/></a></figure></div></figure><h3 id="243bb02b-7ea9-4892-9d7f-eb3f242681f6" class="">GP Tree: Programs as Tree Structures</h3><figure id="3c314571-9bd2-4f51-8fcc-07935dd835a0" class="image"><a href="Untitled%20296.png"><img style="width:624px" src="Untitled%20296.png"/></a></figure><h3 id="8aa59951-78b8-46b9-ad67-ff56527d69f3" class="">GA vs GP: Representation</h3><figure id="7bed539e-bc3e-48b8-8586-7ace25875ddb" class="image"><a href="Untitled%20297.png"><img style="width:624px" src="Untitled%20297.png"/></a></figure><h3 id="8173d3d7-20cb-4471-a845-91f228eabde2" class="">A Basic GP algorithm</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="857b8dca-0663-46b1-858b-813751b8a285" class="code"><code class="language-C++">• Initialise the population

• Repeat until the stopping criteria is met:
		• Evaluate the fitness of each program in the current population
		• Create an empty new population
		• Repeat until the new population is full:
				• Select programs in the current generation
				(often tournament selection)
				• Apply genetic operators to the selected programs to
				generate offspring (e.g. 80% crossover, 15% mutation, 5%
				reproduction).
				• Insert the children programs into the new generation.

• Output the best individual program in the population. </code></pre></details></li></ul></details></li></ul><ul id="083aa194-7dd8-4360-a916-3be97a428612" class="toggle"><li><details><summary>Search Algorithms</summary><ul id="d3202189-f086-4931-9a86-78fbf3076c5e" class="toggle"><li><details><summary>General Search Algorithm</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="dd31ace3-46b7-43fc-a60c-6e83d7fe8f50"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">To find the best solution from a set of possible solutions<figure id="8b6bd707-d68d-42ef-ae17-ccb6a422eb83" class="image"><a href="Untitled%20298.png"><img style="width:562px" src="Untitled%20298.png"/></a></figure></div></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="8ff99a43-3723-49d9-84ef-cfa9472ed4a2" class="code"><code class="language-Java">1. Initialization: start with an initial state or node
2. Node Expansion: expand the node by generating successors
3. Goal Test: each expanded node is checked against the goal criteria. If the
							goal is met, the algorithm terminates and returns the solution
4. Strategy for Node Selection: use a specific strategy to decide which of the
							available nodes to explore next
5. Loop: The process repeats with the new current node until the goal is
					reached or no more nodes are available for expansion</code></pre></details></li></ul><ul id="7effebd9-9534-4758-8623-3e5cca82be36" class="toggle"><li><details><summary>Uninformed and Informed Search</summary><figure id="d005d565-42c4-404d-a365-382b00257fc8" class="image"><a href="Untitled%20299.png"><img style="width:624px" src="Untitled%20299.png"/></a></figure></details></li></ul><ul id="fa23af27-1cb0-43ad-9208-d938022d4bf5" class="toggle"><li><details><summary>Greedy Best-First Search</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="f0561ce8-fdd6-49ce-8195-92b742afc976"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">uses heuristic function to chose the next node that <mark class="highlight-red">appears closest</mark> to the goal/best solution<ul id="412c8ca7-1b63-4bb5-897e-7476969aff83" class="bulleted-list"><li style="list-style-type:disc">just look forward and choose without thinking </li></ul><ul id="7a7f5404-cee0-4beb-85a8-1b812e2b5f19" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-red">may not find </mark>the actual <mark class="highlight-red">optimal </mark>solution or just <mark class="highlight-red">fail</mark><p id="5bbdb5bc-82ce-45e1-96d5-08d46be4b024" class="">gets stuck in the local optimum, may never get to global optimum</p></li></ul><figure id="2ffbf3f9-aa20-4884-a631-d333238f4a38" class="image"><a href="Untitled%20300.png"><img style="width:676px" src="Untitled%20300.png"/></a></figure></div></figure></details></li></ul><ul id="45f4eecc-055a-4a25-9bdc-5e2e0f1f7301" class="toggle"><li><details><summary>A* Search</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="2027f683-44fc-4e3c-a91d-893cc2c60b8b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">find the most efficient path between the start and goal<ul id="95ba4d66-d873-4957-985a-a00d4ad55db0" class="bulleted-list"><li style="list-style-type:disc">calculates estimated cost from start to goal using actual cost from goal to current node + heuristic function for the straight line distance from current node to goal<p id="d91e172c-7127-45f0-8db9-58b1edf8e7ce" class="">𝑓(𝑛)=𝑔(𝑛)+ℎ(𝑛)</p></li></ul><ul id="e2471a14-585d-41b6-ba7b-e5e7d9f8fb36" class="bulleted-list"><li style="list-style-type:disc">Looking both forward &amp; backward</li></ul></div></figure><h3 id="c8502810-bc84-4d0a-8083-b88bbcbd9625" class="">Greedy search vs A* search algorithms</h3><figure id="07e2f15e-4b7c-4693-9e2a-eee415a7084b" class="image"><a href="Untitled%20301.png"><img style="width:624px" src="Untitled%20301.png"/></a></figure></details></li></ul><ul id="28d97769-0e07-40ae-9c5c-1a279496c74e" class="toggle"><li><details><summary>Hill climbing vs simulated annealing</summary><figure id="80fd5007-24ea-413b-b386-f3280b893436" class="image"><a href="Untitled%20302.png"><img style="width:624px" src="Untitled%20302.png"/></a></figure><p id="ec2840e1-2c5a-4fc1-b8bd-6a867736f0ff" class="">Compare the performance of the hill climbing and simulated annealing optimization algorithms on a function with multiple local optima</p><figure id="202543f7-d6c9-4f4d-8667-84f7f865a639" class="image"><a href="Untitled%20303.png"><img style="width:1249px" src="Untitled%20303.png"/></a></figure></details></li></ul><ul id="59ec4579-9db5-42f3-9db4-296a907f7b85" class="toggle"><li><details><summary>Beam Search and Breadth-First Search</summary><p id="4c094e32-63ab-4191-bc30-77de03c98c08" class="">compared to breadth first, beam search constrains the search space by top k candidates, while breadth first searches everything</p><figure id="3ffab41c-88c8-45c4-9177-27e7b8951065" class="image"><a href="Untitled%20304.png"><img style="width:1312px" src="Untitled%20304.png"/></a></figure></details></li></ul><ul id="a40816d8-45a0-49aa-89c0-45aa5388b2dd" class="toggle"><li><details><summary>Evolutionary Computation</summary><figure id="71bf7609-41b0-46c4-8a36-57a76fd85971" class="image"><a href="Untitled%20305.png"><img style="width:527px" src="Untitled%20305.png"/></a></figure></details></li></ul><ul id="92f77160-ef12-4770-aa80-99328be56ff3" class="toggle"><li><details><summary>GA and GP (Representation)</summary><figure id="2852a1a3-22a2-45da-9834-c5745a358c8d" class="image"><a href="Untitled%20306.png"><img style="width:929px" src="Untitled%20306.png"/></a></figure><figure id="c437466f-3837-4bed-8ea3-53a0b68e4359" class="image"><a href="Untitled%20307.png"><img style="width:624px" src="Untitled%20307.png"/></a></figure></details></li></ul></details></li></ul><ul id="32b6a3af-6dc8-4c0e-ae6d-57e321c953b1" class="toggle"><li><details><summary>Advanced Regression and Clustering Algorithms</summary><ul id="2fa2adc9-b003-472b-af87-0824e9535ae2" class="toggle"><li><details><summary>Advanced Regression</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="682792a5-7d9e-4246-ae7b-bf456534254c"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">regression analysis examines the relationship between independent variables (features) and a dependent variable (output) <br/><br/>advanced regression <br/><mark class="highlight-blue_background">enhances traditional regression by addressing limitations </mark>such as overfitting, feature selection and handling non linear relations </div></figure><ul id="ceec6b53-f716-4c37-b33c-646825ed29fe" class="toggle"><li><details><summary>Advanced Regression Applications</summary><p id="6b38f02a-ff01-4d12-9e05-18f1f1003079" class="">1. regression technique is used mainly to determine the <mark class="highlight-blue_background">predictor strength</mark>, <mark class="highlight-blue_background">forecast trend</mark>, <mark class="highlight-blue_background">time series</mark>, and <mark class="highlight-blue_background">cause &amp; effect relation</mark></p><ol type="1" id="2fbebab0-cc7a-43c1-b3b0-4fd4e4d028d4" class="numbered-list" start="2"><li>advanced regression analysis - a powerful tool for <mark class="highlight-blue_background">data driven decision</mark>-making in various fields</li></ol></details></li></ul><ul id="f603572d-e708-4040-92b1-e26f77a5c0a7" class="toggle"><li><details><summary>Logistic Regression</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="a97a3af4-db59-4c1d-bb0a-f644064ae4cb"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">regression technique used for discrete target variables <ul id="95ebe6c3-9410-451e-84c0-aa96d51d96cb" class="bulleted-list"><li style="list-style-type:disc">used for classification </li></ul><ul id="00420f61-b078-4e52-b66e-c38cdfa68e58" class="bulleted-list"><li style="list-style-type:disc">estimates a probability that the target variable is a certain class label</li></ul><figure id="0c1b2846-0e69-455a-aece-2de6ba5f2b78" class="image"><a href="Untitled%20308.png"><img style="width:534px" src="Untitled%20308.png"/></a></figure></div></figure><h1 id="f377271f-e96e-42af-80df-fb5046632de1" class="">Coefficient Learning</h1><p id="6db305bb-dedc-4bed-8a3b-af34d6995815" class="">main task in logistic regression is for the model to learn the coefficients </p><p id="f211b39d-a214-4db2-896e-175ff0b54a44" class="">often use the <mark class="highlight-yellow">maximum likelihood method </mark>to determine the model parameters for the logistic regression equation</p><ul id="e5fbe5b3-261f-4b31-a247-a1e6fca80900" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow">Likelihood Function</mark> L(𝜃): indicates how probable it is that the observed data occur<figure id="e3d6322b-8d14-4e0a-b6df-05e14ef14886" class="image"><a href="Untitled%20309.png"><img style="width:512px" src="Untitled%20309.png"/></a></figure></li></ul><ul id="301e3300-2ef6-4a51-a666-b548a1d8e05d" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow">Stochastic gradient descent</mark> to maximize the log likelihood function 𝐿𝑜𝑔(𝐿(𝜃))<figure id="da6e242c-a0e9-4f3b-8254-7df2b299906f" class="image"><a href="Untitled%20310.png"><img style="width:1006px" src="Untitled%20310.png"/></a></figure></li></ul><p id="e45e973d-4ed5-4db3-b7b2-285d89b82b9c" class="">
</p></details></li></ul><ul id="34d802b8-f8dc-41ee-961b-a55d074a8a1e" class="toggle"><li><details><summary>Polynomial Regression</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="e7496b94-7bcb-42a1-9f26-9a246375296c"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">extends logistic regression by adding several terms.<br/>relationship between independent variables and dependent variables are <br/><mark class="highlight-yellow">modeled as nth degree polynomial</mark><figure id="ab9b50d8-5db6-4696-8ec2-a55e1f355ea4" class="image"><a href="Untitled%20311.png"><img style="width:1236px" src="Untitled%20311.png"/></a></figure><ul id="9ccf63c1-a476-479f-9bb3-0cfeaef21610" class="bulleted-list"><li style="list-style-type:disc">increase the degree in the model, it tends to increase the performance of the model</li></ul><figure id="fa211858-87dd-4b1e-8ce2-83e5f05f331b" class="image"><a href="Untitled%20312.png"><img style="width:1402px" src="Untitled%20312.png"/></a></figure></div></figure><h3 id="7f5fbfb5-d107-4265-b434-dfe1f0704f46" class="">Coefficient learning</h3><p id="4d5ad716-2040-4436-9b64-c4d3b824f1b3" class="">same as linear regression, we want to learn the coefficients of the regression equation</p><p id="4bc334fa-a963-461a-ba2f-a8c8b1c158b4" class="">we can learn the coefficient by using <mark class="highlight-yellow">least square estimation</mark> that <mark class="highlight-yellow">minimizes the sum of the squared residuals </mark></p><figure id="bccd0741-d287-4168-97e7-c4826eace948" class="image"><a href="Untitled%20313.png"><img style="width:596px" src="Untitled%20313.png"/></a></figure><ul id="c2986988-1137-4c70-aec7-9b782014f10a" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow">Ordinary Least Squares</mark>: using matrix algebra by solving the normal equations<figure id="7b66eb74-a20c-4bd3-8b51-348cd979a4b7" class="image"><a href="Untitled%20314.png"><img style="width:913px" src="Untitled%20314.png"/></a></figure><p id="df66d05b-2a3d-4ffc-97bb-d14702ec69a5" class="">represents the polynomial function as a matrix </p><p id="de54508c-7e02-4989-88e0-a3f059a29338" class="">each row is one instance</p><p id="eb6ba138-1269-4afe-868a-a02caa4d937c" class="">each column is each term in the polynomial regression</p></li></ul><ul id="051926e6-57ce-4818-8062-4cf421cddbee" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow">Gradient Descent</mark>: iteratively updating the coefficients in the direction of the negative gradient</li></ul><ul id="f5ddc972-d035-46c5-b923-f4175702b27f" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-yellow">Regularization techniques </mark>are used to prevent overfitting in polynomial regression, especially when dealing with high degree polynomials  </li></ul><p id="f090a7b2-d9f5-42a5-83e5-700383331277" class="">
</p></details></li></ul><ul id="c8260f3a-a29f-4b4c-a902-0bc6ef7c3366" class="toggle"><li><details><summary>Symbolic Regression</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="54b99e89-7cf5-4dda-8db8-7c9173062d76"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><mark class="highlight-yellow">explores/searches</mark> for <mark class="highlight-yellow">mathematical expressions that best fits/most suitable model for a given data</mark> <mark class="highlight-yellow">set </mark><ul id="46aac9ef-6949-48c8-aa4b-96fdbb399516" class="bulleted-list"><li style="list-style-type:disc">evolutionary computation techniques </li></ul><ul id="7f8a0962-e341-487a-b117-84fc2abe632c" class="bulleted-list"><li style="list-style-type:disc">instead of fitting data to a model</li></ul><figure id="47d4d925-6b57-4547-9fd9-500e09929b3a" class="image"><a href="Untitled%20315.png"><img style="width:1066px" src="Untitled%20315.png"/></a></figure></div></figure><h3 id="bf4b3083-22b2-4243-a8c9-b6eef907a687" class="">Genetic Programming for Symbolic Regression</h3><figure id="f14bf6d8-dd54-43c7-b1bb-83f3d4f91822" class="image"><a href="Untitled%20316.png"><img style="width:596px" src="Untitled%20316.png"/></a></figure><h3 id="6ff49d27-8566-4f92-bba3-75d2458bf4f2" class="">Symbolic Regression Applications</h3><figure id="1c88a226-a760-4813-bd84-3b1a3c99c875" class="image"><a href="Untitled%20317.png"><img style="width:596px" src="Untitled%20317.png"/></a></figure></details></li></ul></details></li></ul><ul id="6c7c4ea2-11e8-4501-852b-72eb18d6474c" class="toggle"><li><details><summary>Advanced Clustering</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="f6b44092-76e6-4bc4-90d8-093b88b4adc3"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">clustering groups data into clusters<ul id="f3d4eb70-f1ab-4178-8803-3d4f12ebefcc" class="bulleted-list"><li style="list-style-type:disc">its unsupervised because we don’t define the class labels</li></ul><p id="d7f2b49f-d9a4-4dd4-9213-7f53c46b64ab" class="">advanced clustering allows us to handle more complex data such as:</p><ul id="6027e8d8-34f3-498b-8763-ce8bfa19ae4a" class="bulleted-list"><li style="list-style-type:disc">clusters with arbitrary shapes</li></ul><ul id="f7b2583e-4458-4475-9552-8a51542cbc25" class="bulleted-list"><li style="list-style-type:disc">handling large datasets</li></ul><ul id="4af63a16-242d-4ae5-9769-350856c008e0" class="bulleted-list"><li style="list-style-type:disc">identifying and handle noise or outliers</li></ul></div></figure><ul id="d6ef0a84-a688-43ee-a58c-4f6c3f31adc8" class="toggle"><li><details><summary>Applications</summary><p id="ea8558e7-7076-4bd3-9589-7cfc0d43a6cf" class="">Advanced clustering techniques are used in a variety of real world applications such as market segmentation, anomaly detection, bioinformatics, social network analysis, and document clustering</p><figure id="81dc29f4-efa6-4329-aded-58025cba1962" class="image"><a href="Untitled%20318.png"><img style="width:918px" src="Untitled%20318.png"/></a></figure></details></li></ul><ul id="98aba557-5895-464a-b33b-0a99614b0b55" class="toggle"><li><details><summary>Mean Shift Clustering</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="bde87f06-98fc-40dc-965f-b0108d7c9153"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">tries to find dense area of data points using a sliding window<ul id="9df0d2e2-1276-4aaa-bd37-c1c32c7910e1" class="bulleted-list"><li style="list-style-type:disc">the goal is to locate the center points of each group</li></ul><ul id="e81158a1-7584-484e-b2d4-3ef6170e482c" class="bulleted-list"><li style="list-style-type:disc">candidates for centroids to be the mean of the point in the sliding-window</li></ul><ul id="a9dd8a8d-a864-4095-976b-13cb4f34571e" class="bulleted-list"><li style="list-style-type:disc">eliminate near-duplicates candidate windows</li></ul><ul id="a476d465-9691-4db7-9c53-3e87ca1675c4" class="bulleted-list"><li style="list-style-type:disc">don’t need to define amount of clusters, but do need to define “bandwidth” </li></ul><ul id="3f4c76e9-47a6-4907-8569-a2ad4b0783b4" class="bulleted-list"><li style="list-style-type:disc">destiny based clustering technique</li></ul><figure id="7b7ed77a-aef9-421a-87a3-976f573d2e20" class="image"><a href="Untitled%20319.png"><img style="width:534px" src="Untitled%20319.png"/></a></figure></div></figure><h3 id="e00c3098-06b5-4009-b330-ac58651234c4" class="">Algorithm </h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d771af78-0e2e-4643-ae64-5289ba863144" class="code"><code class="language-C++">1. Initialise centers: start with the initial centers of the clusters, 
	can be randomly selected or, typically, 
	every data point is considered as an initial center.
2. Calculate mean shift vectors: for each center, perform the following:
		- identify points within bandwidth
		- compute weighted mean: calculate the weighted mean of these points
			using a kernel function
3. Update centers: update the position of each center to the 
		computed weighted mean
4. Check for convergence: measure the amount each center moved 
		since the last iteration. 
		If all centers move less than a predefined small threshold, 
		then assume convergence and stop the iteration
5. Assign clusters: assign each data point to the cluster 
		of the nearest center
6. Finalize clusters: Optionally, you can merge centers that 
		are very close to each other to reduce the number of clusters</code></pre></details></li></ul><ul id="3edb4769-1176-4771-9e53-045963e1b8c6" class="toggle"><li><details><summary>DBSCAN (Density-based spatial clustering of applications with noise)</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="43652b83-8a7f-4bd8-913b-3cdc357b61de"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">groups data points with highly packed (large number of neighbours) into a clustor<ul id="fb83abb3-8a45-4187-ac0d-f5ea2d6d7e17" class="bulleted-list"><li style="list-style-type:disc">can find cluster with arbitrary shape</li></ul><ul id="38da5ecc-8de8-4c5a-bbfc-4f8d846523cd" class="bulleted-list"><li style="list-style-type:disc">identify noise/outliers from data</li></ul></div></figure><ol type="1" id="785872a5-a832-42ea-a038-15ebe33dee3c" class="numbered-list" start="1"><li>starts from a random point</li></ol><ol type="1" id="7d4ab55e-826c-4bac-ad69-c240389be967" class="numbered-list" start="2"><li>checks if that point is a core point or noise point<ul id="c5f4973d-e834-4bde-bd27-77a219579b7f" class="bulleted-list"><li style="list-style-type:disc">this is done by checking the number of neighbours sitting in epsilon radius </li></ul><ul id="4055c760-173b-4bad-8870-b797ee227e10" class="bulleted-list"><li style="list-style-type:disc">calculates the distance from one point to near by points</li></ul><ul id="1e83dc91-8790-4474-8289-d18d29ed0c6c" class="bulleted-list"><li style="list-style-type:disc">if the distance is greater than ‘eps’ its a core point, else its a noise point</li></ul></li></ol><ol type="1" id="cc60c058-06c6-41e5-a725-f1cbcb52c7b4" class="numbered-list" start="3"><li>creates a new cluster </li></ol><figure id="b1b4e88b-951e-499c-a55c-51fdfdd16afa" class="image"><a href="Untitled%20320.png"><img style="width:596px" src="Untitled%20320.png"/></a></figure><figure id="881f78c9-b69b-4283-9625-22c4f6b40286" class="image"><a href="Untitled%20321.png"><img style="width:596px" src="Untitled%20321.png"/></a></figure></details></li></ul><ul id="812c06b5-daaa-40ee-bf52-69e2114daec5" class="toggle"><li><details><summary>BIRCH Clustering (Balanced Iterative Reducing Clusters using Hierarchies)</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="63090967-14c0-415b-a041-7ac5a0ac256f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">creates a Clustering Feature Tree for given data used to summarize a cluster<ul id="a191cf9c-6771-4e6f-8f6d-f080886ac2a7" class="bulleted-list"><li style="list-style-type:disc">often used to complement other clustering algorithms cluster large datasets</li></ul><ul id="4abeca71-7e7e-4f20-9d0c-425ebbfb5ca5" class="bulleted-list"><li style="list-style-type:disc">creating a summary of the dataset that the other clustering algorithm can now use</li></ul><figure id="07786550-d3c2-4342-ad17-ec02ed637077" class="image"><a href="Untitled%20322.png"><img style="width:1108px" src="Untitled%20322.png"/></a></figure></div></figure></details></li></ul><ul id="fb383926-704f-4798-b3e1-4ea33dc6746e" class="toggle"><li><details><summary>Summary</summary><figure id="f641c8c9-2c3b-4edb-876e-038066417daf" class="image"><a href="Untitled%20323.png"><img style="width:1076px" src="Untitled%20323.png"/></a></figure></details></li></ul></details></li></ul></details></li></ul><ul id="cea9dd4b-819d-4b91-8aa2-1b3629f4ab81" class="toggle"><li><details><summary>Emerging Fields in ML</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="94f4dd49-51c9-4760-98ec-9f78799a27ca"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">to address issues of classical ML, several emerging techniques have came out<ul id="d882e495-b391-44a2-a7bf-ec8a58060b7c" class="bulleted-list"><li style="list-style-type:disc">transfer learning</li></ul><ul id="43144c57-068f-4de9-84fb-57d7b66fc46d" class="bulleted-list"><li style="list-style-type:disc">explainable AI (XAI)</li></ul><ul id="282af17c-d88f-4470-8261-bfacb114c30a" class="bulleted-list"><li style="list-style-type:disc">automated machine learning </li></ul></div></figure><ul id="c6fdca38-c12a-43b3-a2b1-953e9c4c1042" class="toggle"><li><details><summary>Challenges of classical machine learning</summary><figure id="f4e29148-ca19-4da1-8d5d-397ec455bb9a" class="image"><a href="Untitled%20324.png"><img style="width:624px" src="Untitled%20324.png"/></a></figure></details></li></ul><ul id="6eb81447-2c42-4d38-bd0b-ac32a18600ff" class="toggle"><li><details><summary>Transfer Learning</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="84d47658-003e-434b-8557-4d8bdfcbc81d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">transfers knowledge learned from a source task to be re used in order to boast performance of a related target task<ul id="628ae56b-0c36-4927-bb6e-1a6ee6ba710b" class="bulleted-list"><li style="list-style-type:disc">reduces the need for large datasets in the target domain and improves model performance in cases where data is scarce</li></ul><figure id="c427a3da-2920-43d2-9844-038aa8fc15f9" class="image"><a href="Untitled%20325.png"><img style="width:562px" src="Untitled%20325.png"/></a></figure><ul id="47b67115-30b6-4097-828d-784739026761" class="bulleted-list"><li style="list-style-type:disc">rather than starting from scratch, use a pretrained model as the starting point for a new model</li></ul><ul id="643e98e9-dc81-430d-86e4-f4662e85642b" class="bulleted-list"><li style="list-style-type:disc">reduced training time and cost, significantly reduce the amount of data required to develop an accurate model</li></ul><ul id="1a3b1828-1af7-4913-8ac9-173cfec754db" class="bulleted-list"><li style="list-style-type:disc">leverage the knowledge and expertise gained</li></ul><ul id="821b6813-ff75-4989-b157-f188a9cb7734" class="bulleted-list"><li style="list-style-type:disc">improve performance and generalisaiton</li></ul><ul id="0ea933a9-4c77-4f22-9b90-652cdc650474" class="bulleted-list"><li style="list-style-type:disc">reduce overfitting for data with small data sets</li></ul></div></figure><h3 id="b1ba5f08-966f-4307-9f70-523f4d6748cf" class="">Example</h3><p id="4f369801-57d6-4d5c-ac13-f5c0edbd78e6" class="">retains the first several layers from the source task, then replace the latter layers to be trained on the new data set for the source task</p><ul id="564030fd-465d-436e-8d1f-7f8962a03e2c" class="bulleted-list"><li style="list-style-type:disc">this allows us to retain knowledge from the source task data set and apply it to the target task while adding little amount of data for the target task</li></ul><figure id="f86def4a-fad0-43d3-b8d8-4fe66a989224" class="image"><a href="Untitled%20326.png"><img style="width:624px" src="Untitled%20326.png"/></a></figure><h3 id="3412043d-9d95-4282-b3c5-4b5348afe9ee" class="">Three important Questions</h3><figure id="73381dd7-af2a-4e01-b29a-45bec5ff1948" class="image"><a href="Untitled%20327.png"><img style="width:624px" src="Untitled%20327.png"/></a></figure><h3 id="fc61df86-efdb-4679-ba85-625bfa609a45" class="">Different Types of Transfer Learning</h3><figure id="750cbc12-4a78-4458-be1d-17594f31395f" class="image"><a href="Untitled%20328.png"><img style="width:624px" src="Untitled%20328.png"/></a></figure></details></li></ul><ul id="e23da289-dff3-4fca-b5b4-0e2dd266a0da" class="toggle"><li><details><summary>Explainable AI (XAI)</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="b3a47864-39d6-4e13-b33d-4b05171b6efd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">makes the decision-making processes of an AI system transparent/understandable to humans<ul id="17d89014-5d16-4529-a80e-7714b1a10cdf" class="bulleted-list"><li style="list-style-type:disc">XAI = Explainable model + Explanation</li></ul><ul id="582e4ca2-2a9f-44fa-a6f9-7a51ab2d0e95" class="bulleted-list"><li style="list-style-type:disc">ensuring decisions can be trusted, audited, and improved</li></ul></div></figure><h3 id="9d351998-bbb9-412a-aa5c-cd26edd9b04c" class="">Types of explanations</h3><ul id="f69316cf-8d36-416a-979a-f9f1e31293f1" class="bulleted-list"><li style="list-style-type:disc">global explanations, which refer to an overall understanding of the model</li></ul><ul id="ba828fac-8668-4957-b80b-f30539c7ffd3" class="bulleted-list"><li style="list-style-type:disc">cohort explanation provides an explanation for a cohort or subset of instances</li></ul><ul id="22ee7f6c-d548-45f7-bb83-e6bce3315b40" class="bulleted-list"><li style="list-style-type:disc">local explanations, which focus on specific instances</li></ul><figure id="9821f5db-782a-4601-94c5-b5fd7f67e691" class="image"><a href="Untitled%20329.png"><img style="width:664px" src="Untitled%20329.png"/></a></figure><h3 id="e0e0a6e0-6ac2-4906-88b2-c1732927a666" class="">Why Explainable AI (XAI)</h3><p id="aaa6ca38-f399-43e2-b24e-62fd81a97ac1" class="">old ML methods have a trade off of either easy to understand but weak or hard to understand but strong</p><p id="fb75aa5a-bc78-404e-806e-324fa8d88a34" class="">XAI makes complex methods to be easier to understand</p><figure id="70cf0850-eece-431a-b6a9-f591106f7800" class="image"><a href="Untitled%20330.png"><img style="width:1106px" src="Untitled%20330.png"/></a></figure><h3 id="33222bfc-05cb-4f7a-996c-cac3b8f466b3" class="">XAI Methods</h3><figure id="9fde1ee1-dc77-4190-bf5e-a223df847bc0" class="image"><a href="Untitled%20331.png"><img style="width:1109px" src="Untitled%20331.png"/></a></figure><figure id="15793970-daec-4479-97ed-e1764e60b6ad" class="image"><a href="Untitled%20332.png"><img style="width:624px" src="Untitled%20332.png"/></a></figure></details></li></ul><ul id="3e5fef37-dc98-470b-8b4a-67da7f7cf905" class="toggle"><li><details><summary>Ethical Considerations</summary><figure id="0cb34964-7111-43e7-baf4-129f5112e47b" class="image"><a href="Untitled%20333.png"><img style="width:820px" src="Untitled%20333.png"/></a></figure></details></li></ul><ul id="89ca04f9-873a-4a4b-8eff-7d2e7076f303" class="toggle"><li><details><summary>Summary</summary><figure id="f47f6d68-6623-4af1-9c4c-07569c799922" class="image"><a href="Untitled%20334.png"><img style="width:816px" src="Untitled%20334.png"/></a></figure></details></li></ul><p id="b5c93006-619e-4a1b-ba92-d74dd820f7d4" class="">
</p></details></li></ul><ul id="273253ef-09b8-4757-a800-3346eeb333b9" class="toggle"><li><details><summary>Automated Machine Learning</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="0b1a296c-cef9-41ee-bf1f-1a039d4f1a8f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">technique that automates apart of the machine learning process<p id="bd685435-2a36-4b11-b6a5-43251b3dd862" class="">Key Components</p><ul id="7457d485-2e70-44b1-bc0c-dfa560ae9f9a" class="bulleted-list"><li style="list-style-type:disc">Automated data preprocessing</li></ul><ul id="f1f0715d-45d0-478f-b784-ec02b7d67812" class="bulleted-list"><li style="list-style-type:disc">Model selection</li></ul><ul id="0994bcf7-2ec3-4a70-b4a2-d7f61c18b433" class="bulleted-list"><li style="list-style-type:disc">Hyper-parameter optimization </li></ul><p id="d076922c-aa40-4dcf-9253-cf29b83bf8e0" class="">Benefits</p><ul id="efb96a33-1ff2-42fb-b67a-46f18c91fff5" class="bulleted-list"><li style="list-style-type:disc">Time Efficiency</li></ul><ul id="4e18a746-e45f-4602-a8b6-7610652a3ab1" class="bulleted-list"><li style="list-style-type:disc">Accessibility for Non-Experts</li></ul><ul id="1de719ec-378e-45b7-9efc-87c2eec99c45" class="bulleted-list"><li style="list-style-type:disc">Consistency in Model Performance</li></ul><ul id="0caf8352-99b0-412d-bcfb-1b985186f923" class="bulleted-list"><li style="list-style-type:disc">AutoML can find better models in less time than traditional ML approaches</li></ul></div></figure><ul id="26dcec40-a86a-41ca-b6be-d84b2d5648ca" class="toggle"><li><details><summary>Why AutoML?</summary><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="e50c84f5-66cf-48fb-ba46-3ef5788414d0"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">motivation for automl is due to the success of deep learning<ul id="c6c038b7-04a5-4350-93d8-fb7c0cbc1770" class="bulleted-list"><li style="list-style-type:disc">deep learning is powerful and used in a lot of applications, but its hard to get into because the performance of a deep learning model is sensitive to the model architecture and hyper parameters</li></ul><ul id="e04f7993-f6d7-4d44-83e8-96568907658c" class="bulleted-list"><li style="list-style-type:disc">goal of automl is to make complex machine learning techniques more simple to use for new people and easier for experts </li></ul></div></figure><h3 id="8387a5c8-56e5-4647-939a-68d891b786d6" class="">One Problem of Deep Learning</h3><figure id="f7a86472-e20e-431a-a1bb-31ddaf4d196e" class="image"><a href="Untitled%20335.png"><img style="width:624px" src="Untitled%20335.png"/></a></figure><h3 id="beb79e25-264f-4ce0-9e0b-7da8115a729d" class="">Current Deep Learning Practice vs New AutoML </h3><ul id="4966aa33-7b9a-4d57-8e63-055267254f55" class="bulleted-list"><li style="list-style-type:disc">current deep learning practice used experts to chose architecture and hyper parameters</li></ul><ul id="d6a0a222-5a1f-40db-84a6-762c0594f191" class="bulleted-list"><li style="list-style-type:disc">with automl, we don’t need to experts and we can use meta level learning and optimization from automl to chose the architecture and hyper parameters for us </li></ul><ul id="d6643097-60e5-4134-a88e-0f281e1c26ca" class="bulleted-list"><li style="list-style-type:disc">automl replaces experts with an algorithm</li></ul><ul id="a530f06d-5025-4b08-82fe-532b95cf4ea6" class="bulleted-list"><li style="list-style-type:disc">makes the process more systematic and efficient </li></ul><figure id="fc79201f-b22c-47c5-9fa4-4852182fc3f6" class="image"><a href="Untitled%20336.png"><img style="width:1010px" src="Untitled%20336.png"/></a></figure><h3 id="400b6f9c-8db8-4eaa-8c41-9d0e63f947f5" class="">Go Beyond Deep Learning</h3><figure id="43896e13-9555-4e9f-b488-948fe8441795" class="image"><a href="Untitled%20337.png"><img style="width:624px" src="Untitled%20337.png"/></a></figure></details></li></ul><ul id="ca1050e8-954b-4f12-8334-8d2001385a33" class="toggle"><li><details><summary>Hyperparameter Optimization (HPO) Problem</summary><ul id="c53d95cc-f786-43ea-8def-f84f98d6ef9a" class="toggle"><li><details><summary>what is Hyperparameter Optimization</summary><figure id="b32361fc-74d2-45d4-9905-0835b3025322" class="image"><a href="Untitled%20338.png"><img style="width:971px" src="Untitled%20338.png"/></a></figure></details></li></ul><ul id="4cea10df-3be4-4e14-907f-ad66047537b0" class="toggle"><li><details><summary>Types of Hyperparameters</summary><figure id="ac08d370-fbea-498d-9f3f-5a4bd1b37e77" class="image"><a href="Untitled%20339.png"><img style="width:596px" src="Untitled%20339.png"/></a></figure></details></li></ul><ul id="abac1b6d-4eb4-4f6e-addd-998da168ae5c" class="toggle"><li><details><summary>Conditional Hyperparameter</summary><figure id="fab8db35-867e-4005-928e-7d45599a3358" class="image"><a href="Untitled%20340.png"><img style="width:1032px" src="Untitled%20340.png"/></a></figure><ul id="70620138-6d4f-4098-96ea-6c144a06fd51" class="bulleted-list"><li style="list-style-type:disc">a hyperparameter being active means that it is important</li></ul><ul id="706c50ca-e277-41d5-9f7a-a5e6841c6f5c" class="bulleted-list"><li style="list-style-type:disc">conditional hyperparameters are hyperparameters that are not aways active/important </li></ul></details></li></ul><ul id="407831a5-daef-49d6-a102-c6fea479af3e" class="toggle"><li><details><summary>AutoML as Hyperparameter Optimization</summary><figure id="1285ba87-e7e9-4926-a8a8-839aa36b0a28" class="image"><a href="Untitled%20341.png"><img style="width:596px" src="Untitled%20341.png"/></a></figure></details></li></ul><ul id="5e050162-f729-4de6-8203-71337a2acf85" class="toggle"><li><details><summary>Example Problem for Hyperparameter Optimization</summary><figure id="d35f12c7-7590-41a3-8a3f-88e98c4c6964" class="image"><a href="Untitled%20342.png"><img style="width:713px" src="Untitled%20342.png"/></a></figure></details></li></ul><ul id="756b7591-075a-4f8f-ae5b-6e9522527d71" class="toggle"><li><details><summary>Blackbox Hyperparameter Optimization</summary><figure id="68169d98-99c9-4c81-b36c-4bfb9d57f685" class="image"><a href="Untitled%20343.png"><img style="width:596px" src="Untitled%20343.png"/></a></figure></details></li></ul><ul id="8dab3317-7817-4577-8cc5-bc396d80e6ee" class="toggle"><li><details><summary>Grid Search and Random Search</summary><figure id="2012878a-d4d1-4fc9-b20d-b5ae39a8c502" class="image"><a href="Untitled%20344.png"><img style="width:596px" src="Untitled%20344.png"/></a></figure><h3 id="ea434e39-dc39-40c8-a7bf-fdc39d635b61" class="">Which one is better</h3><p id="41d07d9b-bb57-4cd5-9db1-4c078fd9461f" class="">we could solve blackbox HPO by using grid search</p><figure id="b7534032-3b8f-4ff9-a65c-c8388ddb9a20" class="image"><a href="Untitled%20345.png"><img style="width:1423px" src="Untitled%20345.png"/></a></figure><p id="a1d33d31-8529-49be-ad8e-1080ead6f497" class="">but it is less efficient because it searches each hyperparameter, but not all hyperparameter are important so we should use random search</p></details></li></ul><ul id="73ebf7ce-e6ed-4f43-a16c-956a0704c568" class="toggle"><li><details><summary>Bayesian Optimization</summary><figure id="cd592f74-e54c-4b69-848c-2b850df1374c" class="image"><a href="Untitled%20346.png"><img style="width:596px" src="Untitled%20346.png"/></a></figure><figure id="d49707bd-45d3-44e6-818a-37608d417d6f" class="image"><a href="Untitled%20347.png"><img style="width:776px" src="Untitled%20347.png"/></a></figure></details></li></ul><p id="ac411fbe-96bf-43ec-b748-555f4a5d88bb" class="">
</p></details></li></ul><ul id="e3155595-d21d-47c7-bc49-089aeff83b3f" class="toggle"><li><details><summary>Genetic CNN</summary><figure id="3f851a2a-1d91-4c1e-94d2-101685896599" class="image"><a href="Untitled%20348.png"><img style="width:624px" src="Untitled%20348.png"/></a></figure><figure id="5675953e-23cc-4a7c-b593-623bd029fb41" class="image"><a href="Untitled%20349.png"><img style="width:629px" src="Untitled%20349.png"/></a></figure><figure id="d1425d71-9d79-4e91-8a4c-46895e0d1a30" class="image"><a href="Untitled%20350.png"><img style="width:624px" src="Untitled%20350.png"/></a></figure><figure id="d80ca02f-5929-4f06-92a5-0c17627d9981" class="image"><a href="Untitled%20351.png"><img style="width:621px" src="Untitled%20351.png"/></a></figure><figure id="3e66d842-19bf-4066-a0a1-1324501e04a3" class="image"><a href="Untitled%20352.png"><img style="width:624px" src="Untitled%20352.png"/></a></figure></details></li></ul><p id="7cdf15d8-33c0-4c5f-9003-1febdc3bdda3" class="">
</p></details></li></ul></details></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>